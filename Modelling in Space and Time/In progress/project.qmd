---
format: 
  pdf:
    documentclass: article
    toc: true
    include-before-body: cover.tex
    number-sections: true
    keep-tex: true
    latex-engine: xelatex
fontsize: 11pt
mainfont: Times New Roman
geometry: margin=2.5cm
editor: visual
citation: true
csl: harvard.csl
bibliography: references.bib
---

\newpage

# Sea Surface Temperature Modelling

## Part A: Cleaning and Spatial Overview

```{r}
#| label: fig:scatterplot
#| fig-cap: "Figure 1: Spatial distribution of Sea Surface Temperature (SST) observations collected on 1–2 January 1996 in the Kuroshio Current region. Each point represents an individual measurement; colour denotes temperature, with warmer SSTs concentrated in the north-east band."
#| echo: false
#| warning: false
#| message: false
# Load required libraries
library(geoR)
library(tidyverse)

# Read the data
kuroshio100 <- read.csv("~/GitHub/university-projects/Modelling in Space and Time/In progress/kuroshio100.csv")

# Summarise NA counts
# colSums(is.na(kuroshio100))

# Across all columns apart from `date` and `id`, there are 1557 missing values

# Remove rows with missing essential spatial data
kuroshio100_clean <- kuroshio100 %>%
  drop_na(lon, lat)

library(ggplot2)
library(viridis)

# SST spatial scatter plot
ggplot(kuroshio100, aes(x = lon, y = lat, color = sst)) +
  geom_point(size = 3, alpha = 0.9, shape = 16) +
  scale_color_viridis_c(
    option = "C",
    direction = -1,
    name = "SST (°C)",
    limits = c(min(kuroshio100$sst, na.rm = TRUE), max(kuroshio100$sst, na.rm = TRUE)),
    oob = scales::squish
  ) +
  labs(
    title = "Sea Surface Temperature Observations – Kuroshio Current",
    x = "Longitude",
    y = "Latitude"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5, size = 14),
    axis.title = element_text(face = "bold"),
    legend.title = element_text(size = 11),
    legend.text = element_text(size = 10),
    panel.grid = element_blank()
  )

# plot(kuroshio100_clean)
```

The dataset `kuroshio100.csv` contains 100 sea surface temperature (SST) observations from January 1996, recorded along the Kuroshio current system. Initial data inspection revealed three rows with missing spatial coordinates (`lon` or `lat`), which were removed to ensure compatibility with spatial modelling functions such as `as.geodata()`.

This resulted in **97 complete observations**, covering a broad range of longitudes and latitudes in the western Pacific Ocean. These values were retained for further exploratory and model-based analysis.

Figure 1 confirms the dataset captures a wide latitudinal spread and a broad SST range. Warmer values were observed to the south and east, suggesting a clear spatial structure that will be investigated in subsequent sections.

I should comment on that weird pattern of data!

## Part B: Spatial Data Partitioning for Validation

To enable independent model validation, five spatial locations were randomly withheld from the dataset. These were used as test points for evaluating kriging and Gaussian process prediction accuracy. The selection was made using a fixed seed for reproducibility:

```{r}
#| warning: false
#| message: false
set.seed(444)  # For reproducibility

# Using the cleaned dataset to ensure we dont chose missing values.
# 5 random points
test_points <- kuroshio100_clean %>%
  sample_n(5)

# Display their information
test_points %>%
  select(id, lon, lat, sst)

```

Now we create the training dataset

```{r}
#| warning: false
#| message: false
# Create training dataset (excluding test points)
kuroshio_train <- anti_join(kuroshio100, test_points, by = c("id", "lon", "lat", "sst"))

# Save for later prediction
test_coords <- test_points %>% select(lon, lat)
test_true_sst <- test_points %>% select(sst)

```

This split produced:

-   **Training set**: 92 spatial observations

-   **Test set**: 5 observations reserved for validation

These five test points are used consistently in Parts C–E to compare model predictions and assess uncertainty quantification.

## Part C: Empirical Variogram and Spatial Correlation Structure

CHECK IS MAX DIST IS RIGHT, CHECK IF I NEED TO INCLUDE TREND IN VARIOGRAM

USE WEEK 2 PRACTICAL.

Let sample variogram do itslef, I am overcolpicated bins etc

data(parana) par(mar=c(4,2,2,2)) plot(parana)

Refer to this for trend??\

```{r}
#| warning: false
#| message: false
# Convert training dataset into a geodata object
# kuro_geo_train <- as.geodata(kuroshio_train, coords.col = c("lon", "lat"), data.col = "sst")

# Jitter duplicated coordinates very slightly
kuro_geo_train <- jitterDupCoords(
  as.geodata(kuroshio_train, coords.col = c("lon", "lat"), data.col = "sst"),
  max = 1e-5
)
```

`max = 1e-5` means the jitter is on the order of 0.00001 degrees — negligible in geographic terms. This preserves modelling validity while avoiding duplicated-location errors.

During conversion to geodata format, it was found that 19 data points shared identical coordinates. This is problematic for geostatistical modelling, as duplicated locations can lead to ill-defined variogram structures and singular covariance matrices. To address this, we applied a minimal spatial jitter using `jitterDupCoords()`, introducing negligible noise to break coordinate ties while preserving the underlying spatial pattern.

### Empirical Variogram Estimation

```{r}
#| warning: false
#| message: false
# Empirical variogram with binning
# Full range
emp_variog_full <- variog(kuro_geo_train, option = "bin", max.dist = 2.5, uvec = seq(0, 2.5, length.out = 20))

# Mid-range (preferred candidate for fitting)
emp_variog_2 <- variog(kuro_geo_train, option = "bin", max.dist = 2.0, uvec = seq(0, 2.0, length.out = 20))

# Cleanest for model fitting
emp_variog_1.8 <- variog(kuro_geo_train, option = "bin", max.dist = 1.8, uvec = seq(0, 1.8, length.out = 18))
```

To assess the spatial dependence in SST, the semi-variance is computed as:

$$
\gamma(h) = \frac{1}{2N(h)} \sum_{\substack{i,j: \\ \|s_i - s_j\| \approx h}} \left( z(s_i) - z(s_j) \right)^2
$$

where:

-   $N(h)$ is the number of location pairs separated by distance hhh,

-   \$s_i\$​ and \$s_j\$​ are spatial coordinates of observations,

-   $z$$(s_i)$ is the SST value at location $s_i$​.

The semi-variance $\gamma(h)$ increases with distance $h$ if spatial correlation is present.

```{r}
#| label: fig:variogcompare
#| fig-cap: "Figure 2: Empirical variograms computed using three different maximum distance thresholds. The max.dist = 1.8 version was selected for model fitting due to reduced instability in the tail while preserving the spatial structure."
#| echo: false
#| warning: false
#| message: false
library(tibble)
library(dplyr)

variog_df <- bind_rows(
  tibble(dist = emp_variog_full$u, semivar = emp_variog_full$v, version = "max.dist = 2.5"),
  tibble(dist = emp_variog_2$u, semivar = emp_variog_2$v, version = "max.dist = 2.0"),
  tibble(dist = emp_variog_1.8$u, semivar = emp_variog_1.8$v, version = "max.dist = 1.8")
)


ggplot(variog_df, aes(x = dist, y = semivar)) +
  geom_point(size = 2, colour = "black") +
  geom_line(colour = "darkred", linewidth = 1) +
  facet_wrap(~ version, ncol = 1, scales = "free_x") +
  labs(
    title = "Comparison of Empirical Variograms",
    x = "Distance (spatial units)",
    y = "Semi-variance"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    axis.title = element_text(face = "bold"),
    strip.text = element_text(face = "bold", size = 12),
    panel.grid.major = element_line(color = "grey85"),
    panel.grid.minor = element_blank()
  )
```

Empirical variograms were computed using the `variog()` function in `geoR`, with binned distance lags defined via `uvec`. Three distance thresholds were tested — `max.dist = 2.5`, `2.0`, and `1.8` — to explore how maximum distance cutoff affects stability and interpretability.

#### Evaluation of Distance Cutoffs

Each version of the empirical variogram exhibited the expected monotonic increase with distance, but stability varied across choices:

-   The `max.dist = 2.5` variogram covered the full spatial range but showed noisy tail behaviour due to low bin counts (e.g. 2–7 observations).

-   The `max.dist = 2.0` improved stability but retained some variance at large distances.

-   The `max.dist = 1.8` offered the cleanest structure, with well-populated bins throughout and no extreme tail volatility.

Bin counts were monitored using `emp_variog$n`, and those for the selected `max.dist = 1.8` were generally robust (e.g., \>15 in most bins).

#### Nugget Effect Justification

The variogram curve did not pass through the origin, suggesting a non-zero intercept (nugget). This supports inclusion of a nugget effect in parametric fitting, likely reflecting:

-   Instrument noise

-   Sub-grid-scale oceanic variation

#### Final Selection

The `max.dist = 1.8` empirical variogram was selected for fitting parametric models in Part C2. It achieves a balance between full-range coverage and stable bin-level variance estimation, making it well-suited to weighted least squares variogram fitting.

### Fitting Parametric Variogram Models

```{r}
#| warning: false
#| message: false
#  Fit Parametric Variogram Models
# Exponential model
fit_exp <- variofit(
  emp_variog_1.8,
  cov.model = "exponential",
  ini.cov.pars = c(1, 1),
  nugget = 0.1,
  weights = "equal"
)

# Gaussian model
fit_gau <- variofit(
  emp_variog_1.8,
  cov.model = "gaussian",
  ini.cov.pars = c(1, 1),
  nugget = 0.1,
  weights = "equal"
)

# Adjusted first Matérn model as: sum of the nugget and partial sill initial values was too small. Two new improved models below:

# Matérn model (kappa = 1.5)
fit_mat1 <- variofit(
  emp_variog_1.8,
  cov.model = "matern",
  kappa = 1.5,
  ini.cov.pars = c(2, 1),   # partial sill = 2, range = 1
  nugget = 0.5,             # starting nugget guess
  weights = "equal"
)

fit_mat2 <- variofit(
  emp_variog_1.8,
  cov.model = "matern",
  kappa = 1.5,
  ini.cov.pars = c(1.5, 0.8),
  nugget = 0.3,
  weights = "equal"
)
```

Equal weights were used to avoid overweighting short-distance bins, which typically contain more pairs and could disproportionately influence the fit.

```{r}
#| label: fig:variogfit
#| fig-cap: "Parametric variogram models (Exponential, Gaussian, Matérn) fitted to the empirical variogram with max.dist = 1.8. The Matérn model offered the best fit to the empirical structure and lowest residual sum of squares."
#| echo: false
#| warning: false
#| message: false
# Plot empirical variogram
plot(emp_variog_1.8,
     main = "Fitted Variogram Models (max.dist = 1.8)",
     xlab = "Distance (spatial units)", ylab = "Semi-variance")

# Overlay each fitted model
lines(fit_exp, col = "blue", lwd = 2)
lines(fit_gau, col = "forestgreen", lwd = 2)
lines(fit_mat1, col = "purple", lwd = 2, lty = 2)
lines(fit_mat2, col = "darkorange", lwd = 2, lty = 3)

# Legend for identification
legend("bottomright",
       legend = c("Exponential", "Gaussian", "Matérn (v1)", "Matérn (v2)"),
       col = c("blue", "forestgreen", "purple", "darkorange"),
       lty = c(1, 1, 2, 3),
       lwd = 2)

# Residual sums of squares
fit_exp$value  # 7.123
fit_gau$value  # 6.829
fit_mat1$value # 6.797
```

Parametric variogram models were fitted to the empirical variogram with `max.dist = 1.8` using the `variofit()` function. Three covariance functions were tested:

-   **Exponential**: assumes rough sample paths and rapid correlation decay

-   **Gaussian**: assumes smooth sample paths with strong local correlation

-   **Matérn**: provides a flexible family; here set with κ=1.5\kappa = 1.5κ=1.5 for moderate smoothness

All models assumed:

-   A **constant mean** function (i.e. no trend component)

-   **Isotropy**, meaning spatial correlation depends only on Euclidean distance

-   **Second-order stationarity**

-   A **non-zero nugget**, motivated by the empirical variogram

Each model was fitted using weighted least squares. Initial parameter guesses were based on visual inspection of the empirical variogram:

### **Model Parameters and Interpretation**

```{r}
# Exponential
params_exp <- fit_exp$cov.pars
nugget_exp <- fit_exp$nugget

# Gaussian
params_gau <- fit_gau$cov.pars
nugget_gau <- fit_gau$nugget

# Matérn
params_mat <- fit_mat1$cov.pars
nugget_mat <- fit_mat1$nugget

# Create parameter summary table
param_table <- data.frame(
  Model = c("Exponential", "Gaussian", "Matérn (κ = 1.5)"),
  Nugget = c(nugget_exp, nugget_gau, nugget_mat),
  Partial_Sill = c(params_exp[1], params_gau[1], params_mat[1]),
  Range = c(params_exp[2], params_gau[2], params_mat[2]),
  Residual_SS = c(fit_exp$value, fit_gau$value, fit_mat1$value)
)
```

| Model            | Nugget (τ²) | Partial Sill (σ²) | Range (ϕ) | Residual SS |
|------------------|-------------|-------------------|-----------|-------------|
| Exponential      | 0.000       | 4,208,359         | 2,718,693 | 7.12        |
| Gaussian         | 0.255       | 282.69            | 17.22     | 6.83        |
| Matérn (κ = 1.5) | 0.180       | 26.68             | 3.13      | 6.80        |

**Parametric Variogram Fitting and Selection**

Despite different assumptions, both Matérn and Gaussian produced similar fits. The exponential model showed higher residual error and a nugget of zero, suggesting underestimation of short-scale variation.

The Matérn model was selected for spatial prediction due to its balanced fit across distances and lowest residual sum of squares (6.80). Its parameters suggest a moderate range of spatial correlation (ϕ ≈ 3.13) and a nugget variance of 0.18, indicating non-negligible unexplained microscale variation. This model was used in the kriging stage.

**Spatial Prediction and Model Validation**

```{r}
# Kriging prediction at 5 withheld locations
kriged <- krige.conv(
  geodata = kuro_geo_train,
  locations = test_coords,
  krige = krige.control(
    cov.model = "matern",
    cov.pars = fit_mat1$cov.pars,
    nugget = fit_mat1$nugget,
    kappa = 1.5
  )
)

# Add predicted values and residuals
test_results <- test_coords %>%
  mutate(
    observed_sst = test_true_sst$sst,
    predicted_sst = kriged$predict,
    kriging_var = kriged$krige.var,
    residual = observed_sst - predicted_sst
  )
```

Ordinary kriging assumes a constant spatial mean and was used here given the absence of strong deterministic trends in SST across the study area.

```{r}
#| label: fig:krigscatter
#| fig-cap: "Observed vs predicted sea surface temperature (SST) at five withheld locations using ordinary kriging with the fitted Matérn model. Most points lie near the 1:1 line, though one outlier indicates higher uncertainty."
#| echo: false
#| warning: false
#| message: false

# Visualise prediction accuracy
ggplot(test_results, aes(x = observed_sst, y = predicted_sst)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", colour = "red") +
  labs(
    title = "Observed vs Predicted SST at Withheld Locations",
    x = "Observed SST (°C)",
    y = "Predicted SST (°C)"
  ) +
  theme_minimal(base_size = 13)
```

```{r}
#| message: false
#| label: fig:cvkrig
#| fig-cap: "LOOCV residual diagnostics for the Matérn kriging model (κ = 1.5), showing minimal bias and good predictive alignment."
# Perform LOOCV
xv.kriging <- xvalid(kuro_geo_train, model = fit_mat1)

# Plot residuals
par(mfrow = c(3, 2), mar = c(4, 2, 2, 2))
plot(xv.kriging, error = TRUE, std.error = FALSE, pch = 19)
```

```{r}
#| label: tab:krigsummary
#| tbl-cap: "Summary of SST predictions at withheld locations. Residuals and kriging variances highlight spatial uncertainty and model accuracy."
#| echo: false
#| warning: false
#| message: false

# Compute RMSE and MAE
rmse <- sqrt(mean(test_results$residual^2))
mae <- mean(abs(test_results$residual))

# Summary table
library(knitr)

results_table <- test_results %>%
  mutate(
    `Observed SST (°C)` = round(observed_sst, 2),
    `Predicted SST (°C)` = round(predicted_sst, 2),
    `Residual (°C)` = round(residual, 2),
    `Kriging Variance` = round(kriging_var, 3)
  ) %>%
  select(lon, lat, `Observed SST (°C)`, `Predicted SST (°C)`, `Residual (°C)`, `Kriging Variance`)

kable(results_table, format = "latex", booktabs = TRUE,
      caption = "Observed vs Predicted SST at Withheld Locations")
```

Using the final Matérn variogram model (κ = 1.5), ordinary kriging was performed at five randomly withheld locations. A constant mean was assumed, and predictions were made using the fitted covariance parameters: nugget = 0.18, partial sill = 26.68, and range = 3.13.

Predictive accuracy was evaluated against the observed SSTs, yielding a root mean squared error (RMSE) of 3.49 °C and mean absolute error (MAE) of 2.11 °C. As shown in Figure \@ref(fig:krigscatter), most predictions aligned with observations, except for one large residual at a high-variance site. This reflects the model’s ability to express spatial uncertainty through the kriging variance.

The model captured the spatial SST structure well and provided meaningful uncertainty estimates. Further improvements could include denser sampling or Bayesian spatial models to better propagate uncertainty and improve prediction at poorly supported locations.

## Part D: Gaussian Process via Maximum Likelihood

### Model Setup and Fitting

We now fit a spatial Gaussian Process (GP) model to the training dataset using maximum likelihood estimation. This approach directly maximises the log-likelihood of the spatial model, as opposed to the weighted least squares (WLS) method used in variogram fitting.

The Matérn covariance function with κ = 1.5 was retained from Part C due to its strong fit and interpretability. The `likfit()` function in the `geoR` package was used to estimate the nugget, partial sill, and range parameters.

#### Model Setup and Attempted Optimisation

To fit a Gaussian Process (GP) model via maximum likelihood, the likfit() function from the geoR package was applied to the same training dataset used in Part C. The goal was to estimate the spatial covariance parameters — partial sill, range, and nugget — directly by maximising the full likelihood over all observations, as opposed to the weighted least squares approach used in variogram fitting.

A series of attempts were made to improve or stabilise the model fit:

\- Fixing the nugget value (e.g., nugget = 0.2, nugget = 0.3) repeatedly led to numerical singularity in the variance-covariance matrix.

\- Introducing a first-order or second-order trend component (e.g., trend = "1st" or "2nd") caused matrix inversion failures due to collinearity and overparameterisation.

\- Explicitly setting the covariance model to Matérn with kappa = 1.5 frequently triggered decomposition errors, despite being theoretically appropriate.

Ultimately, the only configuration that converged successfully used the most minimal and default structure:

\- A constant mean function (default trend = "cte"),

\- Unspecified covariance model and kappa (which defaults to Matérn with kappa = 0.5, i.e., the exponential model). Note that the default covariance model in `likfit()` is the Matérn family with fixed κ = 0.5, corresponding to the exponential model.

\- Automatic nugget estimation.

This resulted in a valid and stable model:

```{r}
#| message: false
# Fit spatial GP model via MLE using default exponential covariance
fit_gp <- likfit(
  kuro_geo_train,
  ini.cov.pars = c(26, 4)
)

fit_gp
```

The fitted model yielded the following parameter estimates:

\- Mean (β): 15.99

\- Nugget (τ²): 0.0067

\- Partial Sill (σ²): 8.34

\- Range (φ): 3.9996

\- Practical Range (cor ≈ 0.05): 11.98 spatial units

\- Maximised log-likelihood: –61.54

Compared to the kriging model from Part C, which used a Matérn model with κ = 1.5, nugget = 0.18, sill = 26.68, and range = 3.13, the MLE-based GP model estimated a much smaller nugget and sill, and a longer spatial range. Although the fitted GP used a slightly different covariance assumption (Matérn with κ = 0.5), it still captured the dominant spatial structure. This provides a useful benchmark for comparing inference and prediction against both classical kriging and the Bayesian model in Part D2.

Model Validation

```{r}
#| label: fig:cvgp
#| fig-cap: "LOOCV residual plots for the GP model fitted via maximum likelihood, showing broadly unbiased predictions with slightly greater residual spread."
# Perform LOOCV
xv.gp <- xvalid(kuro_geo_train, model = fit_gp)

# Plot residuals
par(mfrow = c(3, 2), mar = c(4, 2, 2, 2))
plot(xv.gp, error = TRUE, std.error = FALSE, pch = 19)
```

#### Model Output

The maximum likelihood estimation returned updated estimates for the spatial covariance parameters. These will now be used to make predictions at the same five withheld test locations used in Part C.

#### GP Prediction at Withheld Locations

Unlike the variogram-based kriging approach in Part C, which fits the spatial correlation structure via weighted least squares, the GP model in Part D maximises the full multivariate Gaussian likelihood. This accounts for spatial correlation among all data points simultaneously, improving parameter coherence.

Predictions were made at the five withheld locations using `krige.conv()` with the MLE-fitted covariance parameters, enabling fully probabilistic interpolation under the GP model.

```{r}
#| message: false
# Kriging prediction using GP mode
pred_gp <- krige.conv(
  geodata = kuro_geo_train,
  locations = test_coords,
  krige = krige.control(
    obj.model = fit_gp
  )
)

# Combine predictions with actual values
gp_results <- test_coords %>%
  mutate(
    observed_sst = test_true_sst$sst,
    predicted_sst = pred_gp$predict,
    kriging_var = pred_gp$krige.var,
    residual = observed_sst - predicted_sst
  )
```

```{r}
#| label: fig:gp_pred_scatter
#| fig-cap: "Observed vs predicted SST at withheld locations using the Gaussian Process model (maximum likelihood). The red dashed line shows the 1:1 agreement."
#| echo: false
#| warning: false
#| message: false
# Plot: Observed vs Predicted
ggplot(gp_results, aes(x = observed_sst, y = predicted_sst)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Observed vs Predicted SST (GP Model)",
    x = "Observed SST (°C)",
    y = "Predicted SST (°C)"
  ) +
  theme_minimal(base_size = 13)
```

```{r}
#| label: tab:gp_krigsummary
#| tbl-cap: "Observed vs Predicted SST at Withheld Locations – GP Model"
#| echo: false
#| warning: false
#| message: false

# Compute error metrics
rmse_gp <- sqrt(mean(gp_results$residual^2))
mae_gp <- mean(abs(gp_results$residual))

library(tibble)
library(gt)

# Create a named summary table
tibble(
  Metric = c("RMSE", "MAE"),
  Value = c(rmse_gp, mae_gp)
) %>%
  gt() %>%
  fmt_number(columns = "Value", decimals = 3) %>%
  tab_header(
    title = "Gaussian Process Model Performance",
    subtitle = "Prediction Error Metrics on Withheld Data"
  ) %>%
  tab_options(
    table.font.size = "small",
    data_row.padding = px(3)
  )

# Create evaluation table
library(knitr)
gp_results %>%
  mutate(
    `Observed SST (°C)` = round(observed_sst, 2),
    `Predicted SST (°C)` = round(predicted_sst, 2),
    `Residual (°C)` = round(residual, 2),
    `Kriging Variance` = round(kriging_var, 3)
  ) %>%
  select(lon, lat, `Observed SST (°C)`, `Predicted SST (°C)`, `Residual (°C)`, `Kriging Variance`) %>%
  kable(format = "latex", booktabs = TRUE, caption = "Observed vs Predicted SST at Withheld Locations – GP Model")



```

#### Interpretation

Using the Gaussian Process model fitted via maximum likelihood, SST predictions were made at the same five withheld locations used in Part C. **Unlike variogram kriging, this method estimates spatial parameters by maximising the full joint likelihood, leveraging spatial correlation between all observations simultaneously.** Figure \@ref(fig:gp_pred_scatter) displays the predicted versus observed values, while Table \@ref(tab:gp_krigsummary) reports the predicted SSTs, residuals, and associated kriging variances.

The GP model achieved a root mean squared error (RMSE) of **3.01 °C** and a mean absolute error (MAE) of **1.96 °C**, both slightly improved relative to the variogram-based model. **Notably, both models underperformed at a high-variance location (kriging variance = 2.71), indicating limitations driven by weak local data support.**

**Despite using a default Matérn κ = 0.5 (exponential) covariance structure, the MLE-fitted model captured the main spatial structure effectively and required fewer tuning steps.** This aligns with the spatial distribution of errors and supports the model’s probabilistic reliability.

One limitation is the lack of flexibility: the Matérn model from Part C was better able to capture longer-range spatial structure. Additionally, the GP model struggled to converge under more complex assumptions, limiting experimentation.

Overall, the GP model offered competitive accuracy and uncertainty quantification, making it a robust alternative to traditional variogram-based kriging. **While the kriging approach provides transparent semi-variance interpretation, the GP model delivers a principled statistical framework with strong performance and consistency.**

## Part E:

#### Bayesian Parameter Estimation with Discrete Priors

We estimate the parameters of a spatial Gaussian Process model using a Bayesian approach via the `krige.bayes()` function in the `geoR` package. This method uses discrete priors and computes the posterior distribution over spatial parameters by evaluating all combinations within a user-defined grid. As in Part C, we assume a Matérn covariance structure with smoothness parameter κ = 1.5 and a constant mean function. This model structure was selected due to its good empirical fit to the empirical variogram and compatibility with `krige.bayes()`'s variogram-style interface.

Although maximum likelihood estimates were obtained in Part D, the model used there relied on `likfit()` and a default exponential structure (κ = 0.5) due to convergence issues. In contrast, the Bayesian framework requires manual specification of the covariance model, and is more naturally aligned with the Matérn structure successfully fitted in Part C.

#### Prior Specification and Justification

We placed discrete priors on two key hyperparameters: the correlation range (φ) and the nugget effect (τ²). Prior ranges were informed by the maximum likelihood estimates obtained in Part D, where φ ≈ 4.00 and the nugget comprised a very small fraction of the total variance (τ² ≈ 0.0067, σ² ≈ 8.34). Specifically, we defined:

-   A **reciprocal prior** over φ ∈ \[2, 6\], discretised into 50 values. This reflects prior belief that shorter spatial correlation lengths are more plausible, while still allowing exploration of moderate ranges.
-   A **uniform prior** on the relative nugget τ² / (σ² + τ²), defined over the interval \[0.01, 0.3\] using 50 discrete bins.

The partial sill (σ²) was held fixed at 8.34 for computational stability and identifiability.

#### Model Stability Adjustment

An initial attempt using a wider nugget prior range (from 0 to 1) resulted in numerical errors due to near-singular covariance matrices. To address this, the lower bound of the nugget prior was increased to 0.01 and the upper bound reduced to 0.3. This ensured numerical stability while preserving model flexibility.

```{r}
#| include: true
#| results: hide
#| warning: false
#| message: false

set.seed(444)

bayes_model <- krige.bayes(
  geodata = kuro_geo_train,
  model = model.control(cov.model = "matern", kappa = 1.5),
  prior = prior.control(
    phi.discrete = seq(2, 6, l = 50),
    phi.prior = "reciprocal",
    tausq.rel.discrete = seq(0.01, 0.3, l = 50),
    tausq.rel.prior = "unif"
  )
)
summary(bayes_model$posterior$sample)
```

#### Posterior Results and Parameter Comparison

Posterior inference was conducted over 2,500 combinations of φ and relative nugget. The highest posterior density occurred at:

-   φ = 2.00\
-   τ² / (σ² + τ²) = 0.01

This combination received the most support (292 out of 2,500 samples), indicating strong posterior belief in short-range correlation and a negligible nugget effect.

Summary statistics of the posterior distribution (from `bayes_model$posterior$sample`) reinforce this interpretation:

-   **Range (φ):** Median = 2.08, Mean = 2.15 — indicating moderate spatial correlation, slightly shorter than the MLE estimate (φ ≈ 4.00) from Part D.
-   **Relative Nugget (τ² / (σ² + τ²)):** Median = 0.01, Mean = 0.011 — suggesting very low unexplained microscale variability, in line with both the Part C and Part D models.
-   **Partial Sill (σ²):** Mean ≈ 23.12, slightly higher than in the MLE model (σ² ≈ 8.34), possibly compensating for the shorter range estimate.
-   **Mean (β):** Median ≈ 16.64 — consistent with the SST level expected across the region.

Compared to the MLE-based GP model in Part D, the Bayesian model estimated a slightly higher partial sill (23.1 vs. 8.3) and a shorter correlation range (φ ≈ 2.15 vs. 4.00). The nugget proportion remained small, indicating limited microscale variability. Overall, the posterior distributions concentrate around stable, interpretable values, with minimal spread — a sign of informative data and appropriate prior design.

#### Prediction at Withheld Locations

Bayesian kriging was performed at the same five withheld SST locations used in Parts C and D. Posterior predictive means and variances were extracted, and evaluation metrics were computed:

```{r}
#| include: true
#| results: hide
#| warning: false
#| message: false
test_coords_df <- as_tibble(test_coords)


# With predictions
bayes_model <- krige.bayes(
  geodata = kuro_geo_train,
  locations = test_coords,
  model = model.control(cov.model = "matern", kappa = 1.5),
  prior = prior.control(
    phi.discrete = seq(2, 6, l = 50),
    phi.prior = "reciprocal",
    tausq.rel.discrete = seq(0.01, 0.3, l = 50),
    tausq.rel.prior = "unif"
  ))


# Summarise predictions
bayes_results <- test_coords_df %>%
  mutate(
    observed_sst = test_true_sst$sst,
    predicted_sst = bayes_model$predictive$mean,
    kriging_var = bayes_model$predictive$variance,
    residual = observed_sst - predicted_sst
  )

# Compute error metrics
rmse_bayes <- sqrt(mean(bayes_results$residual^2))
mae_bayes <- mean(abs(bayes_results$residual))
```

```{r}
#| echo: true
# Output results
rmse_bayes
mae_bayes
```

LOOCV diagnostics are not available for the Bayesian kriging model due to the discrete posterior sampling framework, which does not support leave-one-out cross-validation via `xvalid()`.

```{r}
#| label: fig:bayes_pred_scatter
#| fig-cap: "Observed vs predicted SST at withheld locations using the Gaussian Process model (maximum likelihood). The red dashed line shows the 1:1 agreement."
#| echo: false
# Bayesian observed vs predicted plot
ggplot(bayes_results, aes(x = observed_sst, y = predicted_sst)) +
  geom_point(size = 3) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", colour = "red") +
  labs(
    title = "Observed vs Predicted SST (Bayesian Model)",
    x = "Observed SST (°C)",
    y = "Predicted SST (°C)"
  ) +
  theme_minimal(base_size = 13)

```

```{r}
#| label: tab:bayessummary
#| tbl-cap: "Summary of SST predictions at withheld locations. Residuals and kriging variances highlight spatial uncertainty and model accuracy."
#| echo: false
#| warning: false
#| message: false
# Output prediction table
bayes_results %>%
  mutate(
    `Observed SST (°C)` = round(observed_sst, 2),
    `Predicted SST (°C)` = round(predicted_sst, 2),
    `Residual (°C)` = round(residual, 2),
    `Kriging Variance` = round(kriging_var, 3)
  ) %>%
  select(lon, lat, `Observed SST (°C)`, `Predicted SST (°C)`, `Residual (°C)`, `Kriging Variance`) %>%
  knitr::kable(format = "latex", booktabs = TRUE,
               caption = "Observed vs Predicted SST at Withheld Locations – Bayesian Model")

```

The predicted SSTs largely follow the 1:1 reference line, confirming reasonable accuracy. One notable residual of -7.29°C occurred at the location with the highest kriging variance, reinforcing the relationship between data density and uncertainty.

#### Model Interpretation

The Bayesian model offered competitive predictive performance (RMSE = 3.50°C, MAE = 2.14°C), close to the results from Part D. While it did not dramatically outperform the MLE approach, it introduced full posterior distributions over parameters and predictive uncertainty — an important advantage when quantifying inferential risk.

LOOCV could not be performed, as the `krige.bayes()` framework does not support this due to its reliance on discrete posterior sampling. Nevertheless, the posterior predictive summaries and residual plots indicate unbiased performance and sensible uncertainty estimates.

## Part F: Comparison of Predictions Across Models

The three models developed — classical kriging (Part C), Gaussian process via maximum likelihood (Part D), and Bayesian kriging with discrete priors (Part E) — were used to predict sea surface temperature (SST) at the same five withheld locations. The predictions, associated residuals, and kriging variances are summarised below:

#### Table: Predicted SST and Residuals from All Models

| Location        | Observed SST (°C) | Kriging (C)   | GP (D)        | Bayesian (E)  |
|-----------------|-------------------|---------------|---------------|---------------|
| (142.10, 38.70) | 6.5               | 6.50 (0.00)   | 6.50 (0.00)   | 6.52 (–0.02)  |
| (145.40, 39.56) | 6.5               | 13.83 (–7.33) | 12.40 (–5.90) | 13.79 (–7.29) |
| (149.56, 30.15) | 19.3              | 19.32 (–0.02) | 19.33 (–0.03) | 19.32 (–0.02) |
| (140.70, 35.00) | 18.2              | 15.57 (2.63)  | 15.80 (2.40)  | 15.44 (2.76)  |
| (142.10, 38.30) | 8.0               | 7.43 (0.57)   | 7.55 (0.45)   | 7.38 (0.62)   |

**Note**: Residuals are shown in parentheses.

#### Performance Comparison

| Metric    | Kriging (C) | GP (D) | Bayesian (E) |
|-----------|-------------|--------|--------------|
| RMSE (°C) | 3.49        | 2.86   | 3.50         |
| MAE (°C)  | 2.11        | 1.76   | 2.14         |

#### Interpretation

-   **All three models** captured the dominant SST spatial structure, with similar predictions at well-supported locations (e.g. Locations 1, 3, and 5).

-   **GP via MLE (Part D)** slightly outperformed the others, achieving the lowest RMSE and MAE, likely due to its direct likelihood-based parameter estimation.

-   **Bayesian kriging (Part E)** achieved comparable accuracy while providing posterior uncertainty estimates — a useful advantage when probabilistic inference is needed.

-   All models **underperformed** at Location 2, where kriging variances were highest. This consistent error highlights a location with sparse local support.

#### Conclusion

Despite differing in estimation strategy, all three models produced consistent SST predictions and residual structures. The GP model offered the best balance between fit and computational simplicity, while the Bayesian approach provided richer uncertainty characterisation. These findings highlight trade-offs between interpretability, flexibility, and predictive precision in spatial modelling.

\newpage

# The Atlantic Overturning Circulation

## Part A: Data Exploration

To begin our analysis of the Atlantic Meridional Overturning Circulation (AMOC) at 26°N, we conduct an exploratory analysis of the monthly mean values from **October 2017 to February 2023**. These values represent the strength of the overturning current in Sverdrups (Sv), and are visualised in the figure below.

```{r}
#| label: fig:monthly amoc
#| fig-cap: "Monthly AMOC time series with LOESS trend (Oct 2017 – Feb 2023)"
#| echo: false
#| warning: false
#| message: false
# Load and plot the data
load("~/GitHub/university-projects/Modelling in Space and Time/In progress/MOC.RData")


# --- Convert MOCmean to a tidy data frame ---
library(tidyverse)
library(lubridate)
library(ggplot2)
# Extract names and values
moc_values <- as.numeric(MOCmean)
moc_dates <- names(MOCmean)

# Fix formatting of date strings    
moc_dates_fixed <- ifelse(nchar(moc_dates) == 6,
                          paste0(substr(moc_dates, 1, 5), "0", substr(moc_dates, 6, 6)),
                          moc_dates)

# Convert to actual Date objects (use first of each month)
moc_dates_parsed <- ym(moc_dates_fixed)

# Create tidy tibble
moc_df <- tibble(
  date = moc_dates_parsed,
  amoc = moc_values
) %>% arrange(date)

# --- Plot ---
library(ggplot2)

ggplot(moc_df, aes(x = date, y = amoc)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_smooth(method = "loess", span = 0.2, se = TRUE, color = "darkred", linetype = "dashed") +
  labs(
    title = "Atlantic Meridional Overturning Circulation (AMOC) at 26°N",
    subtitle = "Monthly Mean Values from October 2017 to February 2023",
    x = "Date",
    y = "AMOC Strength (Sv)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_x_date(date_breaks = "4 months", date_labels = "%b %Y") +
  geom_point(size = 1.5, alpha = 0.7)
```

The AMOC time series exhibits notable **short-term variability** around a relatively stable long-term mean. The dashed LOESS trend line captures fluctuations that reflect short-term anomalies and possible intra-annual structure. While there is no pronounced long-term trend, localised peaks and troughs occur — notably in **early 2018**, **late 2020**, and **early 2023**, with **dips in mid-2021 and late 2022**. These observations suggest the possible presence of a **weak seasonal or cyclical component**, which will be explored in subsequent modelling.

To further investigate the distributional properties of the series, we consider the histogram and density plot shown in Figure 2.

```{r}
#| label: fig:AMOC Histo
#| fig-cap: "Distribution of AMOC Strength"
#| echo: false
#| warning: false
#| message: false
ggplot(moc_df, aes(x = amoc)) +
  geom_histogram(aes(y = ..density..), fill = "lightblue", colour = "black", bins = 15) +
  geom_density(colour = "darkred", size = 1) +
  labs(title = "Distribution of AMOC Strength",
       x = "AMOC Strength (Sv)", y = "Density") +
  theme_minimal()

```

The distribution is approximately symmetric and unimodal, with a central peak around **16–17 Sv**. The density curve closely resembles a Gaussian shape but with **slight right tail elongation**, consistent with the **slight negative skewness** observed in the summary statistics below.

```{r}
#| label: tab:AMOC Summary Stats
#| tab-cap: "Summary Statistics of AMOC (Oct 2017 – Feb 2023)h"
#| echo: false
library(dplyr)
library(gt)
library(moments)

# Enhanced summary
amoc_summary <- moc_df %>%
  summarise(
    Mean = mean(amoc),
    SD = sd(amoc),
    Min = min(amoc),
    Max = max(amoc),
    Median = median(amoc),
    IQR = IQR(amoc),
    CV = sd(amoc) / mean(amoc),
    Skewness = skewness(amoc),
    Kurtosis = kurtosis(amoc),
    N = n()
  ) %>%
  pivot_longer(everything(), names_to = "Statistic", values_to = "Value")

# Display table
gt(amoc_summary) %>%
  fmt_number(columns = "Value", decimals = 2) %>%
  tab_header(
    title = "Expanded Summary Statistics of AMOC Time Series",
    subtitle = "October 2017 – February 2023"
  ) %>%
  tab_options(
    table.font.size = "smaller",        # smaller overall font
    data_row.padding = px(2),           # tighter row padding
    heading.padding = px(3),            # tighter heading spacing
    row_group.padding = px(2)
  )
```

The mean overturning strength is **16.81 Sv**, with a standard deviation of **2.93 Sv**, suggesting moderate dispersion. The **coefficient of variation (CV)** is low (0.17), indicating that relative variability is limited. The **interquartile range (IQR)** of **3.37 Sv** further confirms that most monthly values lie within a narrow range. The **kurtosis value of 3.54** suggests heavier tails than a normal distribution, but this is mild. The data do not exhibit any significant skewness (**−0.24**), further justifying Gaussian modelling assumptions.

Overall, these insights provide strong justification for fitting **weakly stationary time series models** (ARMA/ARIMA), possibly with short memory and mild seasonal structure. The stationarity and homoscedasticity assumptions appear reasonable based on the exploratory findings.

## Part B

We investigate suitable ARMA and ARIMA models for the monthly Atlantic Meridional Overturning Circulation (AMOC) time series. The series was converted to a `ts` object with frequency 12, and the final 8 months (July 2022–Feb 2023) were held out for validation. This gives a training window from October 2017 to June 2022.

#### Exploratory Diagnostics

To assess the autocorrelation structure, we examine the ACF and PACF of the training data:

```{r}
#| label: fig:AMF+PACF
#| fig-cap: "ACF and PACF of Monthly AMOC"
#| echo: true
#| warning: false
#| message: false

amoc_ts <- ts(moc_df$amoc, start = c(2017, 10), frequency = 12)

# Truncate last 8 months (keep for later forecasting)
train_ts <- window(amoc_ts, end = c(2022, 6))  # Leaves Oct 2017–June 2022
test_ts <- window(amoc_ts, start = c(2022, 7)) # July 2022–Feb 2023

# Plot training data
# plot(train_ts, main = "Training Data: Monthly AMOC (Oct 2017 – Jun 2022)",
     # ylab = "AMOC Strength (Sv)", xlab = "Year", col = "steelblue", lwd = 2)

# ACF and PACF
par(mfrow = c(1, 2))  # 1 row, 2 columns

acf(train_ts, main = "ACF of AMOC (Training Set)")
pacf(train_ts, main = "PACF of AMOC (Training Set)")

par(mfrow = c(1, 1))  # reset
```

The ACF shows a slow decay from a strong lag-1 autocorrelation (\> 0.9), suggesting non-stationarity. The PACF cuts off sharply after lag 1, consistent with short-term AR(1) structure. Based on this, we apply first-order differencing:

#### Transforming for Stationarity

```{r}
#| label: fig:diff-acf-pacf
#| fig-cap: "ARIMA differencing ACF/PACF"
#| echo: false
#| warning: false
#| message: false
# First-order differencing
diff_amoc <- diff(train_ts)

# Plot layout: 1 row, 3 plots
par(mfrow = c(1, 3), mar = c(4, 4, 3.5, 1))  # slightly increase top margin for spacing

# Plot 1: Differenced Series
plot(diff_amoc, 
     main = "First-order Differenced AMOC", 
     ylab = "Differenced Value", 
     xlab = "Time", 
     col = "steelblue", 
     lwd = 1.2)

# Plot 2: ACF
acf(diff_amoc, 
    main = "ACF of Differenced Series", 
    col = "black")

# Plot 3: PACF
pacf(diff_amoc, 
     main = "PACF of Differenced Series", 
     col = "black")
```

Visual inspection of the first-order differenced AMOC series (Figure \@ref(fig:diff-acf-pacf)) indicates improved stationarity, with fluctuations more stable around a constant mean. The autocorrelation function (ACF) decays rapidly and remains within the 95% bounds, while the partial autocorrelation function (PACF) suggests short memory dependence. These features are indicative of a stationary series, justifying the use of ARIMA(\$p\$,1,\$q\$) models with first-order differencing (\$d = 1\$).

#### **Model Rationale and Modelling Strategy**

To capture the temporal structure of the Atlantic Meridional Overturning Circulation (AMOC), we consider both **ARMA** and **ARIMA** models. These linear time series models are widely used in climatological applications to model autocorrelation and generate forecasts.

Since the **ACF displays strong persistence at lag 1 and decays slowly**, the series is likely **non-stationary**, suggesting the presence of a stochastic trend. The **PACF cuts off sharply after lag 1**, indicating short-term autoregressive dependence. Based on this, we apply **first-order differencing** to achieve approximate stationarity and proceed to fit **ARIMA(p,1,q)** models.

We adopt a two-pronged approach:

-   **ARMA models** on the original series, to serve as a baseline.

-   **ARIMA models** on the differenced series, to handle non-stationarity.

Candidate models:

-   ARMA(1,0) and ARMA(2,0)

-   ARIMA(1,1,0), ARIMA(0,1,1), ARIMA(1,1,1)

Models are fitted via maximum likelihood using `arima()`, and compared using AIC, residual diagnostics, and Ljung–Box tests.

The following candidate models are selected based on ACF/PACF patterns and parsimony:

-   **ARMA(1,0)** and **ARMA(2,0)**: Autoregressive models without differencing.

-   **ARIMA(1,1,0)**, **ARIMA(0,1,1)**, and **ARIMA(1,1,1)**: First-order differenced models with varying AR/MA terms.

All models are estimated using **maximum likelihood** via the `arima()` function in R. Selection and comparison will be based on **Akaike Information Criterion (AIC)**, residual diagnostics, and **forecast performance on the withheld 8-month test set**.

#### ARMA Model Fitting (Undifferenced Series)

```{r}
# ARMA Models
arma_10 <- arima(train_ts, order = c(1,0,0), method = "ML")
arma_20 <- arima(train_ts, order = c(2,0,0), method = "ML")
```

The ARMA(1,0) model estimated the following relationship:

$$
X_t = 16.88 + 0.28 X_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim \mathcal{N}(0, \sigma^2)
$$ The ARMA(1,0) model estimated the following relationship: $$
X_t = 16.90 + 0.34 X_{t-1} - 0.21 X_{t-2} + \varepsilon_t, \quad \varepsilon_t \sim \mathcal{N}(0, \sigma^2)
$$

```{r}
#| label: fig:arma-diagnostics
#| fig-cap: "Residual Diagnostics for ARMA(1,0) and ARMA(2,0)"
#| warning: false
#| message: false

par(mfrow = c(2,3), mar = c(4,4,2,1))
plot(residuals(arma_10), main = "ARMA(1,0) Residuals")
acf(residuals(arma_10), main = "ARMA(1,0) ACF")
qqnorm(residuals(arma_10)); qqline(residuals(arma_10))

plot(residuals(arma_20), main = "ARMA(2,0) Residuals")
acf(residuals(arma_20), main = "ARMA(2,0) ACF")
qqnorm(residuals(arma_20)); qqline(residuals(arma_20))
par(mfrow = c(1,1))
```

The **ARMA(1,0)** model yielded an AIC of 286.18. The residuals appear centred and roughly homoscedastic, but the ACF reveals mild autocorrelation at low lags. The Ljung–Box test returned a p-value above 0.05, indicating no statistically significant autocorrelation. The Q–Q plot suggests approximate normality, though with slight tail deviations.

The **ARMA(2,0)** model slightly improved the fit, reducing AIC to 285.65. Its residual ACF falls well within the 95% bounds, and the Q–Q plot shows improved linearity. The Ljung–Box p-value increased to 0.26, indicating weaker residual dependence.

#### Model Comparison

| Model | AIC | AR Coefficients | Ljung–Box p-value | Residual Summary |
|----|----|----|----|----|
| ARMA(1,0) | 286.18 | AR(1) = 0.2807 | \> 0.24 | Some residual autocorrelation |
| ARMA(2,0) | 285.65 | AR(1) = 0.342, AR(2) = -0.209 | 0.26 | Slightly improved white noise |

While ARMA(2,0) provides a marginal improvement, both models remain constrained by the assumption of stationarity. Given the **stochastic trend** and **persistent autocorrelation** in the original series, we now proceed to fit **ARIMA models** that incorporate **first-order differencing** to better capture non-stationary behaviour.

#### ARIMA Model Fitting (Differenced Series)

```{r}
arima_110 <- arima(train_ts, order = c(1,1,0), method = "ML")
arima_011 <- arima(train_ts, order = c(0,1,1), method = "ML")
arima_111 <- arima(train_ts, order = c(1,1,1), method = "ML")
```

```{r}
#| label: fig:arima-diagnostics
#| fig-cap: "Residual diagnostics for candidate ARIMA models"
#| message: false
#| warning: false
#| echo: false
#| fig-width: 12
#| fig-height: 6

# Diagnostic plotting function
plot_diagnostics <- function(model, model_name, col_line) {
  layout(matrix(1:3, 1, 3, byrow = TRUE), widths = c(1.4, 1, 1))  # Widen residual plot
  par(mar = c(4, 4, 3, 1))  # Margins: bottom, left, top, right
  
  # Residual time series
  plot(residuals(model), type = "l", col = col_line, lwd = 1.2,
       main = paste(model_name, "Residuals"), ylab = "Residuals", xlab = "Time")
  
  # ACF
  acf(residuals(model), main = "ACF of Residuals", col = "black", lwd = 1.2)
  
  # Q–Q plot
  qqnorm(residuals(model), main = "Normal Q–Q Plot")
  qqline(residuals(model), col = "red", lwd = 1.2)
  
  layout(1)  # Reset layout
}

# Call function for each ARIMA model
plot_diagnostics(arima_110, "ARIMA(1,1,0)", "steelblue")
plot_diagnostics(arima_011, "ARIMA(0,1,1)", "darkred")
plot_diagnostics(arima_111, "ARIMA(1,1,1)", "darkgreen")

```

```{r}
#| echo: false
#| warning: false
#| message: false

# ---- Ljung–Box Tests ----
lb_110 <- Box.test(residuals(arima_110), lag = 10, type = "Ljung-Box")
lb_011 <- Box.test(residuals(arima_011), lag = 10, type = "Ljung-Box")
lb_111 <- Box.test(residuals(arima_111), lag = 10, type = "Ljung-Box")

knitr::kable(data.frame(
  Model = c("ARIMA(1,1,0)", "ARIMA(0,1,1)", "ARIMA(1,1,1)"),
  AIC = c(AIC(arima_110), AIC(arima_011), AIC(arima_111)),
  Ljung_Box_p = c(lb_110$p.value, lb_011$p.value, lb_111$p.value),
  Notes = c("Residual autocorr.", "Good fit", "Best fit")
), caption = "Comparison of ARIMA Models")

```

#### Findings:

-   The **ARIMA(1,1,0)** model exhibited a strong Q–Q plot with minimal tail deviation; however, its **Ljung–Box p-value was 0.005**, indicating significant residual autocorrelation and a poor overall fit (**AIC = 301.4**).

-   In contrast, **ARIMA(0,1,1)** provided a substantial improvement, reducing AIC to **286.6** and eliminating most residual autocorrelation (**p = 0.14**), while maintaining reasonably normal residuals.

-   The best-performing model was **ARIMA(1,1,1)**, which achieved the **lowest AIC (285.1)**, and passed diagnostic checks with approximately white residuals (**p = 0.13**) and a nearly linear Q–Q plot. This model strikes the best balance between parsimony and fit, and is selected as the **benchmark for subsequent comparison**.

#### Model Refinement and Justification

Although ARIMA(1,1,1) performs well, **minor residual autocorrelation** and slight non-normality remain. To assess whether these artefacts reflect underfitting, we explore two higher-order models: **ARIMA(2,1,0)** and **ARIMA(2,1,2)**.

This refinement is motivated by:

-   **Persistent low-lag structure** in the differenced ACF/PACF plots

-   The potential for **medium-term dependence** not captured by first-order terms

-   Precedent from similar climatological time series, where **ARIMA(2,1,2)** often improves predictive performance.

```{r}
arima_210 <- arima(train_ts, order = c(2,1,0), method = "ML")
arima_212 <- arima(train_ts, order = c(2,1,2), method = "ML")

lb_210 <- Box.test(residuals(arima_210), lag = 10, type = "Ljung-Box")
lb_212 <- Box.test(residuals(arima_212), lag = 10, type = "Ljung-Box")
```

```{r}
#| echo: false
#| warning: false
#| message: false

knitr::kable(data.frame(
  Model = c("ARIMA(2,1,0)", "ARIMA(2,1,2)"),
  AIC = c(AIC(arima_210), AIC(arima_212)),
  Ljung_Box_p = c(lb_210$p.value, lb_212$p.value),
  Notes = c("No clear improvement", "Slight AIC gain, more complex")
), caption = "Refined ARIMA Models")

```

As shown in Table \@ref(tab:refined-arima-models), **ARIMA(2,1,2)** achieved a modest reduction in AIC compared to ARIMA(1,1,1), and both refined models returned Ljung–Box p-values above 0.3, suggesting no significant autocorrelation in the residuals.

While ARIMA(2,1,2) performs best by AIC, the improvement over ARIMA(1,1,1) is **minor**, and comes at the cost of a **more complex structure**. Given the principle of model parsimony, and the lack of clear diagnostic benefit, **ARIMA(1,1,1)** is retained as the preferred model.

Through ACF/PACF analysis and iterative model fitting, we identified ARIMA(1,1,1) as the most appropriate model for the monthly AMOC series. While higher-order alternatives offered marginal AIC improvements, residual diagnostics and parsimony considerations supported retention of ARIMA(1,1,1).

## Part C: Quarterly Modelling of AMOC

To explore lower-frequency dynamics in the AMOC time series, the data is aggregated from monthly to quarterly averages. This aligns with climatological practice where quarterly data can help reveal medium-term structure by smoothing high-frequency noise.

```{r}
library(dplyr)
library(zoo)

moc_df_q <- moc_df %>%
  mutate(quarter = as.yearqtr(date)) %>%
  group_by(quarter) %>%
  summarise(amoc_q = mean(amoc, na.rm = TRUE)) %>%
  ungroup()
```

The quarterly data is then converted to a time series object (frequency = 4) spanning 2017 Q1 to 2023 Q1. The final two quarters are withheld for out-of-sample forecast validation.

```{r}
# Create quarterly time series
amoc_q_ts <- ts(moc_df_q$amoc_q, start = c(2017, 3), frequency = 4)

# Training = up to 2022 Q2
train_q_ts <- window(amoc_q_ts, end = c(2022, 3))

# Testing = 2022 Q3 and 2022 Q4
test_q_ts <- window(amoc_q_ts, start = c(2022, 4), end = c(2023, 1))

```

### ACF and PACF of Quarterly AMOC

```{r}
#| label: fig:acf-pacf-quarterly
#| fig-cap: "ACF and PACF of Quarterly AMOC (Training Set)"
#| echo: true
#| message: false
#| warning: false

par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

acf(train_q_ts, main = "ACF - Quarterly AMOC", lag.max = 20)
pacf(train_q_ts, main = "PACF - Quarterly AMOC", lag.max = 20)

par(mfrow = c(1, 1))  # Reset layout
```

The autocorrelation structure of the quarterly AMOC series is shown in Figure \@ref(fig:acf-pacf-quarterly).

-   The ACF exhibits a very strong spike at lag 1, followed by a gradual decay, indicating the presence of a stochastic trend and non-stationarity.

-   The PACF shows a large spike at lag 1 and a second minor spike at lag 2 — characteristic of short-memory autoregressive dependence, possibly AR(2).

This behaviour justifies applying a first-order differencing transformation to stabilise the mean and induce stationarity.

#### First-Order Differencing of Quarterly Series

```{r}
# First-order differencing
diff_q <- diff(train_q_ts)

# Plot ACF and PACF of differenced series
par(mfrow = c(1, 2), mar = c(4, 4, 3, 2))
acf(diff_q, main = "ACF - Differenced Quarterly AMOC")
pacf(diff_q, main = "PACF - Differenced Quarterly AMOC")
par(mfrow = c(1, 1))
```

The **ACF** plot displays a prominent spike at lag 1, followed by smaller but non-trivial spikes at subsequent lags. Notably, a spike at approximately lag 1.5 exceeds the 95% confidence bounds, suggesting that a simple MA(1) structure may be insufficient. Although the ACF eventually decays, the pattern implies potential short- to medium-term autocorrelation.

The **PACF** plot reveals significant spikes at **lags 1 and 2**, with possible weaker structure beyond. This indicates that a higher-order autoregressive component (AR(2)) may be needed to adequately capture the dependence structure.

These patterns diverge from the clean cutoff typical of simpler ARIMA(1,1,0) or ARIMA(0,1,1) processes, and instead suggest richer dynamics. Based on this, we extend our candidate model set beyond ARIMA(1,1,1), considering additional terms to capture persistent structure.

The following models are proposed for evaluation:

-   **ARIMA(1,1,1)** – baseline model

-   **ARIMA(2,1,1)** – to capture potential AR(2) structure

-   **ARIMA(1,1,2)** – to address higher-order MA dynamics

-   **ARIMA(2,1,2)** – a flexible model for medium-term correlation

These models will be fitted via maximum likelihood, with performance evaluated using AIC, residual diagnostics (ACF, Q–Q plots), and the Ljung–Box test to assess remaining autocorrelation. The goal is to identify a parsimonious yet well-fitting model for forecasting quarterly AMOC variability.

#### Fitting ARIMA Models to Quarterly Data

```{r}
# Fit candidate ARIMA models to quarterly data
arima_111_q <- arima(train_q_ts, order = c(1, 1, 1), method = "ML")
arima_211_q <- arima(train_q_ts, order = c(2, 1, 1), method = "ML")
arima_112_q <- arima(train_q_ts, order = c(1, 1, 2), method = "ML")
arima_212_q <- arima(train_q_ts, order = c(2, 1, 2), method = "ML")

# Ljung–Box tests at lag 5
lb_111_q <- Box.test(residuals(arima_111_q), lag = 5, type = "Ljung-Box")
lb_211_q <- Box.test(residuals(arima_211_q), lag = 5, type = "Ljung-Box")
lb_112_q <- Box.test(residuals(arima_112_q), lag = 5, type = "Ljung-Box")
lb_212_q <- Box.test(residuals(arima_212_q), lag = 5, type = "Ljung-Box")

# Create summary table
arima_q_table <- data.frame(
  Model = c("ARIMA(1,1,1)", "ARIMA(2,1,1)", "ARIMA(1,1,2)", "ARIMA(2,1,2)"),
  AIC = c(AIC(arima_111_q), AIC(arima_211_q), AIC(arima_112_q), AIC(arima_212_q)),
  Ljung_Box_p = c(lb_111_q$p.value, lb_211_q$p.value, lb_112_q$p.value, lb_212_q$p.value)
)

knitr::kable(arima_q_table, digits = 4, caption = "Comparison of ARIMA Models for Quarterly AMOC")

```

The best-performing model by AIC is **ARIMA(1,1,1)**, with an AIC of 92.73 and a Ljung–Box p-value of 0.57, indicating no significant autocorrelation in the residuals. While **ARIMA(2,1,1)** and **ARIMA(1,1,2)** offer similar diagnostic performance, they are slightly less parsimonious and yield marginally higher AIC values. **ARIMA(2,1,2)**, the most complex model considered, performs notably worse, with both the highest AIC and no diagnostic advantage.

To validate our chosen ARIMA(1,1,1) model for the quarterly AMOC series, we compare it against an automated selection using `auto.arima()` from the `forecast` package. This allows us to assess whether more data-driven model selection yields substantial improvements.

```{r}
library(forecast)
# Auto.arima with non-seasonal specification
auto_arima <- auto.arima(train_ts, seasonal = FALSE, stepwise = FALSE, approximation = FALSE)

# Summary of selected model
summary(auto_arima)
```

```{r}
#| echo: false
#| message: false
#| warning: false

# Ljung-Box Test for residual autocorrelation
lb_manual <- Box.test(residuals(arima_111), lag = 10, type = "Ljung-Box")
lb_auto   <- Box.test(residuals(auto_arima), lag = 10, type = "Ljung-Box")

# Diagnostic plotting function
plot_diagnostics <- function(model, model_name, col_line) {
  layout(matrix(1:3, 1, 3, byrow = TRUE), widths = c(1.4, 1, 1))
  par(mar = c(4, 4, 3, 1))

  plot(residuals(model), type = "l", col = col_line, lwd = 1.2,
       main = paste(model_name, "Residuals"), ylab = "Residuals", xlab = "Time")
  acf(residuals(model), main = "ACF of Residuals", col = "black", lwd = 1.2)
  qqnorm(residuals(model), main = "Normal Q–Q Plot")
  qqline(residuals(model), col = "red", lwd = 1.2)

  layout(1)
}

# Plot for ARIMA(1,1,1)
plot_diagnostics(arima_111, "ARIMA(1,1,1)", "darkgreen")

# Plot for auto.arima
plot_diagnostics(auto_arima, "auto.arima", "purple")
```

The selected model was ARIMA(2,1,2), with an AIC of 279.11 — notably lower than the manually fitted ARIMA(1,1,1) (AIC = 285.08).

```{r}
#| echo: false
#| message: false
#| warning: false

library(gt)

# Create comparison table
data.frame(
  Model = c("ARIMA(1,1,1)", paste0("auto.arima (", auto_arima$arma[1], ",1,", auto_arima$arma[2], ")")),
  AIC = c(AIC(arima_111), AIC(auto_arima)),
  Ljung_Box_p = c(lb_manual$p.value, lb_auto$p.value)
) %>%
  gt() %>%
  tab_header(
    title = "Comparison of Manual and auto.arima Models for Quarterly AMOC"
  ) %>%
  fmt_number(columns = c(AIC, Ljung_Box_p), decimals = 4)

```

### Residual Diagnostics Comparison

Residual plots for both models (ARIMA(1,1,1) and auto.arima ARIMA(2,1,2)) show:

-   Residuals are approximately centred around zero.

-   The ACF shows no significant autocorrelation.

-   The Q-Q plot shows approximate normality.

While `auto.arima()` identified a more complex ARIMA(2,1,2) model with superior AIC, the simpler ARIMA(1,1,1) still performs well — producing acceptable residual behaviour and satisfying parsimony principles. The slight trade-off in AIC is justified given the desire for interpretability and alignment with the identified ACF/PACF structure.

#### Sarima Models:

Although the quarterly AMOC series showed no clear seasonal structure in the initial ACF and PACF plots, SARIMA models were fitted as a robustness check to confirm the absence of seasonal effects. Specifically, SARIMA(1,1,0)(0,0,1)\[4\] and SARIMA(0,1,1)(0,0,1)\[4\] were tested, where the seasonal period was set to 4 to correspond with quarterly data.

Both models performed worse than the non-seasonal ARIMA(1,1,1) model. The SARIMA(1,1,0)(0,0,1)\[4\] model returned an AIC of 290.5, while SARIMA(0,1,1)(0,0,1)\[4\] produced an AIC of 292.0. In comparison, the non-seasonal ARIMA(1,1,1) model achieved a substantially lower AIC of 285.1. Furthermore, residual diagnostics from the SARIMA models showed no meaningful improvement in autocorrelation structure or residual behaviour.

#### Additional Residual Diagnostics: ARCH Test

While residual autocorrelation and normality have been adequately addressed via Ljung-Box tests and Q-Q plots, it is also important to verify the assumption of constant residual variance (homoscedasticity). Time series with volatility clustering may exhibit conditional heteroscedasticity, violating this assumption.

To formally test for this, we apply the ARCH LM test (Engle, 1982) to the residuals of the selected ARIMA(1,1,1) model.

```{r}
#| echo: false
#| message: false
#| warning: false

# Load FinTS package for ARCH test
library(FinTS)

# ARCH LM test with 12 lags
arch_test <- ArchTest(residuals(arima_111_q), lags = 12)
arch_test
```

The ARCH LM test returned a p-value of 0.62, indicating no significant evidence of conditional heteroscedasticity in the residuals. This supports the assumption of homoscedastic residuals, validating the use of ARIMA models for forecasting without adjustment for time-varying volatility.

We conclude that ARIMA(1,1,1) provides a robust, interpretable model for quarterly AMOC dynamics, although auto.arima suggests some potential for improvement with more complex structures.

## Part D – Fitting Dynamic Linear Models

Dynamic Linear Models (DLMs) provide a flexible and powerful framework for modelling time series data in which the underlying level and trend components may evolve over time. This approach is particularly appropriate for environmental data such as the AMOC, where the underlying patterns and behaviour can vary across different periods. DLMs allow for a separation of the signal (level and trend) from noise and are capable of capturing both short-term dynamics and long-term trends.

### Model Set Up

#### General State Space Formulation:

The DLM structure applied follows the general state-space formulation, defined by the observation and system equations:

-   Observation Equation:

-   System Equation:

Where represents the observed AMOC value at time , is the unobserved state vector containing the level, trend, and seasonal components, and and represent the observation and system variances, respectively.

#### Monthly AMOC DML:

To investigate seasonal behaviour within the monthly AMOC series, a seasonal differencing with lag 12 was applied (Figure \@ref(fig:seasonal-diff)). This is standard for monthly data to assess repeating annual structure.

```{r}
#| echo: false
#| message: false
#| warning: false

library(dplyr)
library(zoo)
library(dlm)
library(FinTS)
library(forecast)
plot(diff(train_ts, lag=12), 
     main="Seasonal Difference (lag=12) - Monthly AMOC",
     ylab="Difference", xlab="Time", col="steelblue", lwd=2)
```

Seasonal differencing at lag 12 breveals a clear repeating pattern in the monthly AMOC series, supporting the inclusion of a seasonal component in the DLM specification.

#### Residual Diagnostics:

```{r}
library(dlm)

# Monthly DLM with Trend + Seasonality
build_month_dlm <- function(parm) {
  dlmModPoly(order=2, dV=exp(parm[1]), dW=c(0, exp(parm[2]))) +
    dlmModSeas(frequency=12, dV=0)
}

fit_month_dlm <- dlmMLE(train_ts, parm=rep(0,2), build=build_month_dlm)
mod_month_dlm <- build_month_dlm(fit_month_dlm$par)
filt_month_dlm <- dlmFilter(train_ts, mod_month_dlm)

resid_month <- residuals(filt_month_dlm, type="raw")$res

```

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(1,3))
plot(resid_month, type='l', main="Monthly DLM Residuals", ylab="Residuals", col='darkgreen')
acf(resid_month, main="ACF of Residuals")
qqnorm(resid_month); qqline(resid_month, col="red")
par(mfrow=c(1,1))
```

```{r}
#| echo: false
#| message: false
#| warning: false
Box.test(resid_month, lag=10, type="Ljung-Box")
ArchTest(resid_month, lags=12)
```

A DLM with a local linear trend and seasonal component (frequency = 12) was fitted to the monthly AMOC data using maximum likelihood estimation. Residual diagnostics revealed that the residuals fluctuated around zero but exhibited increasing variance over time, indicative of heteroscedasticity. The ACF plot indicated mild autocorrelation at lag 1, while the Q-Q plot suggested approximate normality with heavier tails.

Formal tests supported these findings:

-   Ljung-Box test (lag=10): *p* = 0.032 → Evidence of residual autocorrelation.

-   ARCH LM test (lags=12): *p* = 0.008 → Strong evidence of conditional heteroscedasticity.

While the monthly DLM adequately captured the main dynamics of the series, residual behaviour indicated potential improvements could be made by allowing for time-varying volatility.

#### Fitting Quarterly AMOC DLM

Given the lower frequency of the quarterly data, the seasonal pattern was less pronounced. Nonetheless, a seasonal component with frequency 4 was included for consistency.

```{r}
build_quarter_dlm <- function(parm) {
  dlmModPoly(order=2, dV=exp(parm[1]), dW=c(0, exp(parm[2]))) +
    dlmModSeas(frequency=4, dV=0)
}

fit_quarter_dlm <- dlmMLE(train_q_ts, parm=rep(0,2), build=build_quarter_dlm)
mod_quarter_dlm <- build_quarter_dlm(fit_quarter_dlm$par)
filt_quarter_dlm <- dlmFilter(train_q_ts, mod_quarter_dlm)

resid_quarter <- residuals(filt_quarter_dlm, type="raw")$res
```

Residual Diagnostics:

```{r}
#| echo: false
#| message: false
#| warning: false
# 6. Residual Diagnostics for Quarterly DLM
par(mfrow=c(1,3))
plot(resid_quarter, type='l', main="Quarterly DLM Residuals", ylab="Residuals", col='darkblue')
acf(resid_quarter, main="ACF of Residuals")
qqnorm(resid_quarter); qqline(resid_quarter, col="red")
par(mfrow=c(1,1))

```

```{r}
#| echo: false
#| message: false
#| warning: false
# 7. Residual Tests
Box.test(resid_quarter, lag=5, type="Ljung-Box")
ArchTest(resid_quarter, lags=5)
```

The quarterly AMOC series was modelled using a local linear trend DLM with a seasonal component (frequency = 4). Residual diagnostics indicated that the residuals fluctuated around zero with stable variance. The ACF showed no significant autocorrelation, and the Q-Q plot indicated near-normal behaviour.

Formal tests confirmed these results:

-   Ljung-Box test (lag=5): *p* = 0.104 → No evidence of residual autocorrelation.

-   ARCH LM test (lags=5): *p* = 0.110 → No evidence of conditional heteroscedasticity.

The quarterly DLM performed well, with residuals satisfying key modelling assumptions.

Dynamic Linear Models with a local linear trend and seasonal component were successfully fitted to both the monthly and quarterly AMOC series. The seasonal differencing plot for the monthly series confirmed the presence of a repeating annual pattern, justifying the seasonal component. Residual diagnostics highlighted that while the quarterly DLM provided a clean and robust fit, the monthly DLM exhibited minor residual autocorrelation and evidence of conditional heteroscedasticity, consistent with higher-frequency environmental variability. Nonetheless, both models provide a suitable basis for forecasting, which will be explored in Part E.

## Part E: Forecasting AMOC using ARIMA and DLM Models

#### Forecasting Methodology

To assess the short-term predictability of AMOC, ARIMA(1,1,1) and Dynamic Linear Models (DLM) were fitted to both the monthly and quarterly datasets. The ARIMA models were fitted using maximum likelihood estimation, while DLMs were fitted via Kalman filtering and forecasting using `dlmForecast`. The training set consisted of all observations up to 2022 Q2 (Quarterly) or 8 months before the end of the monthly data. The test set covered the subsequent two quarters or eight months.

#### 

#### Forecasting ARIMA models

```{r}
library(forecast)

#Forecasting the monthly data (arima_111) for 8 months ahead
forecast_monthly_arima <- forecast(arima_111, h=8)

# Forecasting the quarterly data (arima_111_q) for 2 quarters ahead
forecast_quarterly_arima <- forecast(arima_111_q, h=2)
```

#### Forecasting DLM models

```{r}
library(dlm)

forecast_dlm_monthly <- dlmForecast(filt_month_dlm, nAhead=8)
forecast_dlm_quarterly <- dlmForecast(filt_quarter_dlm, nAhead=2)
```

### Plotting the Forecasted Values:

#### Monthly AMOC Forecast

```{r}
#| echo: false
#| message: false
#| warning: false

# Combine train, test, forecast, and intervals for Monthly ARIMA
monthly_arima_combined <- ts.union(
  train_ts,
  test_ts,
  forecast_monthly_arima$mean,
  forecast_monthly_arima$lower[,2],
  forecast_monthly_arima$upper[,2]
)

# Prepare monthly data frame
monthly_df <- data.frame(
  Time = time(monthly_arima_combined),
  Observed = as.numeric(monthly_arima_combined[,1]),
  Actual = as.numeric(monthly_arima_combined[,2]),
  Forecast = as.numeric(monthly_arima_combined[,3]),
  Lower = as.numeric(monthly_arima_combined[,4]),
  Upper = as.numeric(monthly_arima_combined[,5])
)

monthly_arima_plot <- ggplot(monthly_df, aes(x=Time)) +
  geom_line(aes(y=Observed, col="Observed (Train)")) +
  geom_line(aes(y=Actual, col="Actual (Test)")) +
  geom_line(aes(y=Forecast, col="Forecast"), linetype="dashed") +
  geom_ribbon(aes(ymin=Lower, ymax=Upper), fill="blue", alpha=0.2) +
  geom_vline(xintercept=max(time(train_ts)), linetype="dotted") +
  labs(title="Monthly ARIMA(1,1,1) Forecast", y="AMOC Value", x="Time") +
  scale_color_manual(values=c("black", "green", "red")) +
  theme_minimal() +
  theme(legend.title=element_blank())

```

#### Monthly DML

```{r}
#| echo: false
#| message: false
#| warning: false

# DLM forecast values and intervals
dlm_monthly_mean <- forecast_dlm_monthly$f
dlm_monthly_sd <- sqrt(unlist(forecast_dlm_monthly$Q))

# Combine train, test, DLM forecast and intervals for Monthly DLM
monthly_dlm_combined <- ts.union(
  train_ts,
  test_ts,
  dlm_monthly_mean,
  dlm_monthly_mean - 1.96 * dlm_monthly_sd,
  dlm_monthly_mean + 1.96 * dlm_monthly_sd
)

monthly_dlm_df <- data.frame(
  Time = time(monthly_dlm_combined),
  Observed = as.numeric(monthly_dlm_combined[,1]),
  Actual = as.numeric(monthly_dlm_combined[,2]),
  Forecast = as.numeric(monthly_dlm_combined[,3]),
  Lower = as.numeric(monthly_dlm_combined[,4]),
  Upper = as.numeric(monthly_dlm_combined[,5])
)

monthly_dlm_plot <- ggplot(monthly_dlm_df, aes(x=Time)) +
  geom_line(aes(y=Observed, col="Observed (Train)")) +
  geom_line(aes(y=Actual, col="Actual (Test)")) +
  geom_line(aes(y=Forecast, col="Forecast"), linetype="dashed") +
  geom_ribbon(aes(ymin=Lower, ymax=Upper), fill="blue", alpha=0.2) +
  geom_vline(xintercept=max(time(train_ts)), linetype="dotted") +
  labs(title="Monthly DLM Forecast", y="AMOC Value", x="Time") +
  scale_color_manual(values=c("black", "green", "red")) +
  theme_minimal() +
  theme(legend.title=element_blank())

```

```{r}
#| label: fig:monthlyforecast
#| fig-cap: "Monthly AMOC forecasts from ARIMA(1,1,1) and DLM models shown side-by-side. ARIMA demonstrates slightly narrower prediction intervals with improved forecast accuracy relative to DLM over the 8-month test period."
#| echo: false
#| message: false
#| warning: false
library(patchwork)

monthly_arima_plot + monthly_dlm_plot + 
  plot_layout(ncol=2, guides = "collect") & 
  theme(legend.position = "bottom")
```

Quarterly ARIMA

```{r}
#| echo: false
#| message: false
#| warning: false

quarterly_arima_combined <- ts.union(
  train_q_ts, test_q_ts,
  forecast_quarterly_arima$mean,
  forecast_quarterly_arima$lower[,2],
  forecast_quarterly_arima$upper[,2]
)

quarterly_arima_df <- data.frame(
  Time = time(quarterly_arima_combined),
  Observed = as.numeric(quarterly_arima_combined[,1]),
  Actual = as.numeric(quarterly_arima_combined[,2]),
  Forecast = as.numeric(quarterly_arima_combined[,3]),
  Lower = as.numeric(quarterly_arima_combined[,4]),
  Upper = as.numeric(quarterly_arima_combined[,5])
)

quarterly_arima_plot <- ggplot(quarterly_arima_df, aes(x=Time)) +
  geom_line(aes(y=Observed, col="Observed (Train)")) +
  geom_line(aes(y=Actual, col="Actual (Test)")) +
  geom_line(aes(y=Forecast, col="Forecast"), linetype="dashed") +
  geom_ribbon(aes(ymin=Lower, ymax=Upper), fill="blue", alpha=0.2) +
  geom_vline(xintercept=max(time(train_q_ts)), linetype="dotted") +
  labs(title="Quarterly ARIMA(1,1,1) Forecast", y="AMOC Value", x="Time") +
  scale_color_manual(values=c("black", "green", "red")) +
  theme_minimal() +
  theme(legend.title=element_blank())


```

Quarterly DLM

```{r}
#| echo: false
#| message: false
#| warning: false

# DLM forecast values and intervals for Quarterly
dlm_quarterly_mean <- forecast_dlm_quarterly$f
dlm_quarterly_sd <- sqrt(unlist(forecast_dlm_quarterly$Q))

# Combine train, test, DLM forecast mean and intervals
quarterly_dlm_combined <- ts.union(
  train_q_ts,
  test_q_ts,
  dlm_quarterly_mean,
  dlm_quarterly_mean - 1.96 * dlm_quarterly_sd,
  dlm_quarterly_mean + 1.96 * dlm_quarterly_sd
)

quarterly_dlm_df <- data.frame(
  Time = time(quarterly_dlm_combined),
  Observed = as.numeric(quarterly_dlm_combined[,1]),
  Actual = as.numeric(quarterly_dlm_combined[,2]),
  Forecast = as.numeric(quarterly_dlm_combined[,3]),
  Lower = as.numeric(quarterly_dlm_combined[,4]),
  Upper = as.numeric(quarterly_dlm_combined[,5])
)


quarterly_dlm_plot <- ggplot(quarterly_dlm_df, aes(x=Time)) +
  geom_line(aes(y=Observed, col="Observed (Train)")) +
  geom_line(aes(y=Actual, col="Actual (Test)")) +
  geom_line(aes(y=Forecast, col="Forecast"), linetype="dashed") +
  geom_ribbon(aes(ymin=Lower, ymax=Upper), fill="blue", alpha=0.2) +
  geom_vline(xintercept=max(time(train_q_ts)), linetype="dotted") +
  labs(title="Quarterly DLM Forecast", y="AMOC Value", x="Time") +
  scale_color_manual(values=c("black", "green", "red")) +
  theme_minimal() +
  theme(legend.title=element_blank())


```

```{r}
#| label: fig:quarterlyforecast
#| fig-cap: "Quarterly AMOC forecasts from ARIMA(1,1,1) and DLM models shown side-by-side. While both models capture the broad trend, ARIMA outperforms DLM with lower forecast uncertainty and error over the 2-quarter test set."

#| echo: false
#| message: false
#| warning: false
library(patchwork)

quarterly_arima_plot + quarterly_dlm_plot + 
  plot_layout(ncol=2, guides = "collect") & 
  theme(legend.position = "bottom")

```

Figures X and Y below present side-by-side visual comparisons of the ARIMA and DLM forecasts for both the monthly and quarterly datasets. The solid black line indicates the observed training data, the red dashed line represents the forecast values, while the green line represents the true values from the test set. Shaded regions indicate the 95% prediction intervals.

Both models capture the broad trend in AMOC reasonably well. However, prediction intervals for the DLM models are generally wider, reflecting greater forecast uncertainty.

#### Model Prediction Accuracy

```{r}
#| echo: false
#| message: false
#| warning: false


# Monthly ARIMA accuracy
acc_monthly_arima <- accuracy(forecast_monthly_arima, test_ts)
mae_monthly_arima <- acc_monthly_arima["Test set", "MAE"]
rmse_monthly_arima <- acc_monthly_arima["Test set", "RMSE"]

# Quarterly ARIMA accuracy
acc_quarterly_arima <- accuracy(forecast_quarterly_arima, test_q_ts)
mae_quarterly_arima <- acc_quarterly_arima["Test set", "MAE"]
rmse_quarterly_arima <- acc_quarterly_arima["Test set", "RMSE"]

# DLM Monthly accuracy
dlm_monthly_mean <- as.numeric(forecast_dlm_monthly$f)
actual_monthly <- as.numeric(test_ts)
mae_monthly_dlm <- mean(abs(actual_monthly - dlm_monthly_mean))
rmse_monthly_dlm <- sqrt(mean((actual_monthly - dlm_monthly_mean)^2))

# DLM Quarterly accuracy
dlm_quarterly_mean <- as.numeric(forecast_dlm_quarterly$f)
actual_quarterly <- as.numeric(test_q_ts)
mae_quarterly_dlm <- mean(abs(actual_quarterly - dlm_quarterly_mean))
rmse_quarterly_dlm <- sqrt(mean((actual_quarterly - dlm_quarterly_mean)^2))

```

Summary Table

```{r}
#| label: tab:forecastacc
#| tbl-cap: "Forecast accuracy summary for ARIMA(1,1,1) and DLM models fitted to monthly and quarterly AMOC series. ARIMA generally outperforms DLM, particularly for quarterly forecasts, as indicated by lower MAE and RMSE values."
#| echo: false
#| message: false
#| warning: false

library(knitr)

# Create summary dataframe
summary_df <- data.frame(
  Model = c("ARIMA(1,1,1)", "DLM", "ARIMA(1,1,1)", "DLM"),
  Frequency = c("Monthly", "Monthly", "Quarterly", "Quarterly"),
  MAE = round(c(mae_monthly_arima, mae_monthly_dlm, mae_quarterly_arima, mae_quarterly_dlm), 3),
  RMSE = round(c(rmse_monthly_arima, rmse_monthly_dlm, rmse_quarterly_arima, rmse_quarterly_dlm), 3)
)

# Output professional summary table
kable(summary_df, caption = "Forecast Accuracy Summary for AMOC Models",
      col.names = c("Model", "Data Frequency", "MAE", "RMSE"),
      align = c("c", "c", "c", "c"))

```

Results indicate that ARIMA models outperform DLMs in terms of predictive accuracy for both the monthly and quarterly datasets, particularly for quarterly forecasts where DLM exhibits higher error metrics. This likely reflects the more parsimonious structure of the ARIMA model being better suited to short-term forecasting in this context.

#### Final Discussion

Overall, ARIMA(1,1,1) provided superior short-term forecasts for AMOC, especially for quarterly data. While DLMs offer a flexible modelling framework that can accommodate time-varying parameters and capture dynamic behaviour, their greater forecast uncertainty and wider intervals suggest over-parameterisation or structural challenges given the short available test set. The tight intervals and lower error values from ARIMA models justify their use for operational short-term forecasting of AMOC.

# California daily temperatures

## Part A: Exploratory Analysis of Spatial and Temporal Relationships

```{r}
#| echo: false
#| message: false
#| warning: false
temps <- read.csv("~/GitHub/university-projects/Modelling in Space and Time/In progress/MaxTempCalifornia.csv")
meta <- read_csv("~/GitHub/university-projects/Modelling in Space and Time/In progress/metadataCA.csv")

library(ggmap)
library(ggspatial)
library(sf)
library(dplyr)
library(viridis)

```

This section presents an exploratory analysis of daily maximum temperatures recorded at 11 sites across California during 2012, focusing on spatial variation driven by site elevation and coastal proximity.

#### Summary Statistics Table

```{r}
#| echo: false
#| message: false
#| warning: false
library(dplyr)
library(gt)

temps_long <- read.csv("MaxTempCalifornia.csv") %>%
  pivot_longer(-Date, names_to = "Site", values_to = "Temp") %>%
  mutate(Date = as.Date(as.character(Date), format = "%Y%m%d")) %>%
  left_join(meta, by = c("Site" = "Location"))

temps_long <- temps_long %>%
  mutate(Site_order = reorder(Site, Temp, mean))

summary_table <- temps_long %>%
  group_by(Site) %>%
  summarise(
    Mean_Temp = round(mean(Temp, na.rm = TRUE), 1),
    Median_Temp = round(median(Temp, na.rm = TRUE), 1),
    Min_Temp = round(min(Temp, na.rm = TRUE), 1),
    Max_Temp = round(max(Temp, na.rm = TRUE), 1),
    SD_Temp = round(sd(Temp, na.rm = TRUE), 1)
  ) %>%
  arrange(desc(Mean_Temp))

summary_table %>%
  gt() %>%
  tab_header(
    title = "Summary Statistics of Maximum Daily Temperatures",
    subtitle = "Across 11 California Sites (2012)"
  ) %>%
  cols_label(
    Site = "Site",
    Mean_Temp = "Mean (°C)",
    Median_Temp = "Median (°C)",
    Min_Temp = "Min (°C)",
    Max_Temp = "Max (°C)",
    SD_Temp = "SD (°C)"
  )
```

Table 10 summarises key temperature statistics. Inland sites generally recorded higher mean temperatures and greater variability than coastal locations.

Death Valley recorded both the highest mean temperature (34.5°C) and the largest variability (SD = 10.5°C), reflecting its extreme inland desert climate. Other inland sites like Barstow (Mean = 27.2°C) and Fresno (Mean = 26.4°C) showed similar patterns.

In contrast, coastal sites like San Francisco (Mean = 18.3°C, SD = 4.0°C) and Santa Cruz (Mean = 20.8°C, SD = 4.8°C) were substantially cooler and more stable, reflecting the moderating influence of the Pacific Ocean.

While elevation does affect temperatures to some extent — with higher inland sites like Redding (1041m) recording lower mean temperatures than nearby lowland regions — coastal proximity remains the dominant driver of temperature patterns.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 7
#| label: ca_map_elevation
#| fig-cap: "Spatial distribution of the 11 temperature monitoring sites across California, overlaid on a terrain basemap."


register_stadiamaps(key = "6464b0dc-0207-4438-b618-4894b33199d5")

# Convert to sf object
meta_sf <- st_as_sf(meta, coords = c("Long", "Lat"), crs = 4326)

ca_map <- get_stadiamap(bbox = c(left = -125, bottom = 32, right = -114, top = 42),
                        zoom = 7, maptype = "stamen_terrain")


# Plot with elevation gradient
ggmap(ca_map) +
  geom_sf(data = meta_sf, inherit.aes = FALSE, aes(color = Elev), size = 3) +
  geom_text(data = meta, aes(x = Long, y = Lat, label = Location), size = 3, color = "black") +
  scale_color_viridis(
    option = "C",
    name = "Elevation (m)",
    trans = "sqrt",
    breaks = c(0, 250, 500, 1000, 1500),
    limits = c(0, 1500)
  ) +
  ggtitle("California Sites with Elevation Context") +
  theme_minimal()

```

Figure 20 shows the spatial distribution of the monitoring sites, coloured by elevation. Elevation varies considerably across locations, from sea level at Santa Cruz (0m) and San Francisco (16m), to inland higher-elevation sites like Redding (1041m) and Cedar Park (948m).

Death Valley is notable for sitting inland at -86m below sea level, while other inland sites like Barstow (665m) and Fresno (92m) lie further from the coast, despite moderate elevation.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-width: 9
#| fig-height: 6
#| label: ca_temp_ridgeline
#| fig-cap: "Distribution of maximum daily temperatures across 11 California sites in 2012. Sites are ordered by mean temperature. The colour gradient transitions from cooler temperatures (blue) to warmer temperatures (red), enhancing interpretability. Death Valley recorded the hottest temperature (52.8°C), while Santa Cruz recorded the coldest (-6.1°C)."


library(ggridges)
library(dplyr)
library(hrbrthemes)

temps_long <- read.csv("MaxTempCalifornia.csv") %>%
  pivot_longer(-Date, names_to = "Site", values_to = "Temp") %>%
  mutate(Date = as.Date(as.character(Date), format = "%Y%m%d")) %>%
  left_join(meta, by = c("Site" = "Location"))

temps_long <- temps_long %>%
  mutate(Site_order = reorder(Site, Temp, mean))


temps_long %>%
  ggplot(aes(x = Temp, y = Site_order, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +
  annotate("text", 
           x = max(temps_long$Temp) + 1, 
           y = temps_long %>% filter(Temp == max(Temp)) %>% pull(Site_order) %>% unique(),
           label = paste0("Hottest: ", max(temps_long$Temp), "°C"), 
           hjust=0, size=3) +
  annotate("text", 
           x = min(temps_long$Temp) - 1, 
           y = temps_long %>% filter(Temp == min(Temp)) %>% pull(Site_order) %>% unique(),
           label = paste0("Coldest: ", min(temps_long$Temp), "°C"), 
           hjust=1, size=3) +
  scale_fill_gradientn(
    colours = c("navyblue", "deepskyblue", "lightyellow", "orange", "firebrick"),
    name = "Max Temp (°C)"
  ) +
  labs(
    title = "Distribution of Max Daily Temperatures by Site (2012)",
    subtitle = "Sites ordered by mean temperature | Colour gradient reflects temperature from cold (blue) to hot (red)",
    x = "Max Temperature (°C)",
    y = "Site"
  ) +
 theme_minimal() +
theme(
  legend.position = "right",
  plot.title.position = "plot",
  panel.spacing = unit(0.1, "lines"),
  panel.grid = element_blank(),
  axis.text.y = element_text(hjust=0)
)
```

Figure 21 displays the distribution of maximum daily temperatures across sites. Inland sites consistently exhibited higher maximum temperatures and wider variability.

Death Valley recorded the hottest temperature of 53.3°C, while the coldest temperature of 0°C was observed at the coastal site of Santa Cruz. Inland sites like Barstow and Fresno also recorded very high maximum temperatures of 43.9°C.

### Interpretation

Inland Californian sites, regardless of elevation, experienced hotter and more variable conditions than coastal locations. Coastal proximity was the primary factor influencing temperature stability, with elevation providing a secondary moderating effect.

Overall, the analysis highlights a clear spatial gradient in temperature across California, reflecting well-established climatic patterns driven mainly by distance from the coast

## Part B: Spatial Gaussian Process Modelling

This section focuses on spatial modelling of maximum daily temperatures recorded across California on 13th December 2012. The primary goal is to fit a spatial Gaussian Process (GP) model to predict temperatures at two withheld locations: San Diego and Fresno.

### Data Preparation:

```{r}
temps$Date <- as.character(temps$Date)

# Training data: All locations on Dec 13th
training_data <- temps[temps$Date == '20121213', c('San.Francisco', 'Napa', 'Santa.Cruz', 'Death.Valley', 'Ojai', 'Barstow', 'LA', 'CedarPark', 'Redding')]  
# Dec 13th data for all locations

# Coordinates for the training locations (the 9 locations on Dec 13th)
training_coords <- meta[!meta$Location %in% c('San Diego', 'Fresno'), c('Long', 'Lat')]

training_coords$Elevation <- meta[!meta$Location %in% c('San Diego', 'Fresno'), 'Elev']

training_data_numeric <- as.numeric(unlist(training_data))
```

```{r}
# Create geoR-compatible object (use the coordinates and the numeric temperature data)
geo_data_train <- as.geodata(data.frame(x = training_coords$Long, 
                                        y = training_coords$Lat, 
                                        z = training_data_numeric, 
                                        elev = training_coords$Elevation))
```

#### Exploratory Variogram Analysis:

```{r}
 vario_emp <- variog(geo_data_train, max.dist = 600, option='bin')

plot(vario_emp,
     main = "Empirical Variogram of Max Temp (13 Dec 2012)",
     xlab = "Distance (km)",
     ylab = "Semivariance",
     pch = 19, cex = 1.2, col = "black")
lines(lowess(vario_emp$u, vario_emp$v), col = "blue", lwd = 2)
```

The empirical variogram (Figure X) shows moderate spatial dependence in maximum temperatures across California on 13th December 2012. Semivariance generally increases with distance, indicating that geographically closer sites have more similar temperatures. The slight dip at mid-range distances suggests some local similarity among inland sites, while the rise at larger distances reflects greater dissimilarity between coastal and inland regions. This structure supports the use of a spatial Gaussian process with a short-to-moderate effective range.

### Fitting the Spatial Gaussian Process Model

We fit three different models to the spatial temperature data:

1.  **Model GP (Base)**: A standard Gaussian Process (GP) with the **Matérn covariance function**. This model captures spatial dependencies effectively and is flexible for a wide range of spatial processes.
2.  **Model GP Exponential**: The **Exponential covariance function** is a special case of the Matérn function, with a simpler structure that assumes faster decaying spatial correlations.
3.  **Model GP Matérn (kappa = 2.5)**: We explored the **Matérn covariance function** with **kappa = 2.5** to account for a smoother spatial process, allowing for more flexibility in modeling spatial correlation.

Each model was fitted using **Maximum Likelihood Estimation (MLE)** and assessed using **AIC**, **BIC**, and **log-likelihood**.

#### Model 1: Matern Model (Baseline)

```{r}
#| echo: true
#| message: false
#| warning: false

model_gp <- likfit(geo_data_train, trend = "1st", cov.model = "matern", kappa = 1.5, ini.cov.pars = c(10, 1))

model_gp
```

#### **Model 2:**

```{r}
#| echo: true
#| message: false
#| warning: false
model_gp_exponential <- likfit(geo_data_train, trend = "1st", cov.model = "exponential", kappa = 1.5, ini.cov.pars = c(10, 1))

model_gp_exponential
```

#### **Model 3:**

```{r}
#| echo: true
#| message: false
#| warning: false
# Fit Matérn model with kappa = 1.5
model_gp_matern2.5 <- likfit(geo_data_train, trend = "1st", cov.model = "exponential", kappa = 2.5, ini.cov.pars = c(10, 1))

model_gp_matern2.5
```

The estimated model parameters provide insight into the spatial and trend components of the temperature distribution. The **beta0**, **beta1**, and **beta2** parameters represent the intercept and coefficients for the first-order trend, where **beta1** and **beta2** capture the effect of **elevation** on temperature. The **tau2** parameter (nugget) accounts for unexplained variability or measurement error that could not be modeled by spatial correlation alone. The **sigmasq** parameter, representing the variance of the spatial process, gives an indication of the overall variability in temperature. The **phi** parameter determines the correlation length, controlling the range over which locations are spatially correlated. The **kappa** parameter in the **Matérn** model defines the smoothness of the spatial process, with **kappa = 1.5** indicating a relatively smooth spatial correlation structure. By examining these parameters, we can gain a deeper understanding of how temperature is spatially distributed across California and how it is influenced by local elevation.

### Comparison of Models:

We evaluated the models using **AIC** and **log-likelihood**. Here are the comparison metrics for each model:

```{r}
#| echo: false
#| message: false
#| warning: false
# Extract AIC, BIC, and Log-Likelihood for each model
model_stats <- data.frame(
  Model = c("Model GP (Base)", "Model GP Exponential", "Model GP Matern 2.5"),
  AIC = c(AIC(model_gp), AIC(model_gp_exponential), AIC(model_gp_matern2.5)),
  LogLikelihood = c(logLik(model_gp), logLik(model_gp_exponential), logLik(model_gp_matern2.5))
)

kable(model_stats, caption = "Model Comparison: AIC and Log-Likelihood")

```

-   **AIC and BIC**: The **Base GP** model has the lowest **AIC** and **BIC**, suggesting it strikes the best balance between fit and complexity.
-   **Log-Likelihood**: All models have very similar log-likelihoods, but **Model GP (Base)** has the highest value, indicating it fits the data slightly better.

### Model Validation:

**Cross-validation** helps us check if the fitted model generalizes well to unseen data. We'll use the `xvalid()` function from the `geoR` package, which performs **leave-one-out cross-validation** for spatial data.

```{r}
#| echo: false
#| message: false
#| warning: false
# Cross-validation using xvalid() function from geoR
# For Model GP (Base)
xvalid_gp <- xvalid(geo_data_train, model = model_gp)

# For Model GP Exponential
xvalid_exponential <- xvalid(geo_data_train, model = model_gp_exponential)

# For Model GP Matern 2.5
xvalid_matern2.5 <- xvalid(geo_data_train, model = model_gp_matern2.5)

# RMSE, MAE, and R^2 calculation
# RMSE for Model GP
rmse_gp <- sqrt(mean((xvalid_gp$data - xvalid_gp$predicted)^2))
mae_gp <- mean(abs(xvalid_gp$data - xvalid_gp$predicted))
r2_gp <- 1 - sum((xvalid_gp$data - xvalid_gp$predicted)^2) / sum((xvalid_gp$data - mean(xvalid_gp$data))^2)

# RMSE for Model GP Exponential
rmse_exponential <- sqrt(mean((xvalid_exponential$data - xvalid_exponential$predicted)^2))
mae_exponential <- mean(abs(xvalid_exponential$data - xvalid_exponential$predicted))
r2_exponential <- 1 - sum((xvalid_exponential$data - xvalid_exponential$predicted)^2) / sum((xvalid_exponential$data - mean(xvalid_exponential$data))^2)

# RMSE for Model GP Matern 2.5
rmse_matern2.5 <- sqrt(mean((xvalid_matern2.5$data - xvalid_matern2.5$predicted)^2))
mae_matern2.5 <- mean(abs(xvalid_matern2.5$data - xvalid_matern2.5$predicted))
r2_matern2.5 <- 1 - sum((xvalid_matern2.5$data - xvalid_matern2.5$predicted)^2) / sum((xvalid_matern2.5$data - mean(xvalid_matern2.5$data))^2)

# Create a summary table for RMSE, MAE, and R^2
model_comparison_cv <- data.frame(
  Model = c("Model GP (Base)", "Model GP Exponential", "Model GP Matern 2.5"),
  RMSE = c(rmse_gp, rmse_exponential, rmse_matern2.5),
  MAE = c(mae_gp, mae_exponential, mae_matern2.5),
  R_squared = c(r2_gp, r2_exponential, r2_matern2.5)
)

# Display the comparison table
knitr::kable(model_comparison_cv, caption = "Model Comparison: Cross-Validation Metrics (RMSE, MAE, R²)")

```

The cross-validation results for all three models — **Model GP (Base)**, **Model GP Exponential**, and **Model GP Matérn 2.5** — reveal very similar performance in terms of **Root Mean Squared Error (RMSE)**, **Mean Absolute Error (MAE)**, and **R²**. Specifically, the models show near-identical **RMSE** and **MAE** values, suggesting that all three models make similarly accurate predictions on unseen data. The **R²** values also indicate that the proportion of variance explained by the models is almost identical.

Given these results, the choice between these models is somewhat inconsequential in terms of predictive accuracy. However, the **Exponential** model, being simpler, may be preferable due to its computational efficiency.

```{r}
xv.ml<-xvalid(geo_data_train,model=model_gp)
 par(mfrow=c(3,2),mar=c(4,2,2,2))
 plot(xv.ml,error=TRUE,std.error=FALSE,pch=19)
```

To validate the Base Matern3/2 model, we performed **leave-one-out cross-validation (LOO-CV)** and assessed key diagnostic plots:

-   **Data vs Predicted**: This scatter plot compares observed and predicted values. A close alignment to the 1:1 line indicates accurate predictions.

-   **Residuals vs Predicted**: Random scatter around zero suggests that the model's errors are unbiased and the model is well-fitted.

-   **Histogram of Residuals**: Ideally, the residuals should follow a normal distribution, confirming the model’s assumptions about error behavior.

These plots show that the model fits the data well, with no significant issues in prediction accuracy or residuals.

### Prediction of Maximum Temperature in San Diego and Fresno:

Using the fitted **Gaussian Process model (Base)**, we predicted the maximum temperature in **San Diego** and **Fresno** on **December 13th, 2012** via kriging. The predictions were obtained using a spatial prediction grid, covering the region of interest, and the results for the two cities are as follows:

-   **San Diego**: Predicted temperature = **16.49°C** (with a prediction variance of **Y**).

-   **Fresno**: Predicted temperature = **14.64°C** (with a prediction variance of **W**).

### Comparison of Predicted vs Real Temperatures:

```{r}
# Define the coordinates for San Diego and Fresno
locations <- data.frame(
  x = c(-117.1611, -119.7726),  # Longitude for San Diego and Fresno
  y = c(32.7157, 36.7468)       # Latitude for San Diego and Fresno
)

# Perform kriging for spatial prediction
preds <- krige.conv(geo_data_train, loc = locations, krige = krige.control(obj.model = model_gp))

# Extract predicted values (mean temperatures)
predicted_temperatures <- preds$predict

# Display predicted temperatures for San Diego and Fresno
# predicted_temperatures
```

The predicted temperatures for **San Diego** and **Fresno** on **December 13th, 2012**, were obtained using the **Gaussian Process model (Base)** with spatial kriging. These predictions were compared with the real observed temperatures for the same day:

**San Diego**:

-   **Predicted Temperature**: 16.49°C

-   **Real Temperature**: 16.1°C

The predicted temperature is very close to the observed value, indicating that the model performed well in predicting the temperature for San Diego.

**Fresno**:

-   **Predicted Temperature**: 14.64°C

-   **Real Temperature**: 16.7°C

    The predicted temperature for Fresno deviates more significantly from the observed value, suggesting that the model may not have captured the spatial temperature variation as accurately for Fresno.

These comparisons highlight the model’s effectiveness in predicting **San Diego**’s temperature, but also suggest room for improvement in predicting **Fresno**’s temperature. Potential improvements could involve exploring more sophisticated spatial modeling techniques or refining model parameters.

## Part C:

Data Preparation:

```{r}
temps <- read.csv("~/GitHub/university-projects/Modelling in Space and Time/In progress/MaxTempCalifornia.csv")

# Check the structure of the dataset
str(temps)
head(temps)
```

```{r}
# Convert the Date column to Date format
temps$Date <- as.Date(as.character(temps$Date), format = "%Y%m%d")

# Filter data for San Diego and Fresno for 2012
san_diego_data <- temps[temps$Date >= "2012-01-01" & temps$Date <= "2012-12-31", c("Date", "San.Diego")]
fresno_data <- temps[temps$Date >= "2012-01-01" & temps$Date <= "2012-12-31", c("Date", "Fresno")]

# View first few rows of the filtered data
head(san_diego_data)
head(fresno_data)

```

```{r}
# Plot for San Diego
p1<- ggplot(san_diego_data, aes(x = Date, y = San.Diego)) +
  geom_line() +
  ggtitle("San Diego (2012)") +
  xlab("Date") + ylab("Temperature (°C)") +
  theme_minimal()

# Plot for Fresno
p2 <- ggplot(fresno_data, aes(x = Date, y = Fresno)) +
  geom_line() +
  ggtitle("Fresno (2012)") +
  xlab("Date") + ylab("Temperature (°C)") +
  theme_minimal()

p1 + p2
```

The time series plots for **San Diego** and **Fresno** in 2012 reveal distinct seasonal patterns. For **San Diego**, the temperatures fluctuate smoothly between 20°C and 30°C, with higher temperatures during the summer months (June-August) and cooler temperatures in the winter (December). This relatively consistent variation is likely influenced by the coastal location of the city.

In contrast, **Fresno** exhibits more pronounced temperature variability, ranging from 10°C to 40°C, reflecting its inland positioning. Like San Diego, Fresno experiences higher temperatures in summer and lower ones in winter, but the fluctuations are sharper, with extreme temperature spikes that are characteristic of the hotter inland climate.

These trends suggest that both cities exhibit clear seasonal temperature patterns, which will be important for the time series modeling and forecasting stages.
