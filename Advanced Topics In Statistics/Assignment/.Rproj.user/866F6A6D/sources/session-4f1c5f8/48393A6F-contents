---
title: "**MTHM017 - Advanced Topics in Statistics, Assignment**"
author: "**James R Lewis**"
date: "**00-00-2025**"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["amsmath", "amssymb", "mathtools", "bm", "caption", "hyperref"]
    includes:
      in_header: preamble.tex
    toc: true
    toc_depth: 2
    number_sections: false
  html_document:
    css: style.css
  word_document: default
  header-includes:
  - \usepackage{mathtools}  # Move this to the top
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{bm}
  - \usepackage{geometry}
  - \geometry{a4paper, margin=1in}
  - \usepackage{setspace}
  - \onehalfspacing

classoption: a4paper,11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading datasets and libraries, include=FALSE}
rtimes <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/rtimes.csv")

ClassificationTrue <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/ClassificationTrue.csv")

Classification <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/Classification.csv")

library(tidyverse)
library(ggplot2)
library(rmarkdown)
library(stargazer)

```

**Declaration of AI Assistance**: I have used OpenAI’s ChatGPT tool in creating this report.

AI-supported/AI-integrated use is permitted in this assessment. I acknowledge the following uses of GenAI tools in this assessment:

1.  I have used GenAI tools to check and debug my code.
2.  I have used GenAI tools to proofread and correct grammar or spelling errors.
3.  I have used GenAI tools to give me feedback on a draft.

I declare that I have referenced use of GenAI outputs within my assessment in line with the University referencing guidelines.

```{r setting plot theme, include = FALSE}
custom_theme <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),  # Centered title
    axis.title = element_text(face = "bold", size = 12),
    axis.text = element_text(size = 11), 
    panel.grid.major = element_line(color = "gray85"),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.8)
  )
```

\newpage

# A. Bayesian Inference

## 1.

[6 marks] Read in the data, then for each person produce a histogram of that given person’s reaction times. The range of the x axis should be the same on each histogram. Visually compare the reaction time distributions of schizophrenic and non-schizophrenic individuals. What differences/similarities can you observe? Reference the histograms of specific individuals to support your conclusions.

I will begin by making the dataset more usable and clear.

```{r, data wrangling rtimes, include=TRUE}
# Renaming first column
names(rtimes)[1] <- "PatientType"
# Classifying the patient type, 1-11 non-schiz, 12-17 schiz
rtimes$PatientType <- c(rep("non-schizophrenic", 11), rep("schizophrenic", 6))
rtimes1 <- rtimes
head(rtimes1)
```

### Creating Histograms

We want to extract the data from these columns **rtimes[, 2:31]**, for all 17 patients. We will plot 17 histograms, displaying the distribution of the 30 reaction times.

In order to keep a consistent range across the 17 histograms, we need to find the absolute minimum and maximum values in this dataset. To do this, we extract all the data, 17 rows \* 30 trials.

```{r extracting data points, include = TRUE}
DataValues <- unlist(rtimes1[, 2:31])
range(DataValues)

# We want to store these values

x_min <- 204
x_max <- 1714
```

Below, we can observe two sets of histograms, the first containing the eleven non-schizophrenic patients, and the seconds containing the 6 schizophrenic patients.

We reshape to data frame to long format, as having the reaction time data in one column (a list) makes much easier to code the histogram. To create a separate plot for each patient, we use facet_wrap() to group data by PatientID.

```{r histo format, include=TRUE}
# Reshaping to long format
rtimes_long <- rtimes1 %>%
  pivot_longer(cols = starts_with("T"),  # Columns T1 to T30
               names_to = "Trial", 
               values_to = "ReactionTime") %>%
  mutate(PatientID = rep(1:17, each = 30)) %>%  # Add patientID
  select(PatientID, PatientType, ReactionTime)

# Splitting into two datasets so I can have seperate histogramws
non_schizo <- rtimes_long %>% filter(PatientType == "non-schizophrenic") 
schizo <- rtimes_long %>% filter(PatientType == "schizophrenic")          
```

```{r histograms, fig.width=7, fig.height=5, fig.align="center", include=TRUE}
# Plotting non-schizophrenic histograms
hist1 <- ggplot(non_schizo, aes(x = ReactionTime)) +
  geom_histogram(binwidth = 150, fill = "dodgerblue1", color = "black") +
  facet_wrap(~ PatientID, ncol = 4, scales = "free_y") +  # 11 plots, 4 columns
  coord_cartesian(xlim = c(x_min, x_max)) +  # Fixed x-axis
  labs(title = "Figure 1: Non-Schizophrenic Reaction Times", 
       x = "Reaction Time (ms)", 
       y = "Count") +
  custom_theme

# Plotting schizophrenic histograms
hist2 <- ggplot(schizo, aes(x = ReactionTime)) +
  geom_histogram(binwidth = 150, fill = "sienna2", color = "black") +
  facet_wrap(~ PatientID, ncol = 3, scales = "free_y") +  # 6 plots, 3 columns
  coord_cartesian(xlim = c(x_min, x_max)) +  # Fixed x-axis
  labs(title = "Figure 2: Schizophrenic Reaction Times", 
       x = "Reaction Time (ms)", 
       y = "Count") +
  custom_theme
```

```{r histograms print, fig.width=7, fig.height=5, fig.align="center", echo=FALSE}
print(hist1)
print(hist2)
```

In Figure 1 we can see that Non-schizophrenic patients such as patient 1 have a narrower and concentrated histogram, with reaction times completely clustered around the second bin (150-300ms). For example, every Non-schizophrenic patient's results show that at least two thirds of their reaction times were faster than 300ms.

In contrast, the Schizophrenic patients histograms in Figure 2 exhibit a much wider spread of reaction times, with some patients having reactions times exceeding 1500ms. This indicates greater variability, for instance, patient 14 has a wide range of reaction times with many being greater than 1000ms, whilst also having a significant count below 500ms. This suggests that Schizophrenic patients have a mixture of normal and delayed responses, and this aligns with the theory of attention deficits and motor retardation in schizophrenics. This is generally observed by their inconsistent distributions and right skew, compared to the tighter uniform distributions of non-schizophrenics

Nonetheless, both groups share similarities in their distributions. They both have peaks within the first two bins 0-500ms, as you can see when comparing patient 8 (non) to 13 (schizophrenic). However, only schizophrenics have reaction time values in the higher bands (greater than 800ms)

Overall, these histograms support the hypothesis that schizophrenics experience attention deficits and motor reflex retardation, as seen by the wider spread of values and more irregular histograms for schizophrenic individuals.

## 2. [5 marks] The above model uses the logarithm of measured reaction times. Explain why taking the

logarithm is necessary here (referencing the relevant output), then perform the transformation yourself. For each person compute the standard deviation of the log transformed reaction times of that individual.

-Calculate standard deviations: For each of the 17 people, compute the standard deviation of their 30 log-transformed reaction times.

Log-transforming the reaction times is necessary for the model above for two main reasons. Firstly, there is skewness in the data as seen in the Schizophrenic reaction times histograms, for example, Patient 14 and 15 are right-skewed distributions with extreme values (over 1500ms), whilst non-schizophrenic's have a tighter distribution. Following this, schizophrenics don't seems to follow a normal distribution, thus, log transforming reduces the skewness and helps stabilise the variance from the effect of outliers/skew. This enambles the data to fit the models normal distributions assumption better. Finally, the log-transformation scales the data, making the mean and variance more interpretable. For example, the original range of 204ms to 1714ms, becomes 5.31 to 7.44 (3 s.f.) when log-transformed. Put simply, handling smaller numbers is easier.

```{r log transforming, include=TRUE}
log_rtimes_long <- rtimes_long %>%
  mutate(LogReactionTime = log(ReactionTime))
range(log_rtimes_long$LogReactionTime) 
# checking the range also lets us knowif any observations are negative.
```

Now we compute the standard deviation of each patients 30 log-transformed reaction times. This is useful as it measures the within personal variability on the log scale.

```{r sd log, include = TRUE}
# Compute standard deviation of log-transformed reaction times for each person
sd_log <- log_rtimes_long %>%
  group_by(PatientID) %>%
  summarise(SD_Log_RT = sd(LogReactionTime))

# Print results
head(sd_log)
```

## 3. [5 marks] List the parameters of the model and assign non-informative uniform prior distributions to each parameter, paying attention to the values these parameters are allowed take.

The reason we are using non-informative uniform prior distributions is because we lack prior knowledge about the parameters. Thus, having a reasonable range of values that are equally likely gives a non-bias estimation and allows the data to strong influence the posterior distribution.

**Parameters and Priors:**

## **For Non-Schizophrenic Individuals:**

-   $\alpha_i$ (for $i = 1$ to 17): Person-specific means on the log scale.

```{r echo=TRUE, results = "hide"}
alpha[i] ~ dunif(0, 10)
```

------------------------------------------------------------------------

-   $\mu$: Mean of $\alpha_i$ for non-schizophrenics, which represents the mean log reaction time. with most values between 200ms and 1800ms), when log transformed they equal, 5.3 and 7.5.. A wide range of -10 - 10 to avoid bias and cover potential outliers.

```{r echo=TRUE, results = "hide"}
mu ~ dunif(-10, 10)
```

-   (\sigma\_y\^2): Varience of log-reaction times.

```{r echo=TRUE, results = "hide"}
sigma_y2 ~ dunif(0.001, 5)
# tau_y <- 1 / sigma_y2 
```

-   (\sigma\_\alpha\^2): Varience of $\alpha_i$.

```{r echo=TRUE, results = "hide"}
sigma_alpha2 ~ dunif(0.001, 5)
# tau_alpha <- 1 / sigma_alpha2

```

**For Schizophrenic Individuals:**

-   $\beta$: Additional variable in $\alpha_i$ mean for schizophrenics, which represents the increase in reaction time due to schizophrenia. Although we expect schizophrenics to be slower, there is still the possibility of motor retardation having no difference, or faster reactions (unlikely, but possible). A range of -10 - 10 is wide enough for the data to determine the direction.

```{r echo=TRUE, results = "hide"}
beta ~ dunif(0, 10)
```

-   $\tau$: Log-transformed delay parameter for schizophrenics ($\tau > 0$). This is a wide range, but remains uninformative and is contrained to positive values.

```{r echo=TRUE, results = "hide"}
tau ~ dunif(0, 10)
```

-   $\lambda$: Probability of delay ($0 \leq \lambda \leq 1$). Value must lie between 0 and 1, as $\lambda$ is a probability.

```{r echo=TRUE, results = "hide"}
lambda ~ dunif(0, 1)
```

## 4.

```{r, include = FALSE}
 library(R2jags)
 library(coda)
 library(lattice)
 library(MCMCvis)
 library(tidyverse)
```

First we need to create a data matrix which JAGS can process. We extracting the raw reaction times in the original wide format, and then Log-transforming them. After this, we can create our data list for our JAGS model. Defining separate matrixs for Schizophrenic and non-Schizophrenic reaction times, the number of patients in each group, and the number of trials. It is important to define these now, as we call upon them in the model definition.

```{r, setting up data, include=TRUE}
extractedRtimes <- rtimes[, 2:31] 
log_rtimes <- log(extractedRtimes)

data_list <- list(
  y.ns = log_rtimes[1:11, ],  # Non-schizophrenic reaction times (11 x 30)
  y.s = log_rtimes[12:17, ],  # Schizophrenic reaction times (6 x 30)
  N_ns = 11,                  # Number of non-schizophrenics
  N_s = 6,                    # Number of schizophrenics
  T = 30                      # Number of trials
)

```

### **Model Specification**

Now we define the hierarchical Bayesian model. The model is account for the two groups of patients, their individual variability.

**Reaction times for Non-Schizophrenic patients (N.S)** ($i$th individual) are modeled as:

($i = 1, 2, \ldots, 11$) $$y_{ij} \sim N(\alpha_i, \sigma_y^2), \quad i = 1, 2, \ldots, 11, \quad j = 1, 2, \ldots, 30$$ Where: 

- $y^{ns}_{ij}$ is the **reaction time** for individual $i$ and trial $j$ 
- $\alpha_i$ is individual specific **mean** reaction time 
- $\sigma_y^2$ is the shared **varience** across all trials.

Individuals **mean** ($\alpha_i$) reaction time is: $$\alpha_i \sim N(\mu, \sigma_\alpha^2), \quad i = 1, 2, \ldots, 11$$ - $\mu$ is the mean reaction time for N.S - $\sigma_\alpha^2$ is the intragroup variance in reaction times


**Reaction times for Schizophrenic patients (S)**

$$y_{ij} \sim N(\alpha_i + \tau z_{ij}, \sigma_y^2) \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Where: 

- $\tau$ is the extra delay 
- $z_{ij}$ is an indicator (one or zero)

$$z_{ij} \sim \text{Bernoulli}(\lambda), \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Schizophrenic patients can have delayed responses due to attention deficit. This is modeled through the above Bernoulli distributed indicator function. $\lambda$ is the probability of a trial being delayed.

Schizophrenics mean reaction time: $$\alpha_i \sim N(\mu + \beta, \sigma_\alpha^2), \quad i = 12, 13, \ldots, 17$$ Where: 

- $\beta$ is the increase in mean reaction time for S

### **Defining the JAGS Model**

```{r jags model, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model <- function(){
    # Reaction time of non-schizophrenics
    for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }

    # Reaction time of schizophrenics
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)  # Bernoulli for delay indicator
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) # Corrected indexing
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
    # Priors
    mu ~ dunif(-10, 10) # Mean for non-schizophrenics
    sigma_y2 ~ dunif(0.001, 5) # Avoiding exact 0 for stability
    sigma_alpha2 ~ dunif(0.001, 5) # Avoiding exact 0 for stability
    beta ~ dunif(0, 10) # Schizophrenics should have longer reaction times
    tau ~ dunif(0, 10) # Delay parameter
    lambda ~ dunif(0, 1) # Probability of delay
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}


```

### **Defining the Parameters, what we what to track, and initial values**

```{r inits, include=TRUE}
 jags.param <- c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")

set.seed(123)

inits1 <- list(
    mu = 4, beta = 1, tau = 1, lambda = 0.5, 
    sigma_y2 = 1, sigma_alpha2 = 1, 
    z = matrix(0, nrow = 6, ncol = 30)
)  
inits2 <- list(
    mu = 6, beta = 4, tau = 5, lambda = 0.3, 
    sigma_y2 = 2, sigma_alpha2 = 2, 
    z = matrix(0, nrow = 6, ncol = 30)
)  
jags.inits <- list(inits1, inits2)

```
Explain the thought process behind picking these initial values


### **Fitting the JAGS Model**

In the first JAGS model, we are running two MCMC chains, with 10000 iterations and discarding the first 5000 iterations to view the convergence. We are also computing the Deviance Information Criterion (DIC), so we can compare this model to future ones later.

```{r fitting jags1, include=TRUE}
 jags.mod.fit <- jags(data = data_list, inits = jags.inits,
  parameters.to.save = jags.param, n.chains = 2, n.iter = 10000,
  n.burnin = 5000, n.thin = 1 , model.file = jags.model, DIC = FALSE)
```

## 5. Monte Carlo Marcov Chain

```{r}
jagsfit.mcmc <- as.mcmc(jags.mod.fit)
 MCMCtrace(jagsfit.mcmc,type = 'trace',ind = TRUE, pdf = FALSE)
```

We can see from the trace plots above, that the chains have converged and look like "fuzzy caterpillars". The alpha.s and alpha.ns (mean reaction time for both groups), seem to have converged for every individual, with values ranging from 5.6 to 5.75 (log transformed mean reaction time) for non-schizophrenics, and 5.9 to 6.3 for schizophrenics.

To check for convergence more accurately, we can extract the Gelman-Rubin test statistic values. This is in the code below.

```{r, include=TRUE}
gelman_stats<- gelman.diag(jagsfit.mcmc,multivariate=FALSE)
gelman_df <- as.data.frame(gelman_stats$psrf)
gelmam.params <- c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")
gelman_subset <- gelman_df[gelmam.params, , drop = FALSE]
```
```{r gelman table, fig.width=7, fig.height=5, fig.align="center", echo=FALSE}
library(gt)
library(dplyr)

gelman.diag(jagsfit.mcmc, multivariate = FALSE)$psrf %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Parameter") %>%
  filter(Parameter %in% c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")) %>%
  select(Parameter, `Point est.`, `Upper C.I.`) %>%
  gt() %>%
  tab_header(title = "Gelman-Rubin Statistics for Key Parameters") %>%
  fmt_number(
    columns = c("Point est.", "Upper C.I."), # Original column names here
    decimals = 3
  ) %>%
  cols_label(
    `Point est.` = "Point Estimate",
    `Upper C.I.` = "Upper CI"
  ) %>%
  tab_options(
    table.font.size = px(14),
    heading.title.font.size = px(16),
    heading.title.font.weight = "bold"
  )

```


As you can see above, the value of all the upper confidence intervals are 1 or very close to 1. This indicates convergence. If a value was above 1.1, this would suggest a lack of convergence.

Interestingly, looking at the traceplots for alpha.s[4] (Upper CI of 1.01) and lambda (Upper CI of 1.01), their traceplots look slightly less concentrated and dense around the center, indicating that they aren't as strongly converged as the other nodes.

Since chains have converged, there is no need to update the model to draw new samples.

## 6. **Posterior Distributions**

### Plots

```{r post distri, fig.width=7, fig.height=5, fig.align="center", include=TRUE}
library(MCMCvis)
MCMCtrace(jagsfit.mcmc,
          params = c("beta", "lambda", "mu"),
          type = "density",
          ind = TRUE,        
          pdf = FALSE,         
          main_den = c("Beta (log scale)", 
                       "Lambda (Probability of Delay)", 
                       "Mu (log scale)"))
```

Next, we check to summary statistics.

```{r, include=TRUE, results='hide'}
print(jags.mod.fit$BUGSoutput$summary[c("beta", "lambda", "mu"), ])
```
The n.eff statistics in the table above is a crude measurement for effective sample size, if the value is above 1000, it indicates that you have sample adequacy, enough independent samples for posterior inference. In our case, we can confidently assume that our model has plenty of valid samples shown by the n.eff values greater than 1000(3000, 3000, 2500).

Now we extract the relevant statistics for Beta, Lamba and Tau. It is important to note that Beta and Tau have been in log scale up until this point, so by multiplying them by exponential, we return them to their true values.
This can be seen in the code below.

### Beta

```{r}
pos_beta<-substr(rownames(jags.mod.fit$BUGSoutput$summary),1,4)=='beta'
# Extract posterior mean for mu
beta_mean<-jags.mod.fit$BUGSoutput$summary[pos_beta,1]
# Extract posterior median
beta_median<-jags.mod.fit$BUGSoutput$summary[pos_beta,5]
# Extract 2.5 percentile of the posterior
beta2.5 <- jags.mod.fit$BUGSoutput$summary[pos_beta,3]
# Extract 97.5 percentile of the posterior
beta97.5 <- jags.mod.fit$BUGSoutput$summary[pos_beta,7]
```

### Lambda

```{r}
pos_lambda <- substr(rownames(jags.mod.fit$BUGSoutput$summary), 1, 6) == "lambda"
lambda_mean <- jags.mod.fit$BUGSoutput$summary[pos_lambda, 1]
lambda_median <- jags.mod.fit$BUGSoutput$summary[pos_lambda, 5]
lambda2.5 <- jags.mod.fit$BUGSoutput$summary[pos_lambda,3]
lambda97.5 <- jags.mod.fit$BUGSoutput$summary[pos_lambda,7]

```

### Tau

```{r}
pos_tau <- substr(rownames(jags.mod.fit$BUGSoutput$summary), 1, 3) == "tau"
tau_mean <- jags.mod.fit$BUGSoutput$summary[pos_tau, 1]
tau_median <- jags.mod.fit$BUGSoutput$summary[pos_tau, 5]
tau2.5 <- jags.mod.fit$BUGSoutput$summary[pos_tau,3]
tau97.5 <- jags.mod.fit$BUGSoutput$summary[pos_tau,7]
```

We add the summary statistics to a data frame.

```{r}
df_stats <- data.frame(
  Parameter = c("Beta", "Lambda", "Tau"),
  Median = c(exp(beta_median), lambda_median, exp(tau_median)),
  Lower_95_CI = c(exp(beta2.5), lambda2.5, exp(tau2.5)),
  Upper_95_CI = c(exp(beta97.5), lambda97.5, exp(tau97.5))
)
```

```{r stargazer table 1, echo=FALSE}
stargazer(df_stats, type = "text", summary = FALSE, title = "Reaction Time Parameters", digits = 3 )
```
Based on the "Reaction Time Parameters" table, conclusions about schizophrenic versus non-schizophrenic reaction times emerge, aligning with motor retardation and attention deficit theories. The median \(\beta\) of 1.374 (95% CI: 1.169–1.626) indicates schizophrenic mean reaction times are 37% longer, with the CI above 1 confirming significant slowing, as seen in Person 14’s wide histogram spread (Question 1). 
This supports motor retardation across all trials. The median \(\lambda\) of 0.118 (CI: 0.070–0.181) suggests 11.8% of trials are delayed (7.0%–18.1%), consistent with attention deficits, matching mixed fast/slow times in Person 15. The median \(\tau\) of 2.337 (CI: 2.082–2.643) shows delayed responses are 2.34 times longer, reinforcing significant delays. 

Compared to non-schizophrenics’ tighter distributions (Person 1), schizophrenics’ dual effects (\(\beta\), \(\tau\), \(\lambda\)) validate the model’s two-component structure.


## 7. 

To predict 30 additional log response time measurement **y.pred**.

HERE I SHOULD EDIT THIS TO DEFINE Y PRED
$$y_{ij} \sim N(\alpha_i + \tau z_{ij}, \sigma_y^2) \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Where: 

- $\tau$ is the extra delay 
- $z_{ij}$ is an indicator (one or zero)



```{r 7a, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model2 <- function(){
 for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) 
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
     # Prediction for additional response times for schizophrenic 
      #individuals i = 12 to 17
      for (i in 1:N_s) {
        for (j in 1:T) {
          # Predicted response times
            y.pred[i,j] ~ dnorm(alpha.s[i] + tau * z.pred[i,j], p.y) 
            z.pred[i,j] ~ dbern(lambda)
        }
    }
  
    # Priors
    mu ~ dunif(-10, 10) 
    sigma_y2 ~ dunif(0.001, 5) 
    sigma_alpha2 ~ dunif(0.001, 5) 
    beta ~ dunif(0, 10) 
    tau ~ dunif(0, 10) 
    lambda ~ dunif(0, 1) 
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}

```

Explain what ive added here
new lists
y.pred, we use the poterior distribution of alpha.s[i]
z.pred + inits

########################################################

### b. 

The nodes below track the standard deviations of the 30 predicted measurements for the Schizophrenic individuals. In the JAGS model below, we place sd_pred[i] in the predicted reaction times node, and the min/max are defined separetly as they are logical(?).

- **sd_pred[i] <- sd(y_pred[i, ])** This node tracks the SD of the thirty predicted measurements of each individual

The nodes below track the minimum and maximum standard deviation of predicted measurement across all individuals.
- **Smin <- min(sd_pred[])**
- **Smax <- max(sd_pred[])**
    
    
```{r adding sd and min/max, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model2 <- function(){
 for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) 
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
     # Prediction for additional response times for schizophrenic 
      #individuals i = 12 to 17
      for (i in 1:N_s) {
          for (j in 1:T) {
          # Predicted response times
            y.pred[i,j] ~ dnorm(alpha.s[i] + tau * z.pred[i,j], p.y) 
            z.pred[i,j] ~ dbern(lambda)
          }
                sd.pred[i] <- sd(y.pred[i, ])
    }
    # Compute min and max of standard deviations
    Smin <- min(sd.pred[])
    Smax <- max(sd.pred[])
    
    # Priors
    mu ~ dunif(-10, 10) 
    sigma_y2 ~ dunif(0.001, 5) 
    sigma_alpha2 ~ dunif(0.001, 5) 
    beta ~ dunif(0, 10) 
    tau ~ dunif(0, 10) 
    lambda ~ dunif(0, 1) 
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}
```


### c. 

Adding new standard deviation parameters to track.

```{r inits2, include=TRUE}
jags.param2 <- c("mu", "beta", "tau", "lambda", "p.y", "p.alpha", "Smin", "Smax")

set.seed(123)

inits1 <- list(
    mu = 4, beta = 1, tau = 1, lambda = 0.5, 
    sigma_y2 = 1, sigma_alpha2 = 1, 
    z = matrix(0, nrow = 6, ncol = 30),
    z.pred = matrix(0, nrow = 6, ncol = 30)
)  
inits2 <- list(
    mu = 6, beta = 4, tau = 5, lambda = 0.3, 
    sigma_y2 = 2, sigma_alpha2 = 2, 
    z = matrix(0, nrow = 6, ncol = 30),
    z.pred = matrix(0, nrow = 6, ncol = 30)
)  
jags.inits <- list(inits1, inits2)
```

Now we run the model, with two chains, 6000 iterations, but discarding 5000 as burnin.

```{r fitting jags2, include=TRUE, results='hide'}
 jags.mod.fit2 <- jags(data = data_list, inits = jags.inits,
  parameters.to.save = jags.param2, n.chains = 2, n.iter = 6000,
  n.burnin = 5000, n.thin = 1 , model.file = jags.model2, DIC = FALSE)
```
```{r}
# Display results
print(jags.mod.fit2)
```

## d. 

Extract the minimum and maximum standard deviation values from the fitted model, and produce
 a scatterplot of the (Smin,Smax) pairs. (Note that each iteration of the model fit will produce
 a minimum-maximum pair). Find the minimum and maximum of the raw standard deviation
 estimates obtained in Question 2, and add an additional point to your scatterplot showing this
 raw minimum-maximum pair.
 
Now we extract the minimum and maximum standard deviation values from the fitted model.

```{r, fig.width=7, fig.height=5, fig.align="center"}
# Extract Smin and Smax from MCMC samples
smin_samples <- jags.mod.fit2$BUGSoutput$sims.list$Smin
smax_samples <- jags.mod.fit2$BUGSoutput$sims.list$Smax

scatter_data <- data.frame(Smin = smin_samples, Smax = smax_samples)

# Calculate raw minimum and maximum standard deviation values from Question 2
raw_min_sd <- min(sd_log$SD_Log_RT[12:17])  # Minimum SD for schizophrenic group
raw_max_sd <- max(sd_log$SD_Log_RT[12:17])  # Maximum SD for schizophrenic group

# Create scatter plot
ggplot(scatter_data, aes(x = Smin, y = Smax)) +
  geom_point(alpha = 0.5, color = "blue") +  # MCMC Samples
  geom_point(aes(x = raw_min_sd, y = raw_max_sd), color = "red", size = 3) + 
  labs(
    title = "Scatterplot of (Smin, Smax) Pairs",
    x = "Minimum Predicted Standard Deviation (Smin)",
    y = "Maximum Predicted Standard Deviation (Smax)"
  ) +
  custom_theme
```

The scatterplot in Figure X displays the relationship between the minimum (Smin) and maximum (Smax) standard deviations of the predicted response times for schizophrenic individuals, obtained from the posterior samples of the JAGS model. Each blue point represents an (Smin, Smax) pair from a single iteration of the Markov Chain Monte Carlo (MCMC) sampling process. The red point corresponds to the empirical minimum and maximum standard deviations computed from the original dataset.

Interpretation of the Scatterplot
The majority of the posterior samples (blue points) form a dense cluster, indicating the range of predicted variability for within-person reaction times. However, the red point lies outside this main distribution, suggesting that the observed reaction time variability is greater than what the model predicts. This discrepancy indicates that the model may underestimate the true within-person variability of schizophrenic individuals.

Possible reasons for this underestimation include:

Restrictive prior assumptions on the variance parameter 
𝜎
𝑦
σ 
y
​
 , which may be limiting the spread of predicted reaction times.
The mixture model structure may not fully capture the extent of individual differences in reaction time delays.
Potential bias in the assumed normal distributions, which may not sufficiently accommodate extreme reaction times observed in schizophrenic individuals.
Conclusion: Can the Model Explain the Variability?
The positioning of the red point outside the cloud of posterior samples suggests that the model does not fully explain the observed variability in schizophrenic reaction times. While the model captures general trends, it likely underestimates reaction time dispersion. Refinements to the prior distributions, particularly on 
𝜎
𝑦
σ 
y
​
 , or adjustments to the model structure (such as incorporating a more flexible variance structure) may be necessary to improve model fit.


## 8.  **double check**



# B. Classification


The following figure shows the information in the dataset Classification.csv- it shows two different groups, plotted against two explanatory variables. This is simulated data - the groupings are determined by a known function of X1 and X2 with added noise/random error.

```{r  fig.width=7, fig.height=5, fig.align="center"}
ggplot(Classification, aes(x = X1, y = X2, color = as.factor(Group))) + 
  geom_point(alpha = 0.8) + 
  labs(
    title = "Scatterplot of Classification Data",
    x = "X1",
    y = "X2"
  ) +
  custom_theme
```


## 1. Create meaningful summaries

First, we use the summary function to get a brief overview of the dataset. We can see that the X1 and X2 are the explanatory variables. To understand how the groups are classified, we will create a more detailed numerical summary.
```{r results='hide'}
summary(Classification)
```

```{r include=FALSE}
num_summary <- Classification %>%
  group_by(Group) %>%
  summarise(
    Mean_X1 = mean(X1),
    SD_X1 = sd(X1),
    Median_X1 = median(X1),
    Min_X1 = min(X1),
    Max_X1 = max(X1),
    Mean_X2 = mean(X2),
    SD_X2 = sd(X2),
    Median_X2 = median(X2),
    Min_X2 = min(X2),
    Max_X2 = max(X2)
  )
```
```{r include=FALSE}
# Load necessary libraries
library(gt)
library(dplyr)

# Create separate summaries for X1 and X2
classification_summary_X1 <- Classification %>%
  group_by(Group) %>%
  summarise(
    `Mean X1` = mean(X1),
    `SD X1` = sd(X1),
    `Median X1` = median(X1),
    `Min X1` = min(X1),
    `Max X1` = max(X1)
  ) %>%
  mutate(Group = ifelse(Group == 0, "Group 0", "Group 1")) %>%
  relocate(Group)

classification_summary_X2 <- Classification %>%
  group_by(Group) %>%
  summarise(
    `Mean X2` = mean(X2),
    `SD X2` = sd(X2),
    `Median X2` = median(X2),
    `Min X2` = min(X2),
    `Max X2` = max(X2)
  ) %>%
  mutate(Group = ifelse(Group == 0, "Group 0", "Group 1")) %>%
  relocate(Group)
```
```{r echo=FALSE}
# GT table for X1 only
gt_table_X1 <- classification_summary_X1 %>%
  gt() %>%
  tab_header(
    title = md("**Summary Statistics for Group 0 and Group 1**"),
    subtitle = md("*Comparison of X1 Across Groups*")
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = px(10),
    table.width = pct(90),
    heading.align = "center",
    column_labels.font.weight = "bold",
    column_labels.border.bottom.width = px(1),
    column_labels.border.bottom.color = "black",
    table.border.top.width = px(1),
    table.border.top.color = "black",
    table.border.bottom.width = px(1),
    table.border.bottom.color = "black",
    data_row.padding = px(4)
  ) %>%
  cols_align(
    align = "center", columns = where(is.numeric)
  ) %>%
  opt_table_lines() %>%
  opt_row_striping()

# GT table for X2 only
gt_table_X2 <- classification_summary_X2 %>%
  gt() %>%
  tab_header(
    title = md("**Summary Statistics for Group 0 and Group 1**"),
    subtitle = md("*Comparison of X2 Across Groups*")
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = px(10),
    table.width = pct(90),
    heading.align = "center",
    column_labels.font.weight = "bold",
    column_labels.border.bottom.width = px(1),
    column_labels.border.bottom.color = "black",
    table.border.top.width = px(1),
    table.border.top.color = "black",
    table.border.bottom.width = px(1),
    table.border.bottom.color = "black",
    data_row.padding = px(4)
  ) %>%
  cols_align(
    align = "center", columns = where(is.numeric)
  ) %>%
  opt_table_lines() %>%
  opt_row_striping()
```

```{r  fig.width=7, fig.height=5, fig.align="center"}
gt_table_X1

```

```{r fig.width=7, fig.height=5, fig.align="center"}
gt_table_X2

```
### **Key Observations from the Summary Statistics:**
X1 Differences Across Groups:

Group 0 has a higher mean X1 (0.26) compared to Group 1 (-0.12).
However, Group 1 exhibits a higher standard deviation (1.12 vs. 0.54 for Group 0), indicating greater variability.
Group 1 has more extreme X1 values (Min = -3.06, Max = 3.29), while Group 0’s range is smaller (Min = -1.09, Max = 1.64).
Group 1’s median X1 (-0.30) is lower than Group 0’s (0.26), suggesting a shift in distribution.
X2 Differences Across Groups:

Group 1 has a higher mean X2 (0.30) compared to Group 0 (-0.60), suggesting a more positive shift.
The standard deviations are similar (0.89 for Group 0 vs. 0.93 for Group 1), implying that X2 has comparable spread in both groups.
The range of X2 is wider in Group 1 (-3.41 to 3.94) than in Group 0 (-3.39 to 1.84).
The median X2 values reinforce this trend, with Group 0’s median at -0.60 and Group 1’s median at 0.31.
Summary of Findings
X1 shows higher variability in Group 1, with more extreme values, while Group 0 is more centered.
X2 is generally more positive in Group 1 than in Group 0.
The spread in X1 suggests that the data is not completely linearly separable, which influences the choice of classification method.

Given the numerical summaries and scatter plot, the choice of classification method depends on the distribution and separability of X1 and X2 between the two groups:

The data and grouping appears to have non-linear decision boundary, as group 1 semi-circularly surrounds group 0. Taking the numerical summary and data relationship into account, we can believe that the following methods are suitable:

1. Quadratic Discriminant Analysis - The quadratic nature of the QDA means the decision boundaries can capture complex non-linear separation between classes.

2. K-Nearest Neighbours (KNN) - This method is very good for classifying data with non-linear decision boundaries. The boundary is determined by K, this method is flexible, but can make you prone to over fitting if K is too high. That being said, it K can also be optimized through cross validation. 

3. Support Vector Machines (SVM) - SVM can handle both linear and nonlinear boundaries by utilizing kernel methods. Nonlinear kernels (e.g., polynomial or radial basis function) allow SVM to effectively classify complex datasets by mapping data into higher-dimensional spaces, enabling separation of classes that aren't linearly separable in the original feature space. The central idea behind SVM is finding an optimal hyperplane that maximizes the margin—the distance between the nearest points (support vectors) of the two classes.

4. Random Forests (RF) - Random forests are particularly suitable due to their robust handling of nonlinear and complex decision boundaries. RF generally achieves stable, high-quality predictions and is suitable for complex datasets with potential nonlinear relationships between predictors and the target class​




## 2. Select75%ofthedatatoactasatrainingset,withtheremaining25%fortesting/evaluation.

```{r}
library(dplyr)
 library(caret)
 library(e1071)
```
```{r}
 # Define function that will split the data into training and test sets
 trainTestSplit <- function(df, trainPercent,seed1){
 ## Sample size percent
 smp_size <- floor(trainPercent/100 * nrow(df))
 ## set the seed
 set.seed(seed1)
 train_ind <- sample(seq_len(nrow(df)), size = smp_size)
 train_ind
 }

#Split as training and test sets
 train_ind <- trainTestSplit (Classification, trainPercent = 75, seed = 123)
 train <- Classification[train_ind ,]
 test <- Classification[-train_ind ,]
```


## 3. 



### **Quadratic Discriminant Analysis**


### **K-Nearest Neighbours**


### **Support Vector Machines**


### **Random Forests**














































































