---
title: "**MTHM017 - Advanced Topics in Statistics, Assignment**"
author: "**James R Lewis**"
date: "**00-00-2025**"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["amsmath", "amssymb", "mathtools", "bm", "caption", "hyperref"]
    includes:
      in_header: preamble.tex
    toc: true
    toc_depth: 2
    number_sections: false
  html_document:
    css: style.css
  word_document: default
  header-includes:
  - \usepackage{mathtools}  # Move this to the top
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{bm}
  - \usepackage{geometry}
  - \geometry{a4paper, margin=1in}
  - \usepackage{setspace}
  - \onehalfspacing

classoption: a4paper,11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading datasets and libraries, include=FALSE}
rtimes <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/rtimes.csv")

ClassificationTrue <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/ClassificationTrue.csv")

Classification <- read.csv("~/GitHub/university-projects/Advanced Topics In Statistics/Assignment/Classification.csv")

library(tidyverse)
library(ggplot2)
library(rmarkdown)
library(stargazer)

```

**Declaration of AI Assistance**: I have used OpenAI’s ChatGPT tool in creating this report.

AI-supported/AI-integrated use is permitted in this assessment. I acknowledge the following uses of GenAI tools in this assessment:

1.  I have used GenAI tools to check and debug my code.
2.  I have used GenAI tools to proofread and correct grammar or spelling errors.
3.  I have used GenAI tools to give me feedback on a draft.

I declare that I have referenced use of GenAI outputs within my assessment in line with the University referencing guidelines.

```{r setting plot theme, include = FALSE}
custom_theme <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),  # Centered title
    axis.title = element_text(face = "bold", size = 12),
    axis.text = element_text(size = 11), 
    panel.grid.major = element_line(color = "gray85"),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.8)
  )
```

\newpage

# A. Bayesian Inference

## 1.

[6 marks] Read in the data, then for each person produce a histogram of that given person’s reaction times. The range of the x axis should be the same on each histogram. Visually compare the reaction time distributions of schizophrenic and non-schizophrenic individuals. What differences/similarities can you observe? Reference the histograms of specific individuals to support your conclusions.

I will begin by making the dataset more usable and clear.

```{r, data wrangling rtimes, include=TRUE}
# Renaming first column
names(rtimes)[1] <- "PatientType"
# Classifying the patient type, 1-11 non-schiz, 12-17 schiz
rtimes$PatientType <- c(rep("non-schizophrenic", 11), rep("schizophrenic", 6))
rtimes1 <- rtimes
head(rtimes1)
```

### Creating Histograms

We want to extract the data from these columns **rtimes[, 2:31]**, for all 17 patients. We will plot 17 histograms, displaying the distribution of the 30 reaction times.

In order to keep a consistent range across the 17 histograms, we need to find the absolute minimum and maximum values in this dataset. To do this, we extract all the data, 17 rows \* 30 trials.

```{r extracting data points, include = TRUE}
DataValues <- unlist(rtimes1[, 2:31])
range(DataValues)

# We want to store these values

x_min <- 204
x_max <- 1714
```

Below, we can observe two sets of histograms, the first containing the eleven non-schizophrenic patients, and the seconds containing the 6 schizophrenic patients.

We reshape to data frame to long format, as having the reaction time data in one column (a list) makes much easier to code the histogram. To create a separate plot for each patient, we use facet_wrap() to group data by PatientID.

```{r histo format, include=TRUE}
# Reshaping to long format
rtimes_long <- rtimes1 %>%
  pivot_longer(cols = starts_with("T"),  # Columns T1 to T30
               names_to = "Trial", 
               values_to = "ReactionTime") %>%
  mutate(PatientID = rep(1:17, each = 30)) %>%  # Add patientID
  select(PatientID, PatientType, ReactionTime)

# Splitting into two datasets so I can have seperate histogramws
non_schizo <- rtimes_long %>% filter(PatientType == "non-schizophrenic") 
schizo <- rtimes_long %>% filter(PatientType == "schizophrenic")          
```

```{r histograms, fig.width=7, fig.height=5, fig.align="center", include=TRUE}
# Plotting non-schizophrenic histograms
hist1 <- ggplot(non_schizo, aes(x = ReactionTime)) +
  geom_histogram(binwidth = 150, fill = "dodgerblue1", color = "black") +
  facet_wrap(~ PatientID, ncol = 4, scales = "free_y") +  # 11 plots, 4 columns
  coord_cartesian(xlim = c(x_min, x_max)) +  # Fixed x-axis
  labs(title = "Figure 1: Non-Schizophrenic Reaction Times", 
       x = "Reaction Time (ms)", 
       y = "Count") +
  custom_theme

# Plotting schizophrenic histograms
hist2 <- ggplot(schizo, aes(x = ReactionTime)) +
  geom_histogram(binwidth = 150, fill = "sienna2", color = "black") +
  facet_wrap(~ PatientID, ncol = 3, scales = "free_y") +  # 6 plots, 3 columns
  coord_cartesian(xlim = c(x_min, x_max)) +  # Fixed x-axis
  labs(title = "Figure 2: Schizophrenic Reaction Times", 
       x = "Reaction Time (ms)", 
       y = "Count") +
  custom_theme
```

```{r histograms print, fig.width=7, fig.height=5, fig.align="center", echo=FALSE}
print(hist1)
print(hist2)
```

In Figure 1 we can see that Non-schizophrenic patients such as patient 1 have a narrower and concentrated histogram, with reaction times completely clustered around the second bin (150-300ms). For example, every Non-schizophrenic patient's results show that at least two thirds of their reaction times were faster than 300ms.

In contrast, the Schizophrenic patients histograms in Figure 2 exhibit a much wider spread of reaction times, with some patients having reactions times exceeding 1500ms. This indicates greater variability, for instance, patient 14 has a wide range of reaction times with many being greater than 1000ms, whilst also having a significant count below 500ms. This suggests that Schizophrenic patients have a mixture of normal and delayed responses, and this aligns with the theory of attention deficits and motor retardation in schizophrenics. This is generally observed by their inconsistent distributions and right skew, compared to the tighter uniform distributions of non-schizophrenics

Nonetheless, both groups share similarities in their distributions. They both have peaks within the first two bins 0-500ms, as you can see when comparing patient 8 (non) to 13 (schizophrenic). However, only schizophrenics have reaction time values in the higher bands (greater than 800ms)

Overall, these histograms support the hypothesis that schizophrenics experience attention deficits and motor reflex retardation, as seen by the wider spread of values and more irregular histograms for schizophrenic individuals.

## 2. [5 marks] The above model uses the logarithm of measured reaction times. Explain why taking the

logarithm is necessary here (referencing the relevant output), then perform the transformation yourself. For each person compute the standard deviation of the log transformed reaction times of that individual.

-Calculate standard deviations: For each of the 17 people, compute the standard deviation of their 30 log-transformed reaction times.

Log-transforming the reaction times is necessary for the model above for two main reasons. Firstly, there is skewness in the data as seen in the Schizophrenic reaction times histograms, for example, Patient 14 and 15 are right-skewed distributions with extreme values (over 1500ms), whilst non-schizophrenic's have a tighter distribution. Following this, schizophrenics don't seems to follow a normal distribution, thus, log transforming reduces the skewness and helps stabilise the variance from the effect of outliers/skew. This enambles the data to fit the models normal distributions assumption better. Finally, the log-transformation scales the data, making the mean and variance more interpretable. For example, the original range of 204ms to 1714ms, becomes 5.31 to 7.44 (3 s.f.) when log-transformed. Put simply, handling smaller numbers is easier.

```{r log transforming, include=TRUE}
log_rtimes_long <- rtimes_long %>%
  mutate(LogReactionTime = log(ReactionTime))
range(log_rtimes_long$LogReactionTime) 
# checking the range also lets us knowif any observations are negative.
```

Now we compute the standard deviation of each patients 30 log-transformed reaction times. This is useful as it measures the within personal variability on the log scale.

```{r sd log, include = TRUE}
# Compute standard deviation of log-transformed reaction times for each person
sd_log <- log_rtimes_long %>%
  group_by(PatientID) %>%
  summarise(SD_Log_RT = sd(LogReactionTime))

# Print results
head(sd_log)
```

## 3. [5 marks] List the parameters of the model and assign non-informative uniform prior distributions to each parameter, paying attention to the values these parameters are allowed take.

The reason we are using non-informative uniform prior distributions is because we lack prior knowledge about the parameters. Thus, having a reasonable range of values that are equally likely gives a non-bias estimation and allows the data to strong influence the posterior distribution.

**Parameters and Priors:**

## **For Non-Schizophrenic Individuals:**

-   $\alpha_i$ (for $i = 1$ to 17): Person-specific means on the log scale.

```{r echo=TRUE, results = "hide"}
alpha[i] ~ dunif(0, 10)
```

------------------------------------------------------------------------

-   $\mu$: Mean of $\alpha_i$ for non-schizophrenics, which represents the mean log reaction time. with most values between 200ms and 1800ms), when log transformed they equal, 5.3 and 7.5.. A wide range of -10 - 10 to avoid bias and cover potential outliers.

```{r echo=TRUE, results = "hide"}
mu ~ dunif(-10, 10)
```

-   (\sigma\_y\^2): Varience of log-reaction times.

```{r echo=TRUE, results = "hide"}
sigma_y2 ~ dunif(0.001, 5)
# tau_y <- 1 / sigma_y2 
```

-   (\sigma\_\alpha\^2): Varience of $\alpha_i$.

```{r echo=TRUE, results = "hide"}
sigma_alpha2 ~ dunif(0.001, 5)
# tau_alpha <- 1 / sigma_alpha2

```

**For Schizophrenic Individuals:**

-   $\beta$: Additional variable in $\alpha_i$ mean for schizophrenics, which represents the increase in reaction time due to schizophrenia. Although we expect schizophrenics to be slower, there is still the possibility of motor retardation having no difference, or faster reactions (unlikely, but possible). A range of -10 - 10 is wide enough for the data to determine the direction.

```{r echo=TRUE, results = "hide"}
beta ~ dunif(0, 10)
```

-   $\tau$: Log-transformed delay parameter for schizophrenics ($\tau > 0$). This is a wide range, but remains uninformative and is contrained to positive values.

```{r echo=TRUE, results = "hide"}
tau ~ dunif(0, 10)
```

-   $\lambda$: Probability of delay ($0 \leq \lambda \leq 1$). Value must lie between 0 and 1, as $\lambda$ is a probability.

```{r echo=TRUE, results = "hide"}
lambda ~ dunif(0, 1)
```

## 4.

```{r, include = FALSE}
 library(R2jags)
 library(coda)
 library(lattice)
 library(MCMCvis)
 library(tidyverse)
```

First we need to create a data matrix which JAGS can process. We extracting the raw reaction times in the original wide format, and then Log-transforming them. After this, we can create our data list for our JAGS model. Defining separate matrixs for Schizophrenic and non-Schizophrenic reaction times, the number of patients in each group, and the number of trials. It is important to define these now, as we call upon them in the model definition.

```{r, setting up data, include=TRUE}
extractedRtimes <- rtimes[, 2:31] 
log_rtimes <- log(extractedRtimes)

data_list <- list(
  y.ns = log_rtimes[1:11, ],  # Non-schizophrenic reaction times (11 x 30)
  y.s = log_rtimes[12:17, ],  # Schizophrenic reaction times (6 x 30)
  N_ns = 11,                  # Number of non-schizophrenics
  N_s = 6,                    # Number of schizophrenics
  T = 30                      # Number of trials
)

```

### **Model Specification**

Now we define the hierarchical Bayesian model. The model is account for the two groups of patients, their individual variability.

**Reaction times for Non-Schizophrenic patients (N.S)** ($i$th individual) are modeled as:

($i = 1, 2, \ldots, 11$) $$y_{ij} \sim N(\alpha_i, \sigma_y^2), \quad i = 1, 2, \ldots, 11, \quad j = 1, 2, \ldots, 30$$ Where: 

- $y^{ns}_{ij}$ is the **reaction time** for individual $i$ and trial $j$ 
- $\alpha_i$ is individual specific **mean** reaction time 
- $\sigma_y^2$ is the shared **varience** across all trials.

Individuals **mean** ($\alpha_i$) reaction time is: $$\alpha_i \sim N(\mu, \sigma_\alpha^2), \quad i = 1, 2, \ldots, 11$$ - $\mu$ is the mean reaction time for N.S - $\sigma_\alpha^2$ is the intragroup variance in reaction times


**Reaction times for Schizophrenic patients (S)**

$$y_{ij} \sim N(\alpha_i + \tau z_{ij}, \sigma_y^2) \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Where: 

- $\tau$ is the extra delay 
- $z_{ij}$ is an indicator (one or zero)

$$z_{ij} \sim \text{Bernoulli}(\lambda), \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Schizophrenic patients can have delayed responses due to attention deficit. This is modeled through the above Bernoulli distributed indicator function. $\lambda$ is the probability of a trial being delayed.

Schizophrenics mean reaction time: $$\alpha_i \sim N(\mu + \beta, \sigma_\alpha^2), \quad i = 12, 13, \ldots, 17$$ Where: 

- $\beta$ is the increase in mean reaction time for S

### **Defining the JAGS Model**

```{r jags model, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model <- function(){
    # Reaction time of non-schizophrenics
    for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }

    # Reaction time of schizophrenics
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)  # Bernoulli for delay indicator
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) # Corrected indexing
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
    # Priors
    mu ~ dunif(-10, 10) # Mean for non-schizophrenics
    sigma_y2 ~ dunif(0.001, 5) # Avoiding exact 0 for stability
    sigma_alpha2 ~ dunif(0.001, 5) # Avoiding exact 0 for stability
    beta ~ dunif(0, 10) # Schizophrenics should have longer reaction times
    tau ~ dunif(0, 10) # Delay parameter
    lambda ~ dunif(0, 1) # Probability of delay
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}


```

### **Defining the Parameters, what we what to track, and initial values**

```{r inits, include=TRUE}
 jags.param <- c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")

set.seed(123)

inits1 <- list(
    mu = 4, beta = 1, tau = 1, lambda = 0.5, 
    sigma_y2 = 1, sigma_alpha2 = 1, 
    z = matrix(0, nrow = 6, ncol = 30)
)  
inits2 <- list(
    mu = 6, beta = 4, tau = 5, lambda = 0.3, 
    sigma_y2 = 2, sigma_alpha2 = 2, 
    z = matrix(0, nrow = 6, ncol = 30)
)  
jags.inits <- list(inits1, inits2)

```
Explain the thought process behind picking these initial values


### **Fitting the JAGS Model**

In the first JAGS model, we are running two MCMC chains, with 10000 iterations and discarding the first 5000 iterations to view the convergence. We are also computing the Deviance Information Criterion (DIC), so we can compare this model to future ones later.

```{r fitting jags1, include=TRUE}
 jags.mod.fit <- jags(data = data_list, inits = jags.inits,
  parameters.to.save = jags.param, n.chains = 2, n.iter = 10000,
  n.burnin = 5000, n.thin = 1 , model.file = jags.model, DIC = FALSE)
```

## 5. Monte Carlo Marcov Chain

```{r}
jagsfit.mcmc <- as.mcmc(jags.mod.fit)
 MCMCtrace(jagsfit.mcmc,type = 'trace',ind = TRUE, pdf = FALSE)
```

We can see from the trace plots above, that the chains have converged and look like "fuzzy caterpillars". The alpha.s and alpha.ns (mean reaction time for both groups), seem to have converged for every individual, with values ranging from 5.6 to 5.75 (log transformed mean reaction time) for non-schizophrenics, and 5.9 to 6.3 for schizophrenics.

To check for convergence more accurately, we can extract the Gelman-Rubin test statistic values. This is in the code below.

```{r, include=TRUE}
gelman_stats<- gelman.diag(jagsfit.mcmc,multivariate=FALSE)
gelman_df <- as.data.frame(gelman_stats$psrf)
gelmam.params <- c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")
gelman_subset <- gelman_df[gelmam.params, , drop = FALSE]
```
```{r gelman table, fig.width=7, fig.height=5, fig.align="center", echo=FALSE}
library(gt)
library(dplyr)

gelman.diag(jagsfit.mcmc, multivariate = FALSE)$psrf %>%
  as.data.frame() %>%
  tibble::rownames_to_column(var = "Parameter") %>%
  filter(Parameter %in% c("mu", "beta", "tau", "lambda", "sigma_y2", "sigma_alpha2")) %>%
  select(Parameter, `Point est.`, `Upper C.I.`) %>%
  gt() %>%
  tab_header(title = "Gelman-Rubin Statistics for Key Parameters") %>%
  fmt_number(
    columns = c("Point est.", "Upper C.I."), # Original column names here
    decimals = 3
  ) %>%
  cols_label(
    `Point est.` = "Point Estimate",
    `Upper C.I.` = "Upper CI"
  ) %>%
  tab_options(
    table.font.size = px(14),
    heading.title.font.size = px(16),
    heading.title.font.weight = "bold"
  )

```


As you can see above, the value of all the upper confidence intervals are 1 or very close to 1. This indicates convergence. If a value was above 1.1, this would suggest a lack of convergence.

Interestingly, looking at the traceplots for alpha.s[4] (Upper CI of 1.01) and lambda (Upper CI of 1.01), their traceplots look slightly less concentrated and dense around the center, indicating that they aren't as strongly converged as the other nodes.

Since chains have converged, there is no need to update the model to draw new samples.

## 6. **Posterior Distributions**

### Plots

```{r post distri, fig.width=7, fig.height=5, fig.align="center", include=TRUE}
library(MCMCvis)
MCMCtrace(jagsfit.mcmc,
          params = c("beta", "lambda", "mu"),
          type = "density",
          ind = TRUE,        
          pdf = FALSE,         
          main_den = c("Beta (log scale)", 
                       "Lambda (Probability of Delay)", 
                       "Mu (log scale)"))
```

Next, we check to summary statistics.

```{r, include=TRUE, }
print(jags.mod.fit$BUGSoutput$summary[c("beta", "lambda", "mu"), ])
```
The n.eff statistics in the table above is a crude measurement for effective sample size, if the value is above 1000, it indicates that you have sample adequacy, enough independent samples for posterior inference. In our case, we can confidently assume that our model has plenty of valid samples shown by the n.eff values greater than 1000(3000, 3000, 2500).

To confirm this, we can check to see if the MCMC standard error is 1-5% of the posterior standard deviation. We do this by dividing Time-series SD by SD.

```{r }
summary(jagsfit.mcmc)
```
```{r}
SampleCheck <- summary(jagsfit.mcmc)
stats <- SampleCheck$statistics

sd_values <- SampleCheck$statistics[, "SD"]
se_values <- SampleCheck$statistics[, "Time-series SE"]

MC_Error <- (se_values / sd_values) * 100
```

```{r, fig.align='center'}
MC_Error[c("beta", "lambda", "tau")]
```


Now we extract the relevant statistics for Beta, Lamba and Tau. It is important to note that Beta and Tau have been in log scale up until this point, so by multiplying them by exponential, we return them to their true values.




This can be seen in the code below.

### Beta

```{r}
pos_beta<-substr(rownames(jags.mod.fit$BUGSoutput$summary),1,4)=='beta'
# Extract posterior mean for mu
beta_mean<-jags.mod.fit$BUGSoutput$summary[pos_beta,1]
# Extract posterior median
beta_median<-jags.mod.fit$BUGSoutput$summary[pos_beta,5]
# Extract 2.5 percentile of the posterior
beta2.5 <- jags.mod.fit$BUGSoutput$summary[pos_beta,3]
# Extract 97.5 percentile of the posterior
beta97.5 <- jags.mod.fit$BUGSoutput$summary[pos_beta,7]
```

### Lambda

```{r}
pos_lambda <- substr(rownames(jags.mod.fit$BUGSoutput$summary), 1, 6) == "lambda"
lambda_mean <- jags.mod.fit$BUGSoutput$summary[pos_lambda, 1]
lambda_median <- jags.mod.fit$BUGSoutput$summary[pos_lambda, 5]
lambda2.5 <- jags.mod.fit$BUGSoutput$summary[pos_lambda,3]
lambda97.5 <- jags.mod.fit$BUGSoutput$summary[pos_lambda,7]

```

### Tau

```{r}
pos_tau <- substr(rownames(jags.mod.fit$BUGSoutput$summary), 1, 3) == "tau"
tau_mean <- jags.mod.fit$BUGSoutput$summary[pos_tau, 1]
tau_median <- jags.mod.fit$BUGSoutput$summary[pos_tau, 5]
tau2.5 <- jags.mod.fit$BUGSoutput$summary[pos_tau,3]
tau97.5 <- jags.mod.fit$BUGSoutput$summary[pos_tau,7]
```

We add the summary statistics to a data frame.

```{r}
df_stats <- data.frame(
  Parameter = c("Beta", "Lambda", "Tau"),
  Median = c(exp(beta_median), lambda_median, exp(tau_median)),
  Lower_95_CI = c(exp(beta2.5), lambda2.5, exp(tau2.5)),
  Upper_95_CI = c(exp(beta97.5), lambda97.5, exp(tau97.5))
)
```

```{r stargazer table 1, echo=FALSE}
stargazer(df_stats, type = "text", summary = FALSE, title = "Reaction Time Parameters", digits = 3 )
```
Based on the "Reaction Time Parameters" table, conclusions about schizophrenic versus non-schizophrenic reaction times emerge, aligning with motor retardation and attention deficit theories. The median \(\beta\) of 1.374 (95% CI: 1.169–1.626) indicates schizophrenic mean reaction times are 37% longer, with the CI above 1 confirming significant slowing, as seen in Person 14’s wide histogram spread (Question 1). 
This supports motor retardation across all trials. The median \(\lambda\) of 0.118 (CI: 0.070–0.181) suggests 11.8% of trials are delayed (7.0%–18.1%), consistent with attention deficits, matching mixed fast/slow times in Person 15. The median \(\tau\) of 2.337 (CI: 2.082–2.643) shows delayed responses are 2.34 times longer, reinforcing significant delays. 

Compared to non-schizophrenics’ tighter distributions (Person 1), schizophrenics’ dual effects (\(\beta\), \(\tau\), \(\lambda\)) validate the model’s two-component structure.


## 7. 

To predict 30 additional log response time measurement **y.pred**.

HERE I SHOULD EDIT THIS TO DEFINE Y PRED
$$y_{ij} \sim N(\alpha_i + \tau z_{ij}, \sigma_y^2) \quad i = 12, 13, \ldots, 17, \quad j = 1, 2, \ldots, 30$$ Where: 

- $\tau$ is the extra delay 
- $z_{ij}$ is an indicator (one or zero)



```{r 7a, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model2 <- function(){
 for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) 
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
     # Prediction for additional response times for schizophrenic 
      #individuals i = 12 to 17
      for (i in 1:N_s) {
        for (j in 1:T) {
          # Predicted response times
            y.pred[i,j] ~ dnorm(alpha.s[i] + tau * z.pred[i,j], p.y) 
            z.pred[i,j] ~ dbern(lambda)
        }
    }
  
    # Priors
    mu ~ dunif(-10, 10) 
    sigma_y2 ~ dunif(0.001, 5) 
    sigma_alpha2 ~ dunif(0.001, 5) 
    beta ~ dunif(0, 10) 
    tau ~ dunif(0, 10) 
    lambda ~ dunif(0, 1) 
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}

```

Explain what ive added here
new lists
y.pred, we use the poterior distribution of alpha.s[i]
z.pred + inits

########################################################

### b. 

The nodes below track the standard deviations of the 30 predicted measurements for the Schizophrenic individuals. In the JAGS model below, we place sd_pred[i] in the predicted reaction times node, and the min/max are defined separetly as they are logical(?).

- **sd_pred[i] <- sd(y_pred[i, ])** This node tracks the SD of the thirty predicted measurements of each individual

The nodes below track the minimum and maximum standard deviation of predicted measurement across all individuals.
- **Smin <- min(sd_pred[])**
- **Smax <- max(sd_pred[])**
    
    
```{r adding sd and min/max, include=TRUE}
set.seed(123) # Setting Seed for reproducibility

jags.model2 <- function(){
 for (i in 1:N_ns){
        for (j in 1:T) {
            y.ns[i,j] ~ dnorm(alpha.ns[i], p.y)
        }
        alpha.ns[i] ~ dnorm(mu, p.alpha)
    }
     for (i in 1:N_s){
        for (j in 1:T) {
            z[i,j] ~ dbern(lambda)
            y.s[i,j] ~ dnorm(alpha.s[i] + tau * z[i,j], p.y) 
        }
        alpha.s[i] ~ dnorm(mu + beta, p.alpha)
    }
  
     # Prediction for additional response times for schizophrenic 
      #individuals i = 12 to 17
      for (i in 1:N_s) {
          for (j in 1:T) {
          # Predicted response times
            y.pred[i,j] ~ dnorm(alpha.s[i] + tau * z.pred[i,j], p.y) 
            z.pred[i,j] ~ dbern(lambda)
          }
                sd.pred[i] <- sd(y.pred[i, ])
    }
    # Compute min and max of standard deviations
    Smin <- min(sd.pred[])
    Smax <- max(sd.pred[])
    
    # Priors
    mu ~ dunif(-10, 10) 
    sigma_y2 ~ dunif(0.001, 5) 
    sigma_alpha2 ~ dunif(0.001, 5) 
    beta ~ dunif(0, 10) 
    tau ~ dunif(0, 10) 
    lambda ~ dunif(0, 1) 
  
    # Precision parameters
    p.y <- 1 / sigma_y2   
    p.alpha <- 1 / sigma_alpha2
}
```


### c. 

Adding new standard deviation parameters to track.

```{r inits2, include=TRUE}
jags.param2 <- c("mu", "beta", "tau", "lambda", "p.y", "p.alpha", "Smin", "Smax")

set.seed(123)

inits1 <- list(
    mu = 4, beta = 1, tau = 1, lambda = 0.5, 
    sigma_y2 = 1, sigma_alpha2 = 1, 
    z = matrix(0, nrow = 6, ncol = 30),
    z.pred = matrix(0, nrow = 6, ncol = 30)
)  
inits2 <- list(
    mu = 6, beta = 4, tau = 5, lambda = 0.3, 
    sigma_y2 = 2, sigma_alpha2 = 2, 
    z = matrix(0, nrow = 6, ncol = 30),
    z.pred = matrix(0, nrow = 6, ncol = 30)
)  
jags.inits <- list(inits1, inits2)
```

Now we run the model, with two chains, 6000 iterations, but discarding 5000 as burnin.

```{r fitting jags2, include=TRUE, results='hide'}
 jags.mod.fit2 <- jags(data = data_list, inits = jags.inits,
  parameters.to.save = jags.param2, n.chains = 2, n.iter = 6000,
  n.burnin = 5000, n.thin = 1 , model.file = jags.model2, DIC = FALSE)
```
```{r}
# Display results
print(jags.mod.fit2)
```

## d. 

Extract the minimum and maximum standard deviation values from the fitted model, and produce
 a scatterplot of the (Smin,Smax) pairs. (Note that each iteration of the model fit will produce
 a minimum-maximum pair). Find the minimum and maximum of the raw standard deviation
 estimates obtained in Question 2, and add an additional point to your scatterplot showing this
 raw minimum-maximum pair.
 
Now we extract the minimum and maximum standard deviation values from the fitted model.

```{r, fig.width=7, fig.height=5, fig.align="center"}
# Extract Smin and Smax from MCMC samples
smin_samples <- jags.mod.fit2$BUGSoutput$sims.list$Smin
smax_samples <- jags.mod.fit2$BUGSoutput$sims.list$Smax

scatter_data <- data.frame(Smin = smin_samples, Smax = smax_samples)

# Calculate raw minimum and maximum standard deviation values from Question 2
raw_min_sd <- min(sd_log$SD_Log_RT[12:17])  # Minimum SD for schizophrenic group
raw_max_sd <- max(sd_log$SD_Log_RT[12:17])  # Maximum SD for schizophrenic group

# Create scatter plot
ggplot(scatter_data, aes(x = Smin, y = Smax)) +
  geom_point(alpha = 0.5, color = "blue") +  # MCMC Samples
  geom_point(aes(x = raw_min_sd, y = raw_max_sd), color = "red", size = 3) + 
  labs(
    title = "Scatterplot of (Smin, Smax) Pairs",
    x = "Minimum Predicted Standard Deviation (Smin)",
    y = "Maximum Predicted Standard Deviation (Smax)"
  ) +
  custom_theme
```

The scatterplot in Figure X displays the relationship between the minimum (Smin) and maximum (Smax) standard deviations of the predicted response times for schizophrenic individuals, obtained from the posterior samples of the JAGS model. Each blue point represents an (Smin, Smax) pair from a single iteration of the Markov Chain Monte Carlo (MCMC) sampling process. The red point corresponds to the empirical minimum and maximum standard deviations computed from the original dataset.

Interpretation of the Scatterplot
The majority of the posterior samples (blue points) form a dense cluster, indicating the range of predicted variability for within-person reaction times. However, the red point lies outside this main distribution, suggesting that the observed reaction time variability is greater than what the model predicts. This discrepancy indicates that the model may underestimate the true within-person variability of schizophrenic individuals.

Possible reasons for this underestimation include:

Restrictive prior assumptions on the variance parameter 
𝜎
𝑦
σ 
y
​
 , which may be limiting the spread of predicted reaction times.
The mixture model structure may not fully capture the extent of individual differences in reaction time delays.
Potential bias in the assumed normal distributions, which may not sufficiently accommodate extreme reaction times observed in schizophrenic individuals.
Conclusion: Can the Model Explain the Variability?
The positioning of the red point outside the cloud of posterior samples suggests that the model does not fully explain the observed variability in schizophrenic reaction times. While the model captures general trends, it likely underestimates reaction time dispersion. Refinements to the prior distributions, particularly on 
𝜎
𝑦
σ 
y
​
 , or adjustments to the model structure (such as incorporating a more flexible variance structure) may be necessary to improve model fit.


## 8.  **double check**



# B. Classification


The following figure shows the information in the dataset Classification.csv- it shows two different groups, plotted against two explanatory variables. This is simulated data - the groupings are determined by a known function of X1 and X2 with added noise/random error.

```{r  fig.width=7, fig.height=5, fig.align="center"}
ggplot(Classification, aes(x = X1, y = X2, color = as.factor(Group))) + 
  geom_point(alpha = 0.8) + 
  labs(
    title = "Scatterplot of Classification Data",
    x = "X1",
    y = "X2"
  ) +
  custom_theme
```


## 1. Create meaningful summaries

First, we use the summary function to get a brief overview of the dataset. We can see that the X1 and X2 are the explanatory variables. To understand how the groups are classified, we will create a more detailed numerical summary.
```{r results='hide'}
summary(Classification)
```

```{r include=FALSE}
num_summary <- Classification %>%
  group_by(Group) %>%
  summarise(
    Mean_X1 = mean(X1),
    SD_X1 = sd(X1),
    Median_X1 = median(X1),
    Min_X1 = min(X1),
    Max_X1 = max(X1),
    Mean_X2 = mean(X2),
    SD_X2 = sd(X2),
    Median_X2 = median(X2),
    Min_X2 = min(X2),
    Max_X2 = max(X2)
  )
```
```{r include=FALSE}
# Load necessary libraries
library(gt)
library(dplyr)

# Create separate summaries for X1 and X2
classification_summary_X1 <- Classification %>%
  group_by(Group) %>%
  summarise(
    `Mean X1` = mean(X1),
    `SD X1` = sd(X1),
    `Median X1` = median(X1),
    `Min X1` = min(X1),
    `Max X1` = max(X1)
  ) %>%
  mutate(Group = ifelse(Group == 0, "Group 0", "Group 1")) %>%
  relocate(Group)

classification_summary_X2 <- Classification %>%
  group_by(Group) %>%
  summarise(
    `Mean X2` = mean(X2),
    `SD X2` = sd(X2),
    `Median X2` = median(X2),
    `Min X2` = min(X2),
    `Max X2` = max(X2)
  ) %>%
  mutate(Group = ifelse(Group == 0, "Group 0", "Group 1")) %>%
  relocate(Group)
```
```{r echo=FALSE}
# GT table for X1 only
gt_table_X1 <- classification_summary_X1 %>%
  gt() %>%
  tab_header(
    title = md("**Summary Statistics for Group 0 and Group 1**"),
    subtitle = md("*Comparison of X1 Across Groups*")
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = px(10),
    table.width = pct(90),
    heading.align = "center",
    column_labels.font.weight = "bold",
    column_labels.border.bottom.width = px(1),
    column_labels.border.bottom.color = "black",
    table.border.top.width = px(1),
    table.border.top.color = "black",
    table.border.bottom.width = px(1),
    table.border.bottom.color = "black",
    data_row.padding = px(4)
  ) %>%
  cols_align(
    align = "center", columns = where(is.numeric)
  ) %>%
  opt_table_lines() %>%
  opt_row_striping()

# GT table for X2 only
gt_table_X2 <- classification_summary_X2 %>%
  gt() %>%
  tab_header(
    title = md("**Summary Statistics for Group 0 and Group 1**"),
    subtitle = md("*Comparison of X2 Across Groups*")
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 2
  ) %>%
  tab_options(
    table.font.size = px(10),
    table.width = pct(90),
    heading.align = "center",
    column_labels.font.weight = "bold",
    column_labels.border.bottom.width = px(1),
    column_labels.border.bottom.color = "black",
    table.border.top.width = px(1),
    table.border.top.color = "black",
    table.border.bottom.width = px(1),
    table.border.bottom.color = "black",
    data_row.padding = px(4)
  ) %>%
  cols_align(
    align = "center", columns = where(is.numeric)
  ) %>%
  opt_table_lines() %>%
  opt_row_striping()
```

```{r  fig.width=7, fig.height=5, fig.align="center"}
gt_table_X1

```

```{r fig.width=7, fig.height=5, fig.align="center"}
gt_table_X2

```
### **Numerical Summaries and Interpretation**

#### Variable X1:
- **Mean and Variability:** Group 1 shows higher variability in X1 (SD = 1.12) compared to Group 0 (SD = 0.59), indicating greater dispersion around its mean.
- **Median and Range**: The median of Group 1 (-0.30) is lower than that of Group 0 (0.06), suggesting a shift towards lower values for Group 1. Group 1 also exhibits more extreme values with a broader range (Min = -3.06, Max = 3.29) compared to Group 0 (Min = -1.09, Max = 1.64).

### Summaries for Variable X2
X2 shows greater similarity between the two groups compared to X1:
- **Central Tendency**: The median X2 value of Group 1 (0.31) is higher than Group 0 (-0.05), indicating that Group 1 typically has higher X2 values.
- **Spread and Range:** Both groups have similar variability, indicated by their standard deviations (0.93 for Group 1 vs. 0.89 for Group 0). However, Group 1 again has a wider range (-3.41 to 3.94) compared to Group 0 (-3.39 to 1.84).

### Implications for Classification
The observed numerical summaries and scatter plot highlight several key points:

- The differences in distributions and notable overlap suggest that the classes are not linearly separable.
- The variability, particularly in X1, suggests complex interactions or non-linear boundaries between groups.

### Recommended Classification Methods
Given the distribution and structure of the data, suitable classification methods include:

1. **K-Nearest Neighbors (KNN):** 
   - Flexible in handling non-linear boundaries.
   - The optimal choice of neighbors (K) can be tuned to balance flexibility and overfitting risks.

2. **Quadratic Discriminant Analysis (QDA):**
   - Effective at modeling non-linear class boundaries if the assumption of normality holds.

3. **Random Forest (RF):** 
   - Capable of capturing complex interactions and nonlinear patterns in the data without strong assumptions.

4. **Support Vector Machines (SVM):**
   - Suitable for data where clear linear boundaries do not exist, leveraging kernels to create non-linear separations.

Each of these methods accommodates the observed complexity and variability, offering a balanced approach between flexibility and generalizability.




## 2. Select75%ofthedatatoactasatrainingset,withtheremaining25%fortesting/evaluation.

```{r}
library(dplyr)
 library(caret)
 library(e1071)
```
```{r}
# Define function that will split the data into training and test sets
 trainTestSplit <- function(df, trainPercent,seed1){
# Sample size percent
 smp_size <- floor(trainPercent/100 * nrow(df))
# set the seed
 set.seed(seed1)
 train_ind <- sample(seq_len(nrow(df)), size = smp_size)
 train_ind
 }

# Split as training and test sets
 train_ind <- trainTestSplit (Classification, trainPercent = 75, seed = 123)
 train <- Classification[train_ind ,]
 test <- Classification[-train_ind ,]
 
# Convert 'Group' to a factor in both train and test sets, as this represents the class.
 train$Group <- factor(train$Group)
 test$Group <- factor(test$Group)

 train_X <- train[, c("X1", "X2")]
 test_X <- test[, c("X1", "X2")]
 
 train_y <- train$Group
 
 
 
# Checking dimensions:
dim(train) 
dim(test)
```

The function **trainTestSplit** is employed to divide the dataset into training and test subsets to facilitate model evaluation in Part B. It randomly partitions the dataset based on a specified training percentage (**trainPercent**), with a fixed random seed (**seed1**) to ensure reproducibility. The training subset is used for model fitting, while the test subset provides an unbiased measure of predictive performance. This approach ensures a rigorous assessment of model generalization to new data.


## 3. 

theory behind how the classification method classifies the data:

present the results of an evaluation of the method:
Describe findings:

Detailed description of model performance:

optimize the (hyper) parameters of the method.

## **K-Nearest Neighbors (KNN):**

```{r}
library(class) # This is the package which has the KNN function
```

The KNN classifier is a supervised, non- parametric classification algorithm. It classifies a new observation by examining the classes of its **k** nearest neighbours, and assigning  the class by through a "majority vote" (most common class of neighbours). 

The value of k controls the **flexibility** of the model:

- Smaller k leads to a flexible decision boundary (low bias but high variance, prone to overfitting).

- Larger k smooths the decision boundary (high bias but lower variance, risk underfitting).

The goal is to select an **optimal k** that balances bias and variance, typically done via cross-validation.

### **Fitting the model**
```{r fitting knn1}
knn_fit1 = knn(train=train_X, test=test_X, cl=train$Group, k=7)

# Confusion Matrix
confusionMatrix(knn_fit1, test$Group)
```
Initially, the model was fitted using an arbitrary choice of **k = 7**, resulting in an accuracy of 79.6%. Although this initial accuracy was good, further hyperparameter tuning was necessary to achieve the best predictive performance.

### **Hyperparameters Optimisation**

Tuning hyperparameters essentially means improve a machine learning algorithm's structure, so it can be flexible enough to fit the data but constrained enough to not fit the noise. In **KNN 'K'** is a hyperparameter. 

We use the **K-Fold cross validation**. This divides the dataset into K Folds, then the ML model is trained on K-1 folds and tested on Kth fold. This method estimates the average prediction error from the K runs and optiaml values for hyper parameters to build the final model.

We use the train() function here which does the fitting and tuning simultaenously.

```{r warning=FALSE}
set.seed(123)
# Set training options
# Repeat 5-fold cross-validation, ten times
opts <- trainControl(method='repeatedcv', number=10, repeats=5)
# Find optimal k (model)
mdl <- train(x=train_X, y=train$Group, # training data
method='knn', # machine learning model
trControl=opts, # training options
tuneGrid=data.frame(k=seq(4, 20))) # range of k's to try
print(mdl) # print the outcome
```
The cross-validation results indicated an optimal value at k = 9, which achieved the highest average accuracy (82.0%).

### **Optimised KNN Model**
```{r}
knn_fit2 = knn(train=train_X, test=test_X, cl=train$Group, k=9)
# Confusion Matrix
confusionMatrix(knn_fit2, test$Group)
```

### Evaluation and Model Comparison: K-Nearest Neighbour (KNN)

The optimized KNN classifier performed well overall, producing an accuracy of 78.8%, demonstrating strong specificity (88.1%) and acceptable balanced accuracy (75.2%). However, sensitivity (62.2%) was relatively lower, indicating some limitations in correctly classifying positive cases. The moderate Cohen’s kappa (0.52) also suggests moderate agreement beyond random chance, affirming good but not excellent classifier stability.

### Comparison of Models

Interestingly, the single train-test evaluation initially favored k = 7 (accuracy 79.6%), while cross-validation identified k = 9 as optimal. However, due to the greater reliability and robustness of cross-validation, which provides more dependable estimates of a model’s generalization capability, **k = 9**remains the preferred choice, as cross-validation offers a more comprehensive indication of how well the model will perform with unseen data.


## **Quadratic Discriminant Analysis (QDA):**

Quadratic Discriminant Analysis (QDA) is a classification method that extends Linear Discriminant Analysis (LDA) by allowing each class to have its own covariance matrix. Like LDA, QDA assumes observations within each class are generated from a multivariate Gaussian distribution; however, unlike LDA, QDA does not assume these covariance matrices to be equal across classes. Due to this flexibility, QDA produces quadratic decision boundaries, enabling it to capture more complex, non-linear separations between classes effectively. Consequently, QDA is particularly suitable when the classes exhibit differing covariance structures, as it can adapt to variability within individual classes to achieve improved classification accuracy.

### **Fitting the model**

```{r}
library(MASS)
set.seed(123)
# Fitting QDA model
qda_fit1 <- qda(Group ~ ., data=train) # Used '.' syntax to call all other variables in the data set.

# Predicting classes using fitted model
qda_predict <- predict(qda_fit1,test)
confusionMatrix(qda_predict$class, test$Group)
```



### **Interpretation of Results**

The QDA model demonstrated strong specificity (93.1%), indicating excellent performance in correctly classifying negative cases. However, the sensitivity was notably lower (52.2%), suggesting a limitation in correctly identifying positive cases. Balanced accuracy (72.7%) indicates that the model provides good predictive capability overall, though with an imbalance in classification effectiveness between positive and negative instances. A moderate Cohen’s Kappa value (0.49) reflects acceptable performance beyond chance, but room for improvement remains, particularly in sensitivity.

###  **Evaluation of Model**

Unlike KNN, QDA does not have hyperparameters (such as k) that require optimization. The method directly estimates distinct covariance matrices for each class based on training data, leaving no flexibility for tuning beyond potential transformations or feature selection strategies.

In summary, the QDA classifier provided a robust classification performance, excelling in specificity but demonstrating limitations in sensitivity. Given its inherent lack of hyperparameters, further performance improvements would likely involve feature engineering or combining QDA with complementary classification techniques.



## **Random Forest (RF):** 

Random Forests (RF) is an ensemble learning method that enhances classification accuracy by constructing multiple decision trees and aggregating their predictions. It extends **bagging** by de-correlating the trees, reducing model variance and improving generalisation.

**Bagging**: RF builds multiple trees using bootstrap samples (sampling with replacement) from the original dataset. Each tree is grown deep and unpruned, reducing variance through averaging. Predictions are aggregated using majority voting for classification.

**Random Feature Selection**: To reduce correlation between trees, each split considers only a random subset of predictors (typically \(\sqrt{p}\)). This de-correlates trees, enhancing model robustness and lowering variance.

**Out-of-Bag (OOB) Error**: On average, one-third of observations are left out of each bootstrap sample. These "out-of-bag" observations provide a natural method for estimating model error without a separate validation set.


### **Fitting the model**

```{r warning=FALSE}
library(randomForest)

# Fitting random forest model
set.seed(123)

mdl <- train(x=train_X, y=train$Group,
             method='rf',
             ntree=200,
             tuneGrid=data.frame(mtry=2))
print(mdl)
```

We can try to optimise hyperparameters. By adjusting the number of trees from 200 to 500, resampling accuracy fell 0.002. Adjusting the mtry parameter (number of predictors randomly samples at each split) showed no effect on the accuracy, so it will remain at the value 2.  


```{r}
set.seed(123)
# Test model on testing data
TestPred <- predict(mdl, newdata=test_X)
confusionMatrix(TestPred, test$Group) # predicted/true
```
```{r}
# Variable importance by mean decrease in gini index
varImp(mdl$finalModel)
```

### **Variable Importance**
  Variable importance, assessed by the mean decrease in Gini index, indicated that `X1` (175.67) contributed more significantly to the model’s classification decisions compared to `X2` (141.44). This suggests that `X1` is the most influential predictor in distinguishing between the target classes.


### **Interpretation of Results**

The optimized Random Forest model achieved an overall accuracy of 75.2%, with high specificity (83.1%) indicating strong performance in correctly identifying negative cases. However, sensitivity was comparatively lower (61.1%), highlighting some limitations in classifying positive cases. The balanced accuracy of 72.1% reflects moderate but balanced classification performance across both classes. The Cohen's Kappa statistic (0.45) suggests moderate agreement beyond chance.

### **Conclusion**

Random Forests provided a powerful and stable classification approach by reducing variance through bagging and random feature selection. However, its accuracy is around 3% lower than KNN and QDA.






## **Support Vector Machines (SVM):**

Support Vector Machines (SVM) are supervised classification algorithms designed to find the optimal hyperplane that best separates data points belonging to different classes. SVM seeks the hyperplane that maximizes the margin between groups 0 and 1 while minimizing classification errors.

When even more complex data structures arise, where linear separation is infeasible, SVM generalizes through the use of kernel functions. Kernels implicitly map the original feature space to a higher-dimensional space where a linear separator can be found. Although it might seem counter-intuitive to increase dimensionality, the kernel trick ensures this is done implicitly, allowing SVM to handle non-linear relationships efficiently. Kernels can be considered as generalized distance measures, and common types include the linear, polynomial, and radial basis function (RBF) kernels. Kernel choice is a hyperparameter that can be optimized via cross-validation. However, in caret, each kernel is treated as a separate model, requiring manual cross-validation when comparing kernels.

Since we already know the data displays non-linear relationships, we wont fit the linear kernal as we alrady know it wont classify the data well., but will fit the Polynomial and Radial Basis Function (RFB) kernal.


### **Fitting the model**

1. **RFB Kernal**

First we will fit the RFB Kernal. This is a strong option does to its versatility and flexibility.
We will also perform hyperparamater stuning in this, using 5-fold cross-validation method.

```{r warning=FALSE}
library(caret)
library(kernlab)
set.seed(123)

# Initial Model (without manual hyperparameter tuning)
rbf_mdl <- train(x = train_X, y = train$Group, 
                 method = 'svmRadial',
                 trControl = trainControl(method = "cv", number = 5))

# View the best tuning parameters
print(rbf_mdl$bestTune)

# Plot model accuracy vs different values of Cost and Sigma
plot(rbf_mdl)

```

Implementing the optimal hyperparameter values:
```{r warning=FALSE}
# Manually tuning the hyperparameters C and sigma
rbf_mdl2 <- train(x = train_X, y = train$Group,
                  method = "svmRadial",
                  trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = expand.grid(C = seq(0, 1, length = 20),
                                         sigma = seq(0.01, 2, length = 10)))

# View the best tuning parameters
print(rbf_mdl2$bestTune)

# Plot model accuracy vs different values of Cost and Sigma
plot(rbf_mdl2)

# Evaluate the optimized model on the test data
yTestPred_rbf <- predict(rbf_mdl2, newdata = test_X)
confusionMatrix(yTestPred_rbf, test$Group)

```


2. **Polynomial Kernal**

We will also fit an this model, and fit its optimised version to see if this can create a better fitted decsion boundary.

```{r warning=FALSE}
set.seed(123)

poly_mdl <- train(x = train_X, y = train$Group,
                  method = "svmPoly")

# View the best tuning parameters
print(poly_mdl$bestTune)

# Plot the accuracy vs hyperparameters to visualize performance
plot(poly_mdl)

# Evaluate the optimized model on the test data
yTestPred_poly <- predict(poly_mdl, newdata = test_X)
confusionMatrix(yTestPred_poly, test$Group)
```


























