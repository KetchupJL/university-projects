---
title: "MTHM506-Statistical Data Modelling - Individual Project"
author: "James R Lewis"
date: "00-02-2025"
output:
  html_document:
    css: style.css
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r nlmodel, include=FALSE}
load("~/GitHub/university-projects/Statistical Data Modelling Projects - MTHM506/Coursework 1 - Practical Modelling Excersises and Theoretical Problems/datasets.RData")
library(tidyverse)
library(ggplot2)
library(rmarkdown)
```

# Question 1 {.unnumbered}

 The data frame nlmodel contains data on a response variable y and a single explanatory variable x. A scatter plot of y versus x suggests a strong non-linear relationship:
``` {r setting plot theme, include = FALSE}
custom_theme <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),  # Centered title
    axis.title = element_text(face = "bold", size = 12),
    axis.text = element_text(size = 11), 
    panel.grid.major = element_line(color = "gray85"),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 0.8)
  )
```
 
```{r initial_plot, fig.width=6, fig.height=4, fig.align="center", echo=FALSE}
ggplot(nlmodel, aes(x = x, y = y)) +
       geom_point() +
       theme_minimal(base_size = 12) +
  custom_theme
```
 
Suppose for this data we wish to consider this model:


<div style="text-align: center;">
$$
Y_i \sim N\left( \frac{\theta_1 x_i}{\theta_2 + x_i} , \sigma^2 \right)
$$
 </div>
 <div style="text-align: center;">
  i = 1, 2,..., 100, Y<sub>i</sub> independent
</div>


a. **Why can‚Äôt this model be fit using a linear (regression)  model?** 
 
From looking at the model and the plot above we tell that x and y have a non-linear relationship.
The mean of this model exhibits a non-linear relationship involving the unknown parameters Œ∏<sub>1</sub> and Œ∏<sub>2</sub>. Specifically, the presence of Œ∏<sub>2</sub> + x<sub>i</sub>  in the denominator introduces a dependency between x<sub>i</sub> and the parameters that cannot be expressed as a linear combination. Consequently, this violates the linearity assumption required for a standard linear regression model, as the relationship between x<sub>i</sub> and y<sub>i</sub> cannot be transformed into a linear form, as indicated by the scatter plot.



b. **Write down the likelihood L(Œ∏<sub>1</sub>,Œ∏<sub>2</sub>,œÉ<sub>2</sub>;y,x) and the log-likelihood ‚Ñì(Œ∏<sub>1</sub>, Œ∏<sub>2</sub>, œÉ<sub>2</sub>;y,x)** 
 
 The likelihood function L(Œ∏<sub>1</sub>,Œ∏<sub>2</sub>,œÉ<sub>2</sub>;y,x) is the product of the probability density functions of the normal distribution for each observation y<sub>i</sub>:
 This is central to estimating parameters which best explain the observed data.
 
 **Likelihood Function**:
 \[
L(\theta_1, \theta_2, \sigma^2; y, x) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)
\]
 
 The log-likelihood ‚Ñì(Œ∏<sub>1</sub>, Œ∏<sub>2</sub>, œÉ<sub>2</sub>;y,x)
Since the likelihood function involves a product over all observations,  working with the log-likelihood function, which converts the product into a sum, makes computations more manageable. By maximizing the log-likelihood, we obtain the maximum likelihood estimates, which ensure the best possible fit to the data.
 
 **Log-Likelihood Function**:
\[
\ell(\theta_1, \theta_2, \sigma^2; y, x) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} \left(y_i - \frac{\theta_1 x_i}{\theta_2 + x_i}\right)^2
\]


c. **Write an R function mylike() which evaluates the negative log-likelihood (i.e. ‚àí‚Ñì(Œ∏<sub>1</sub>,Œ∏<sub>2</sub>,œÉ ;y,x)) for any values of the three parameters**

Writing the Log-Likelihood Function in R:

``` {r negative log-likelihood function, include=TRUE}
mylike <- function(params, y, x) {
  # Defining the Parameters
  theta1 <- params[1]
  theta2 <- params[2]
  sigma <- params[3]
  
  # Defining the Mean
  mu <- (theta1 * x) / (theta2 +x)
  
  # Defining n
  n <- length(y)
  
  result <- -n/2 * log(2 * pi) - n/2 * log(sigma^2) -
    sum((y - mu)^2) / (2 * sigma^2)
  
  # Here I multiply by a minus, so I work with the negative log-likelihood
  return(-result)
}
```

d.  **Use the R function nlm() in association with your function mylike() to numerically minimise the log-likelihood. Provide some evidence of how you chose sensible starting values. Report the maximum likelihood estimates of the parameters and super impose a plot of the associated mean relationship on a scatter plot of y versus x.**

Using the mylike() function above, I can now use the non-linear minimisation nlm() function to estimate the parameters. However, nlm() will minimise by default, so I computed the **negative log-likelihood** above to maximise the log-likelihood.

Setting up the nlm function:
``` {r Minimising Log-likelihood, include=TRUE}
x <- nlmodel$x
y <- nlmodel$y

```


Good starting values are needed to ensure the model optimisation and efficient convergence, as poor values lead to the slow convergence or local minima.

``` {r Log-likelihood estimates, include=TRUE}
# max(y) = 225.0743
theta1_init <- max(y)
# median(x) = 0.5302
theta2_init <- median(x)
# sd(y) = 39.45
sigma_init <- sd(y)
# Combine into a vector
inits <- c(theta1_init, theta2_init, sigma_init)

result <- nlm(mylike, p = inits, hessian = T, x = x, y = y, iterlim = 10000, steptol = 1e-10)
```


```{r Formatting estimates, include=FALSE}
# Reporting estimates
mle_params <- result$estimate
names(mle_params) <- c("theta1", "theta2", "sigma")
```
```{r Printing estimates, echo=FALSE}
print(mle_params)
```
Starting values:

- **Theta 1** A reasonable initial estimate for Œ∏<sub>1</sub> is the maximum observed ùë¶ value, because as seen in the models mean function, as ùë•increases, the function asymptotically approaches Œ∏<sub>1</sub>. Indicating  that Œ∏<sub>1</sub> represents the upper limit of ùë¶ for large ùë•.

- **Theta 2** appears in the denominator and affects how quickly the function increases as ùë• increases. When ùë•= Œ∏<sub>2</sub>, the function reaches half of Œ∏<sub>1</sub>. Thus a reasonable estimate for Œ∏<sub>2</sub> is the median of ùë•. This way, it represents the center of the distribution whilst, not taking into account extreme values (unlike the mean).

- **Sigma**  represents the variance of residuals around the mean function. This assumes that the spread of ùë¶ around the mean is roughly similar to its overall variance. Thus a logical first approximation is simply the variance of ùë¶.

``` {r Setting up plot, include=TRUE}
# Creating scatter plot

# First extracting MLEs
theta1_mle <- mle_params[1]
theta2_mle <- mle_params[2]

# Calculate the fitted mean
fitted_mu <- (theta1_mle * x)/(theta2_mle + x)

plot_data <- data.frame(x = x, y = y, fitted_mu = fitted_mu)
```

The figure below, shows the fitted mean obtained from estimated parameters. By plotting this against the observed data, we can see whether it is a good fit.

``` {r Scatter plot, fig.align="center", echo = FALSE}
ggplot(plot_data, aes(x = x, y = y)) +
  geom_point(size = 2, colour = "black") +
  geom_line(aes(y = fitted_mu), colour = "red", linewidth = 1.1) +
  labs(x = "X", y = "Y", title = "Figure 1: Scatter Plot with Fitted Mean Relationship") +
  theme_minimal(base_size = 12) +
 custom_theme
```

The fitted mean relationship shows a diminishing returns effect, where \( Y \) increases rapidly at lower \( X \) values before plateauing. The mean curve effectively captures the trend, indicating a good fit for modeling the data. However, variability in the data points increases at higher \( X \) values, suggesting potential heteroscedasticity.

e. **Report the standard errors for Œ∏<sub>1</sub> and Œ∏<sub>2</sub>, and use those to construct 95% confidence intervals.**

Having obtained the maximum likelihood estimates (MLEs) for Œ∏<sub>1</sub> and Œ∏<sub>2</sub>, the next step is to assess the uncertainty associated with these estimates. This is achieved by computing standard errors (SEs) and using them to construct 95% confidence intervals (CIs).

The standard errors are derived from the observed information matrix (OIM), which is the inverse of the Hessian matrix
The hessian matrix provides information on how sensitive the likelihood function is to changes in parameter values.

``` {r Hessian Matrix, include=TRUE}
result$hessian

OIM <- solve(result$hessian) # Observed information matrix.

# Extract standard errors by squareroot of diagonal elements
std_errors <- sqrt(diag(OIM))
names(std_errors) <- c("theta1", "theta2", "sigma")
```
Standard Errors for Œ∏<sub>1</sub> and Œ∏<sub>2</sub>
``` {r Standard Errors, echo=FALSE}
print(std_errors)
```

The standard errors quantify the variability in our estimates ‚Äî  a smaller standard error relative to the size of the parameter indicates higher precision, where as a larger SE indicated greater uncertainty in the estimates. 

The estimates all have relatively low standard errors, so we can have a higher level of confidence in the models accuracy.

```{r Constructing CIs, include=TRUE}
# Constructing 95% CIs

# Extract standard errors for theta1 and theta2
theta1_se <- std_errors["theta1"]
theta2_se <- std_errors["theta2"]

# Compute 95% CIs
theta1_ci <- c(theta1_mle - 1.96 * theta1_se, theta1_mle + 1.96 * theta1_se)
theta2_ci <- c(theta2_mle - 1.96 * theta2_se, theta2_mle + 1.96 * theta2_se)
```

95% Confidence Intervals for Œ∏<sub>1</sub> and Œ∏<sub>2</sub>

``` {r CIs, echo=FALSE}
# Printing CIs
cat("95% CI for theta1: [", theta1_ci[1], ",", theta1_ci[2], "]\n")
cat("95% CI for theta2: [", theta2_ci[1], ",", theta2_ci[2], "]\n")
```
We can have confidence in this model the parameters sit within a well defined and narrow range, adding reliability to the model.

f. **Test the hypothesis that Œ∏<sub>2</sub> = 0.08 at the 5% significance level (not using the confidence interval) and compute the associated p-value of the test.**

We test the hypothesis that \(\theta_2 = 0.08\) at the 5% significance level. This allows us to assess whether Œ∏<sub>2</sub> is significantly different from a hypothesised value, providing insight into the reliability of our model estimates.

The null and alternative hypotheses are defined as follows:

- **Null Hypothesis (\(H_0\)):** \(\theta_2 = 0.08\)
- **Alternative Hypothesis (\(H_1\)):** \(\theta_2 \neq 0.08\)


Z-statistic Test: measures how many standard deviations our estimate is from the hypothesized value

The Z-statistic is computed as:

\[
Z = \frac{\hat{\theta_2} - 0.08}{\text{SE}(\hat{\theta_2})}
\]

Where:

- \(\hat{\theta_2}\) is the MLE for \(\theta_2\),
- \(\text{SE}(\hat{\theta_2})\) is the standard error of \(\hat{\theta_2}\).

A two-tailed test is appropriate here because we are testing whether Œ∏<sub>2</sub> is either greater than or less than 0.08, not just in one direction.

The p-value for the two-tailed test is given by:

\[
\text{p-value} = 2 \cdot (1 - \Phi(|Z|))
\]

Where:

- \(\Phi\) is the cumulative distribution function (CDF) of the standard normal distribution

```{r Z-Stat, include=TRUE}

# Compute the z-statistic
z_stat <- (theta2_mle - 0.08) / theta2_se

# Print the z-statistic
z_stat
```
A Z-Statistic test value of -3.20 suggests that Œ∏<sub>2</sub> is far from 0.08, providing evidence against the null hypothesis.

Computing the P-Value

``` {r Pvalue, include=TRUE}
# z-stat is negative so use pnorm() which computes P(Z < z)
# multiply by 2 to account for deviations in both directions
p_value <- 2*(pnorm(z_stat, 0, 1))
#Print p-value
p_value
```
The P-Value is smaller than 0.05, so we reject the null hypothesis at the 5% significance level. This indicates that Œ∏<sub>2</sub> is significantly different from 0.08, moreover, 0.08 doesn't sit within the 95% confidence intervals computed earlier, which further supports the rejection of the null hypothesis.

g. **Use plug-in prediction to construct and plot 95% prediction intervals.**

In this section, we construct 95% prediction intervals using the plug-in method and visualise them alongside the observed data and fitted values.

Prediction intervals account for both the uncertainty in the mean estimate and the inherent variability in new observations. This makes them wider and more useful when predicting new responses.

``` {r P value, include=TRUE}
# Compute 95% prediction intervals
lower_bound <- fitted_mu - 1.96 * mle_params["sigma"]
upper_bound <- fitted_mu + 1.96 * mle_params["sigma"]
```

Plotting the Observed Data, Fitted Mean, and Prediction Intervals

```{R prediction plot, fig.align="center", echo=FALSE}
# Create a data frame for plotting
plot_data <- data.frame(x = x, y = y, fitted_mu = fitted_mu, 
                        lower = lower_bound, upper = upper_bound)

# Plot the observed data, fitted mean, and prediction intervals
ggplot(plot_data, aes(x = x, y = y)) +
  geom_point(size = 2, colour = "black") +  # Observed data
  geom_line(aes(y = fitted_mu), colour = "red", linewidth = 1.1) +  # Fitted mean
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = "blue") +  # Prediction interval
  labs(x = "X", y = "Y", title = "Figure 2: Scatter Plot with 95% Prediction Intervals") +
  theme_minimal(base_size = 12) +
  custom_theme
```

The prediction intervals appropriately capture most data points, with some deviations in the middle-to-upper range of ùë•, indicating possible variance underestimation.

While the model follows the data trend well, slight asymmetry in residual spread suggests potential heteroscedasticity.



## Question 2 {.unnumbered}


The data frame aids data relates to the number of quarterly AIDS cases in the UK, y<sub>i</sub>, from January  1983 to March 1994']. The variable cases is y<sub>i</sub> and date is time, symbolised here as x<sub>i</sub>. In this question we consider two competing models to describe the trend in the number of cases. 

Model 1 (Poisson) is

\[
Y_i \sim \text{Pois}(\lambda_i)
\]

\[
\log(\lambda_i) = \beta_0 + \beta_1 x_i
\]

And Model 2 (Normal) is:

\[
Y_i \sim N(\mu_i, \sigma^2)
\]

\[
\log(\mu_i) = \gamma_0 + \gamma_1 x_i
\]

a. Plot ùë¶<sub>i</sub> against ùë•<sub>i</sub> and comment on whether the two proposed models are sensible in terms of the distribution and the relationship of x with the mean.

To understand the trend in the number of AIDS cases over time, below is a plot of ùë¶<sub>i</sub>(cases) against ùë•<sub>i</sub> (date). This allows us to assess the distribution of cases and determine whether the two proposed models are reasonable.

*Note: As you can see in the data below, the date variable isn't in a usable format, so it is transformed.*
``` {r Setting up dataset, include=TRUE}
head(aids)

# Transform the time variable, from 1983 to 1994
aids$year <- floor(aids$date) + 1900  # Convert year properly
aids$quarter <- as.numeric(as.character(aids$quarter))
aids$month <- (aids$quarter - 1) * 3 + 1  # Convert quarter to month
aids$date_proper <- as.Date(sprintf("%d-%02d-15", aids$year, aids$month))

# Check the updated data frame
head(aids)
```
``` {r plotting y against x aids, fig.align="center", echo=FALSE}
# Checking data: 
ggplot(aids, aes(x = date_proper, y = cases)) +
  geom_point(alpha = 0.5, size = 1, colour = "black") +
  labs(x = "Date (x)", y = "Cases (y)", title = "Figure 3: Aids cases over time") +
  theme_minimal(base_size = 12) +
 custom_theme
```

The data suggests a non-linear relationship between time and the number of AIDS cases. Both proposed models assume a log-linear relationship between the mean number of cases and time. However, the variance increases over time, as seen in the greater spread of data points after 1988. This suggests potential overdispersion, where variance exceeds the mean.

The Poisson model is generally well-suited for count data, as it assumes a log-link function and non-negative integer values. However, a key assumption of the Poisson distribution is that variance equals the mean. Since we observe heteroscedasticity, the Poisson model may underestimate uncertainty in the data. A possible improvement could involve using a Negative Binomial Model, which introduces an extra dispersion parameter to account for overdispersion.

The Gaussian (Normal) model assumes normally distributed errors, which is problematic for count data since counts are non-negative and often right-skewed. While the log transformation may help stabilize variance, it does not fully address the discrete nature of the data. This makes the Gaussian model less appropriate for modeling count-based data.

b. Fit the two models in R. Plot the estimated trends from each model (ÀÜ Œª<sub>i</sub> and ÀÜ ¬µ<sub>i</sub>) on
 top of the data with approximate 95% confidence intervals around the mean. Comment
 on the validity of each model (based on the plot). Obtain the AIC for each model and thus
 comment on which model is preferable.
 
### **Fitting the Models** 
 
To assess the fit of each model, we construct the Poisson and Normal models using the generalised linear model (GLM) framework in R. Both models assume a log-link function, modeling the number of cases over time:

- Pmodel1: Poisson Model
- Nmodel1: Normal Model

 
``` {r echo=TRUE}
# Model 1:
Pmodel1 <- glm(cases ~ date_proper, data = aids, family = poisson(link = "log"))


# Model 2:
Nmodel1 <- glm(cases ~ date_proper, data = aids, family = gaussian(link = "log"))
```
```{r  models, include=FALSE}

Pmodel1 <- glm(cases ~ date_proper, data = aids, family = poisson(link = "log"))
Nmodel1 <- glm(cases ~ date_proper, data = aids, family = gaussian(link = "log"))
# Summarise the models
summary(Pmodel1)
summary(Nmodel1)
```  

Next, we plot the predicted means against the observed data, including 95% confidence intervals to visualise the range of expected values. Since both models use a log transformation, we apply a back-transformation to obtain meaningful predictions.
 
```{r include=TRUE }
# Create a sequence of dates for prediction
prediction_data <- data.frame(date_proper = seq(min(aids$date_proper), max(aids$date_proper), length.out = 45))

# Model 1 Predictions (Poisson)
pred1 <- predict(Pmodel1, newdata = prediction_data, type = "link", se.fit = TRUE)
prediction_data$fit_model1 <- exp(pred1$fit)  # Back-transform from log
prediction_data$lower_model1 <- exp(pred1$fit - 1.96 * pred1$se.fit)  # 95% CI
prediction_data$upper_model1 <- exp(pred1$fit + 1.96 * pred1$se.fit)

# Model 2 Predictions (Gaussian)
pred2 <- predict(Nmodel1, newdata = prediction_data, type = "link", se.fit = TRUE)
prediction_data$fit_model2 <- exp(pred2$fit)
prediction_data$lower_model2 <- exp(pred2$fit - 1.96 * pred2$se.fit)
prediction_data$upper_model2 <- exp(pred2$fit + 1.96 * pred2$se.fit)

```

```{R AIDS Predictions, fig.align="center", echo=FALSE}
library(ggplot2)
library(patchwork)

# Plot observed cases and model predictions
plot11 <- ggplot() +
  geom_point(data = aids, aes(x = date_proper, y = cases), alpha = 0.3, color = "black") +
  geom_line(data = prediction_data, aes(x = date_proper, y = fit_model1), color = "blue") +
  geom_ribbon(data = prediction_data, aes(x = date_proper, ymin = lower_model1, ymax = upper_model1), 
              fill = "blue", alpha = 0.2) +
  labs(x = "Date", y = "Cases", title = "Poisson") +
  custom_theme


plot22 <- ggplot() +
  geom_point(data = aids, aes(x = date_proper, y = cases), alpha = 0.3, color = "black") +
  geom_line(data = prediction_data, aes(x = date_proper, y = fit_model2), color = "red") +
  geom_ribbon(data = prediction_data, aes(x = date_proper, ymin = lower_model2, ymax = upper_model2), fill = "red", alpha = 0.2) +
  labs(x = "Date", y = "Cases", title = "Normal") +
  custom_theme

  (plot11 + plot22) +
  plot_annotation(
    title = "Figure 4: AIDS Cases with Model Predictions",
    theme = theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14))
  )
``` 
``` {r AIC, echo=FALSE}
 # Calculate AIC for both models
aic_poisson <- AIC(Pmodel1)
aic_normal <- AIC(Nmodel1)

cat("AIC for Poisson Model:", aic_poisson, "\n")
cat("AIC for Normal Model:", aic_normal, "\n")
``` 
### **Model Fit Analysis**

The **Poisson Model** exhibits a poor fit, with observed data points widely scattered around the predicted line, indicating overdispersion (variance exceeding the mean).

The **Normal Model** shows a marginally better fit, with data more symmetrically distributed around the predicted line. This improved alignment is likely due to the log transformation stabilizing variance.


**Model Selection Using AIC** 

To formally compare model suitability, we use the Akaike Information Criterion (AIC):

Poisson Model AIC = 1154.09
Normal Model AIC = 482.82

A lower AIC suggests a better balance between model fit and complexity. The Normal Model has a substantially lower AIC, indicating it is statistically preferred‚Äîprovided its assumptions (e.g., normality of residuals) hold.

However, if the Poisson model's poor fit is due to overdispersion, a Negative Binomial Model could be a more appropriate alternative.
 
 
 c. Produce the deviance residuals vs fitted values (ÀÜ Œª<sub>i</sub> and ÀÜ ¬µ<sub>i</sub>) plot for each model, comment appropriately and thus propose a way that the two models might be extended to improve the fit.
 
Now we conduct model diagnostic tests, to check the deviance residuals against the fitted values for each model. First we extract the residuals and fitted values from the models, using the code below.
 
``` {r Residuals vs fitted, include=TRUE}
# Compute deviance residuals and fitted values for both models
aids$residuals_model1 <- residuals(Pmodel1, type = "deviance")
aids$fitted_model1 <- fitted(Pmodel1)

aids$residuals_model2 <- residuals(Nmodel1, type = "deviance")
aids$fitted_model2 <- fitted(Nmodel1)
```
``` {r loading packages, include=FALSE}
library(ggplot2)
library(patchwork)  # To combine plots
```

```{r redisuals plot, fig.align="center", echo=FALSE}
# Plot for Model 1
plot1 <- ggplot(aids, aes(x = fitted_model1, y = residuals_model1)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (ŒªÃÇ)", y = "Deviance Residuals", title = "Poisson") +
  custom_theme

# Plot for Model 2
plot2 <- ggplot(aids, aes(x = fitted_model2, y = residuals_model2)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (¬µÃÇ)", y = "Deviance Residuals",  title = "Normal") +
  custom_theme

# Combine the plots with a centered title
(plot1 + plot2) + 
  plot_annotation(
    title = "Figure 5: Deviance Residuals vs Fitted Values for Both Models",
    theme = theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)))
```

Figure 5 shows deviance residuals against fitted values for both the Poisson and Normal models.

Boths models shows a lack of fit. 

The Poisson Model shows a curved trend and a right skew, moreover indicates overdispersion given that the residual spread increases with the number of fitted values.

To address the heteroscedasticity, adding **polynomial terms** may help capture the relationship more accurately. 

To address the over dispersion, this model can be adapted into a **Quasi-Poisson Model**. This model adjusts the standard errors for overdispersion.

over dispersion in the Poisson Model, this model should introduce an extra dispersion parameter, turning this model into a **Negative Binomial Distribution**. This model lets the variance change from the mean, making this model more flexible.


The Normal Model's residuals also resemble a normal distribution curve, but the values of the residuals are much larger. As we can see, the log-transformation didn't handle the variance very well.

To improve the Normal Model, adding a **quadratic** or **cubic terms** could better capture the non-linear relationships in the data. This may improve the models fit.



 d. Implement the proposed extensions to each model,to arrive at a final version for each of them (justified by appropriate hypothesis tests).

## Extending Models
 
```{r date to numeric, include=FALSE}
aids$date_numeric <- as.numeric(aids$date_proper - min(aids$date_proper))
```

### **Hypothesis Testing - Model Extensions**

We conduct hypothesis tests to assess whether adding more polynomial terms improve the fit of the model.

H0
the relationship shit is strictly linear, meaning the quadratic term improves the model fit

H1
polynomical terms significantly improve model fit



h0 cubic term overfits the relationship adding unneeded complexity 
h2 Cubic term significantly improves the fits the model better etc


h0 4th term overfits the relationship adding unneeded complexity 
h2 4th term significantly improves the fits the model better etc



### **Poisson Hypothesis Test**

The null and alternative hypotheses are defined as follows:

- **Null Hypothesis (\(H_0\)):** Extended Models with cubic terms overfit the model.
- **Alternative Hypothesis (\(H_1\)):** Models with the cubic term provide a more accurate fit.

Normal Hypothesis tests

The null and alternative hypotheses are defined as follows:

- **Null Hypothesis (\(H_0\)):** Extended Models with cubic terms overfit the model.
- **Alternative Hypothesis (\(H_1\)):** Models with the cubic term provide a more accurate fit.

**Poisson Model with polynomial terms**
``` {r Possion extended, include = TRUE}
Pmodel_quad <- glm(cases ~ date_numeric + I(date_numeric^2), data = aids, family = poisson(link = "log"))

Pmodel_cubic <- glm(cases ~ date_numeric + I(date_numeric^2) + I(date_numeric^3), data = aids, family = poisson(link = "log"))

```

Quasi-Poisson Model

**Normal Model with polynomial terms**
``` {r Normal extended, include = TRUE}
# Fitting polynomial terms
Nmodel_quad <- glm(cases ~ date_numeric + I(date_numeric^2), 
                   data = aids, family = gaussian(link = "log"))

Nmodel_cubic <- glm(cases ~ date_numeric + I(date_numeric^2) + I(date_numeric^3), 
                   data = aids, family = gaussian(link = "log"))

```

```{r residuals for extended models}
aids$residuals_model1_quad <- residuals(Pmodel_quad, type = "deviance")
aids$fitted_model1_quad <- fitted(Pmodel_quad)

aids$residuals_model2_quad <- residuals(Nmodel_quad, type = "deviance")
aids$fitted_model2_quad <- fitted(Nmodel_quad)


aids$residuals_model1_poly <- residuals(Pmodel_cubic, type = "deviance")
aids$fitted_model1_poly <- fitted(Pmodel_cubic)

aids$residuals_model2_poly <- residuals(Nmodel_cubic, type = "deviance")
aids$fitted_model2_poly <- fitted(Nmodel_cubic)
```

### Polynomial Terms

```{r redisuals plot for extended models, fig.align="center", echo=FALSE}
# Plot for Model 1
plot1 <- ggplot(aids, aes(x = fitted_model1_quad, y = residuals_model1_quad)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (ŒªÃÇ)", y = "Deviance Residuals", title = "Poisson") +
  custom_theme

# Plot for Model 2
plot2 <- ggplot(aids, aes(x = fitted_model2_quad, y = residuals_model2_quad)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (¬µÃÇ)", y = "Deviance Residuals",  title = "Normal") +
  custom_theme

# Combine the plots with a centered title
(plot1 + plot2) + 
  plot_annotation(
    title = "Figure 5: Deviance Residuals vs Fitted Values for Extended Models with Quadratic terms",
    theme = theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)))
```


```{r redisuals plot for cubic models, fig.align="center", echo=FALSE}
# Plot for Model 1
plot1 <- ggplot(aids, aes(x = fitted_model1_poly, y = residuals_model1_poly)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (ŒªÃÇ)", y = "Deviance Residuals", title = "Poisson") +
  custom_theme

# Plot for Model 2
plot2 <- ggplot(aids, aes(x = fitted_model2_poly, y = residuals_model2_poly)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (¬µÃÇ)", y = "Deviance Residuals",  title = "Normal") +
  custom_theme

# Combine the plots with a centered title
(plot1 + plot2) + 
  plot_annotation(
    title = "Figure 5: Deviance Residuals vs Fitted Values for both Extended Models with Cubic terms",
    theme = theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)))
```

### Quasi-Poisson

```{r quasi-poisson model, include=TRUE}
QPmodel <- glm(cases ~ date_numeric, data = aids, family = quasipoisson(link = "log"))
```
```{r plotting quasi, include = FALSE}
aids$residuals_model1_quasi <- residuals(QPmodel, type = "deviance")
aids$fitted_model1_quasi <- fitted(QPmodel)
```
``` {r quasiP, fig.align="center", echo = TRUE }
# Plot for Model 1
ggplot(aids, aes(x = fitted_model1_quasi, y = residuals_model1_quasi)) +
  geom_point(alpha = 0.8, color = "black") +
  geom_smooth(method = "loess", color = "red", se = FALSE) +
geom_hline(yintercept = 0, linetype = "dashed", colour = "black") +
  labs(x = "Fitted Values (ŒªÃÇ)", y = "Deviance Residuals", title = "Quasi-Poisson") +
  custom_theme
```

Comment on the models and improvements, Comment on which terms are better and why

Compare AIC values and use ANOVA TESTS, LLR tests, looking at pvalues


 e. On the basis of your answer to (a), analogous plots as in (b) and(c), but also on arguments of model fit based on the deviance and the AIC,comment on which (if any) of the two final models in (d) you would choose as the best. Mention at least one reason why either model is not ideal.
 
 
 
 
 f. Further extend your final Poisson model to a Negative Binomial model and
 comment on whether this model is preferable to the other two, on the basis of all the criteria used for comparison so far.
 
